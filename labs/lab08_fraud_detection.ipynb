{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 8: Blockchain Transaction Analysis & Fraud Detection\n",
        "\n",
        "Anomaly detection and network analytics for financial surveillance\n",
        "\n",
        "> **Expected Time**\n",
        ">\n",
        "> -   Core lab: â‰ˆ 75 minutes\n",
        "> -   Optional extensions: +30â€“60 minutes\n",
        "\n",
        "<figure>\n",
        "<a\n",
        "href=\"https://colab.research.google.com/github/quinfer/fin510-colab-notebooks/blob/main/labs/lab08_blockchain_fraud.ipynb\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n",
        "<figcaption>Open in Colab</figcaption>\n",
        "</figure>\n",
        "\n",
        "## Before You Code: The Big Picture\n",
        "\n",
        "Blockchainâ€™s transparency is a double-edged sword: **all transactions\n",
        "are public**, but **identities are pseudonymous**. Can we detect fraud\n",
        "and money laundering at scale using data science?\n",
        "\n",
        "> **The Blockchain Transparency Paradox**\n",
        ">\n",
        "> **The Promise:** 1. **Transparency**: Every transaction recorded\n",
        "> immutably on public ledger 2. **Traceability**: Follow the money from\n",
        "> source to destination 3. **Auditability**: Regulators can inspect\n",
        "> entire transaction history\n",
        ">\n",
        "> **The Reality:** - **Pseudonymity**: Addresses â‰  identities (hard to\n",
        "> link to real people) - **Mixing services**: Tumblers obscure\n",
        "> transaction trails - **Privacy coins**: Monero, Zcash use cryptography\n",
        "> to hide amounts/recipients - **Volume**: Billions of transactions â†’\n",
        "> needle-in-haystack problem\n",
        ">\n",
        "> **The Scale of Crypto Crime:** - \\$14 billion in crypto scams/hacks in\n",
        "> 2021 (Chainalysis 2022) - ~2.7% of crypto transaction volume illicit\n",
        "> in 2022 - Ransomware payments: \\$602M in 2021 (up 78% YoY) - North\n",
        "> Koreaâ€™s Lazarus Group: \\$1.7B stolen in 2022 (largest year ever)\n",
        ">\n",
        "> **Regulatory Response:** - **Travel Rule**: FATF requires identity\n",
        "> sharing for transfers \\>\\$1000 - **AML/KYC**: Exchanges must screen\n",
        "> users, report suspicious activity - **OFAC sanctions**: Block\n",
        "> transactions to/from sanctioned addresses\n",
        "\n",
        "### What Youâ€™ll Build Today\n",
        "\n",
        "By the end of this lab, you will have:\n",
        "\n",
        "-   âœ… Statistical anomaly detection (Z-scores, Mahalanobis distance)\n",
        "-   âœ… Machine learning methods (Isolation Forest, Autoencoders)\n",
        "-   âœ… Network analysis to detect fraud rings\n",
        "-   âœ… Trade-off analysis (detection rate vs.Â false positives)\n",
        "-   âœ… Understanding of real-world compliance challenges\n",
        "\n",
        "**Time estimate:** â‰ˆ 75 minutes (plus optional extensions)\n",
        "\n",
        "> **Why This Matters**\n",
        ">\n",
        "> Financial crime surveillance is a \\$100B+ industry (Bloomberg 2023).\n",
        "> Traditional banks use rules-based systems (slow, high false\n",
        "> positives). Your skills in anomaly detection and network analysis are\n",
        "> directly applicable to FinCrime teams at banks, exchanges, and\n",
        "> regulators.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "-   Analyze blockchain transaction data and identify suspicious patterns\n",
        "-   Implement statistical anomaly detection methods for fraud screening\n",
        "-   Apply machine learning algorithms (Isolation Forest, Autoencoders)\n",
        "    for anomaly detection\n",
        "-   Conduct network analysis to detect fraud rings and money laundering\n",
        "-   Visualize transaction flows and suspicious subnetworks\n",
        "-   Evaluate trade-offs between detection accuracy and false positive\n",
        "    rates\n",
        "-   Connect technical detection methods to regulatory compliance\n",
        "    (AML/KYC)\n",
        "\n",
        "## Setup and Dependencies"
      ],
      "id": "2174b7ee-f683-400c-8fc8-67a451ad5d0e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine learning\n",
        "try:\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.decomposition import PCA\n",
        "except ImportError:\n",
        "    print(\"Installing scikit-learn...\")\n",
        "    !pip install -q scikit-learn\n",
        "\n",
        "# Network analysis\n",
        "try:\n",
        "    import networkx as nx\n",
        "except ImportError:\n",
        "    print(\"Installing networkx...\")\n",
        "    !pip install -q networkx\n",
        "\n",
        "# Deep learning (for autoencoders)\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "except ImportError:\n",
        "    print(\"Installing tensorflow...\")\n",
        "    !pip install -q tensorflow\n",
        "\n",
        "# Visualization settings\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "print(\"âœ“ Setup complete - ready for fraud detection analysis\")"
      ],
      "id": "87d7c093"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Transaction Data Analysis and Pattern Identification\n",
        "\n",
        "### Understanding Blockchain Transaction Structure\n",
        "\n",
        "Blockchain transactions differ fundamentally from traditional banking\n",
        "transactions. Rather than simple transfers between accounts, Bitcoin\n",
        "transactions consume â€œunspent transaction outputsâ€ (UTXOs) and create\n",
        "new ones. Understanding this structure is essential for effective fraud\n",
        "detectionâ€”patterns that seem normal in account-based systems might\n",
        "indicate suspicious activity in UTXO-based systems.\n",
        "\n",
        "Weâ€™ll analyze simulated Bitcoin-like transaction data capturing\n",
        "realistic patterns whilst avoiding issues with actual blockchain data\n",
        "(size, privacy, changing patterns). The simulation includes normal\n",
        "transactions and injected fraud examples.\n",
        "\n",
        "### Loading and Exploring Transaction Data"
      ],
      "id": "d870ea52-ed48-4181-b006-015718fc1b18"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic blockchain transaction data with fraud examples\n",
        "np.random.seed(42)\n",
        "\n",
        "# Normal transaction parameters\n",
        "n_normal = 9500\n",
        "normal_amounts = np.random.lognormal(mean=3, sigma=1.5, size=n_normal)\n",
        "normal_inputs = np.random.poisson(lam=1.5, size=n_normal) + 1\n",
        "normal_outputs = np.random.poisson(lam=1.8, size=n_normal) + 1\n",
        "normal_fees = normal_amounts * 0.001 + np.random.normal(0, 0.0001, n_normal)\n",
        "\n",
        "# Fraud transaction parameters (unusual patterns)\n",
        "n_fraud = 500\n",
        "fraud_amounts = np.random.lognormal(mean=6, sigma=2, size=n_fraud)  # Larger amounts\n",
        "fraud_inputs = np.random.poisson(lam=5, size=n_fraud) + 1  # More inputs (mixing)\n",
        "fraud_outputs = np.random.poisson(lam=8, size=n_fraud) + 1  # More outputs (distribution)\n",
        "fraud_fees = fraud_amounts * 0.005 + np.random.normal(0, 0.0005, n_fraud)  # Higher fees\n",
        "\n",
        "# Combine into DataFrame\n",
        "transactions = pd.DataFrame({\n",
        "    'amount': np.concatenate([normal_amounts, fraud_amounts]),\n",
        "    'n_inputs': np.concatenate([normal_inputs, fraud_inputs]),\n",
        "    'n_outputs': np.concatenate([normal_outputs, fraud_outputs]),\n",
        "    'fee': np.concatenate([normal_fees, fraud_fees]),\n",
        "    'is_fraud': np.concatenate([np.zeros(n_normal), np.ones(n_fraud)])\n",
        "})\n",
        "\n",
        "# Add temporal features\n",
        "base_time = datetime(2024, 1, 1)\n",
        "transactions['timestamp'] = [\n",
        "    base_time + timedelta(seconds=int(x)) \n",
        "    for x in np.random.uniform(0, 86400*30, len(transactions))\n",
        "]\n",
        "transactions = transactions.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "# Add derived features\n",
        "transactions['hour'] = transactions['timestamp'].dt.hour\n",
        "transactions['fee_rate'] = transactions['fee'] / transactions['amount']\n",
        "transactions['total_io'] = transactions['n_inputs'] + transactions['n_outputs']\n",
        "\n",
        "# Display summary\n",
        "print(\"=\"*70)\n",
        "print(\"BLOCKCHAIN TRANSACTION DATASET\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal transactions: {len(transactions):,}\")\n",
        "print(f\"Fraud transactions: {int(transactions['is_fraud'].sum()):,} ({transactions['is_fraud'].mean()*100:.1f}%)\")\n",
        "print(f\"Normal transactions: {int((~transactions['is_fraud'].astype(bool)).sum()):,}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"STATISTICAL SUMMARY\")\n",
        "print(\"-\"*70)\n",
        "print(transactions[['amount', 'n_inputs', 'n_outputs', 'fee', 'fee_rate']].describe())\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"SAMPLE TRANSACTIONS\")\n",
        "print(\"-\"*70)\n",
        "print(transactions.head(10))"
      ],
      "id": "a805ddf5"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Transaction Patterns"
      ],
      "id": "9b91eaba-0bc1-4a9b-9db6-16d6076b902d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# Amount distribution (log scale)\n",
        "axes[0, 0].hist(np.log10(transactions[transactions['is_fraud']==0]['amount']), \n",
        "                bins=50, alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0, 0].hist(np.log10(transactions[transactions['is_fraud']==1]['amount']), \n",
        "                bins=50, alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0, 0].set_xlabel('Log10(Amount)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Transaction Amount Distribution', fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Input count distribution\n",
        "axes[0, 1].hist(transactions[transactions['is_fraud']==0]['n_inputs'], \n",
        "                bins=range(0, 20), alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0, 1].hist(transactions[transactions['is_fraud']==1]['n_inputs'], \n",
        "                bins=range(0, 20), alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0, 1].set_xlabel('Number of Inputs')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Transaction Inputs Distribution', fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Output count distribution\n",
        "axes[0, 2].hist(transactions[transactions['is_fraud']==0]['n_outputs'], \n",
        "                bins=range(0, 25), alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0, 2].hist(transactions[transactions['is_fraud']==1]['n_outputs'], \n",
        "                bins=range(0, 25), alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0, 2].set_xlabel('Number of Outputs')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "axes[0, 2].set_title('Transaction Outputs Distribution', fontweight='bold')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(alpha=0.3)\n",
        "\n",
        "# Fee rate distribution\n",
        "axes[1, 0].hist(transactions[transactions['is_fraud']==0]['fee_rate']*100, \n",
        "                bins=50, alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[1, 0].hist(transactions[transactions['is_fraud']==1]['fee_rate']*100, \n",
        "                bins=50, alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[1, 0].set_xlabel('Fee Rate (%)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Transaction Fee Rate Distribution', fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Hourly transaction volume\n",
        "hourly_counts = transactions.groupby('hour').size()\n",
        "hourly_fraud = transactions[transactions['is_fraud']==1].groupby('hour').size()\n",
        "axes[1, 1].bar(hourly_counts.index, hourly_counts.values, alpha=0.7, label='Total', color='blue')\n",
        "axes[1, 1].bar(hourly_fraud.index, hourly_fraud.values, alpha=0.9, label='Fraud', color='red')\n",
        "axes[1, 1].set_xlabel('Hour of Day')\n",
        "axes[1, 1].set_ylabel('Transaction Count')\n",
        "axes[1, 1].set_title('Hourly Transaction Patterns', fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "# Scatter: Amount vs Total I/O\n",
        "axes[1, 2].scatter(transactions[transactions['is_fraud']==0]['amount'], \n",
        "                   transactions[transactions['is_fraud']==0]['total_io'],\n",
        "                   alpha=0.5, s=10, label='Normal', color='blue')\n",
        "axes[1, 2].scatter(transactions[transactions['is_fraud']==1]['amount'], \n",
        "                   transactions[transactions['is_fraud']==1]['total_io'],\n",
        "                   alpha=0.7, s=20, label='Fraud', color='red', marker='^')\n",
        "axes[1, 2].set_xlabel('Amount')\n",
        "axes[1, 2].set_ylabel('Total Inputs + Outputs')\n",
        "axes[1, 2].set_title('Amount vs Transaction Complexity', fontweight='bold')\n",
        "axes[1, 2].set_xscale('log')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate separation between normal and fraud\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FRAUD vs NORMAL COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for col in ['amount', 'n_inputs', 'n_outputs', 'fee_rate', 'total_io']:\n",
        "    normal_mean = transactions[transactions['is_fraud']==0][col].mean()\n",
        "    fraud_mean = transactions[transactions['is_fraud']==1][col].mean()\n",
        "    pct_diff = ((fraud_mean - normal_mean) / normal_mean) * 100\n",
        "    \n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Normal mean: {normal_mean:.4f}\")\n",
        "    print(f\"  Fraud mean:  {fraud_mean:.4f}\")\n",
        "    print(f\"  Difference:  {pct_diff:+.1f}%\")"
      ],
      "id": "51639f26"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 1)\n",
        "\n",
        "Write 200-250 words addressing:\n",
        "\n",
        "1.  **Pattern Identification**: What transaction features distinguish\n",
        "    fraud from normal activity in this dataset? How might real-world\n",
        "    fraud patterns differ from these simulated examples?\n",
        "\n",
        "2.  **Detection Challenges**: Why might simple rule-based detection\n",
        "    (e.g., â€œflag all transactions \\>\\$Xâ€) miss sophisticated fraud\n",
        "    whilst generating many false positives?\n",
        "\n",
        "3.  **Temporal Patterns**: The hourly distribution shows some variation.\n",
        "    How might criminals deliberately time transactions to avoid\n",
        "    detection? What temporal features might improve fraud detection?\n",
        "\n",
        "## Exercise 2: Statistical and Machine Learning Anomaly Detection\n",
        "\n",
        "### Statistical Outlier Detection\n",
        "\n",
        "Weâ€™ll start with simple statistical methods establishing baseline\n",
        "performance before implementing more sophisticated machine learning\n",
        "approaches."
      ],
      "id": "8419ea30-c572-404e-8d73-5b6505978493"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features for anomaly detection\n",
        "feature_cols = ['amount', 'n_inputs', 'n_outputs', 'fee', 'fee_rate', 'total_io', 'hour']\n",
        "X = transactions[feature_cols].values\n",
        "y_true = transactions['is_fraud'].values\n",
        "\n",
        "# Standardize features (zero mean, unit variance)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Method 1: Z-score based detection (univariate)\n",
        "def zscore_anomaly_detection(data, threshold=3):\n",
        "    \"\"\"\n",
        "    Detect anomalies using Z-scores (standard deviations from mean).\n",
        "    \n",
        "    Classic statistical method: flag transactions with extreme values on\n",
        "    any feature. Fast and interpretable, but assumes independence.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_samples, n_features)\n",
        "        Scaled transaction features (zero mean, unit variance)\n",
        "    threshold : float, default=3\n",
        "        Number of standard deviations to use as cutoff\n",
        "        Common choices: 2.5 (broader), 3 (standard), 4 (conservative)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    ndarray, shape (n_samples,)\n",
        "        Boolean array: True = anomaly, False = normal\n",
        "        \n",
        "    Notes\n",
        "    -----\n",
        "    **How it works:**\n",
        "    - Compute Z-score for each feature: z = (x - Î¼) / Ïƒ\n",
        "    - Flag transaction if |z| > threshold on ANY feature\n",
        "    - Uses \"any\" aggregation (union rule), not \"all\" (intersection)\n",
        "    \n",
        "    **Pros:**\n",
        "    - Fast: O(n Ã— d) where n = samples, d = features\n",
        "    - Interpretable: Can explain why each transaction flagged\n",
        "    - No training required: Works on new data instantly\n",
        "    \n",
        "    **Cons:**\n",
        "    - Assumes features independent (ignores correlations)\n",
        "    - Sensitive to outliers in training data (affects Î¼, Ïƒ)\n",
        "    - High false positive rate (flags ~0.3% if threshold=3)\n",
        "    \n",
        "    **Real-World Usage:**\n",
        "    - First-line screening for manual review queue\n",
        "    - Rule-based systems in traditional banks\n",
        "    - Often combined with domain rules (e.g., \"amount > $10k AND ...\")\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> X_scaled = scaler.fit_transform(transactions[features])\n",
        "    >>> anomalies = zscore_anomaly_detection(X_scaled, threshold=3)\n",
        "    >>> print(f\"Flagged {anomalies.sum()} / {len(anomalies)} transactions\")\n",
        "    Flagged 45 / 10000 transactions (0.45%)\n",
        "    \"\"\"\n",
        "    z_scores = np.abs(stats.zscore(data, axis=0, nan_policy='omit'))\n",
        "    # Flag if ANY feature exceeds threshold\n",
        "    anomalies = (z_scores > threshold).any(axis=1)\n",
        "    return anomalies\n",
        "\n",
        "# Method 2: Percentile-based detection (multivariate using Mahalanobis distance)\n",
        "def mahalanobis_anomaly_detection(data, threshold_percentile=95):\n",
        "    \"\"\"\n",
        "    Detect anomalies using Mahalanobis distance (multivariate outlier detection).\n",
        "    \n",
        "    Accounts for feature correlations by measuring distance in transformed space\n",
        "    where features are uncorrelated. More sophisticated than Z-scores.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_samples, n_features)\n",
        "        Scaled transaction features (zero mean, unit variance)\n",
        "    threshold_percentile : float, default=95\n",
        "        Percentile cutoff (e.g., 95 = flag top 5% most distant)\n",
        "        Higher percentile = more conservative (fewer flags)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    tuple of (anomalies, distances)\n",
        "        anomalies : ndarray, shape (n_samples,), boolean flags\n",
        "        distances : ndarray, shape (n_samples,), Mahalanobis distances\n",
        "        \n",
        "    Notes\n",
        "    -----\n",
        "    **How it works:**\n",
        "    - Compute covariance matrix Î£ capturing feature correlations\n",
        "    - Mahalanobis distance: d = sqrt((x - Î¼)áµ€ Î£â»Â¹ (x - Î¼))\n",
        "    - Flag transactions with distances > percentile threshold\n",
        "    \n",
        "    **Why better than Z-scores:**\n",
        "    - Accounts for correlations (e.g., high amount + low fee = suspicious)\n",
        "    - Single distance metric (not \"any\" rule over features)\n",
        "    - Scale-invariant (like Z-scores, but multivariate)\n",
        "    \n",
        "    **Pros:**\n",
        "    - Captures multivariate patterns Z-scores miss\n",
        "    - Probabilistic interpretation (Chi-squared distribution)\n",
        "    - Lower false positive rate than Z-scores (for same recall)\n",
        "    \n",
        "    **Cons:**\n",
        "    - Assumes Gaussian distribution (fails on heavy tails)\n",
        "    - Requires covariance matrix inversion (unstable if d > n)\n",
        "    - Computationally slower: O(n Ã— dÂ²) vs O(n Ã— d) for Z-scores\n",
        "    \n",
        "    **Real-World Usage:**\n",
        "    - Second-line screening after Z-score pre-filter\n",
        "    - Works well when fraud exhibits correlated anomalies\n",
        "    - Used by payment processors (Visa, Mastercard)\n",
        "    \n",
        "    **Mathematical Note:**\n",
        "    Under Gaussian assumption, squared Mahalanobis distance follows Ï‡Â²(d)\n",
        "    distribution. Can use this for principled p-values instead of percentiles.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> X_scaled = scaler.fit_transform(transactions[features])\n",
        "    >>> anomalies, distances = mahalanobis_anomaly_detection(X_scaled, threshold_percentile=95)\n",
        "    >>> print(f\"Flagged {anomalies.sum()} transactions\")\n",
        "    >>> print(f\"Max distance: {distances.max():.2f}\")\n",
        "    Flagged 500 transactions (top 5%)\n",
        "    Max distance: 12.34\n",
        "    \"\"\"\n",
        "    # Calculate covariance matrix\n",
        "    cov_matrix = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.pinv(cov_matrix)\n",
        "    mean = np.mean(data, axis=0)\n",
        "    \n",
        "    # Calculate Mahalanobis distance for each point\n",
        "    diff = data - mean\n",
        "    mahal_dist = np.sqrt(np.sum(diff @ inv_cov * diff, axis=1))\n",
        "    \n",
        "    # Flag based on percentile threshold\n",
        "    threshold = np.percentile(mahal_dist, threshold_percentile)\n",
        "    anomalies = mahal_dist > threshold\n",
        "    \n",
        "    return anomalies, mahal_dist\n",
        "\n",
        "# Apply statistical methods\n",
        "print(\"=\"*70)\n",
        "print(\"STATISTICAL ANOMALY DETECTION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Z-score method\n",
        "z_anomalies = zscore_anomaly_detection(X_scaled, threshold=3)\n",
        "z_precision = y_true[z_anomalies].mean()\n",
        "z_recall = (y_true * z_anomalies).sum() / y_true.sum()\n",
        "z_fpr = ((1-y_true) * z_anomalies).sum() / (1-y_true).sum()\n",
        "\n",
        "print(f\"\\nZ-Score Method (threshold=3Ïƒ):\")\n",
        "print(f\"  Flagged: {z_anomalies.sum()} transactions ({z_anomalies.mean()*100:.1f}%)\")\n",
        "print(f\"  Precision: {z_precision:.3f} (of flagged, what % are fraud?)\")\n",
        "print(f\"  Recall: {z_recall:.3f} (of fraud, what % detected?)\")\n",
        "print(f\"  False Positive Rate: {z_fpr:.3f}\")\n",
        "\n",
        "# Mahalanobis method\n",
        "mahal_anomalies, mahal_dist = mahalanobis_anomaly_detection(X_scaled, threshold_percentile=95)\n",
        "mahal_precision = y_true[mahal_anomalies].mean()\n",
        "mahal_recall = (y_true * mahal_anomalies).sum() / y_true.sum()\n",
        "mahal_fpr = ((1-y_true) * mahal_anomalies).sum() / (1-y_true).sum()\n",
        "\n",
        "print(f\"\\nMahalanobis Distance Method (95th percentile threshold):\")\n",
        "print(f\"  Flagged: {mahal_anomalies.sum()} transactions ({mahal_anomalies.mean()*100:.1f}%)\")\n",
        "print(f\"  Precision: {mahal_precision:.3f}\")\n",
        "print(f\"  Recall: {mahal_recall:.3f}\")\n",
        "print(f\"  False Positive Rate: {mahal_fpr:.3f}\")"
      ],
      "id": "b534989b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Isolation Forest Anomaly Detection"
      ],
      "id": "fc67d26d-b2aa-49ab-b586-e1832cb28c97"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Isolation Forest - tree-based anomaly detection\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ISOLATION FOREST ANOMALY DETECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train Isolation Forest\n",
        "iso_forest = IsolationForest(\n",
        "    n_estimators=100,\n",
        "    contamination=0.05,  # Expected proportion of anomalies\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "iso_forest.fit(X_scaled)\n",
        "\n",
        "# Predict anomalies (-1 for anomaly, 1 for normal)\n",
        "iso_predictions = iso_forest.predict(X_scaled)\n",
        "iso_anomalies = (iso_predictions == -1)\n",
        "\n",
        "# Calculate scores (more negative = more anomalous)\n",
        "iso_scores = iso_forest.score_samples(X_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "iso_precision = y_true[iso_anomalies].mean()\n",
        "iso_recall = (y_true * iso_anomalies).sum() / y_true.sum()\n",
        "iso_fpr = ((1-y_true) * iso_anomalies).sum() / (1-y_true).sum()\n",
        "\n",
        "print(f\"\\nIsolation Forest Results:\")\n",
        "print(f\"  Flagged: {iso_anomalies.sum()} transactions ({iso_anomalies.mean()*100:.1f}%)\")\n",
        "print(f\"  Precision: {iso_precision:.3f}\")\n",
        "print(f\"  Recall: {iso_recall:.3f}\")\n",
        "print(f\"  False Positive Rate: {iso_fpr:.3f}\")\n",
        "\n",
        "# Visualize anomaly scores\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Score distribution\n",
        "axes[0].hist(iso_scores[y_true==0], bins=50, alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0].hist(iso_scores[y_true==1], bins=50, alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0].set_xlabel('Anomaly Score (more negative = more anomalous)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Isolation Forest Anomaly Scores', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# ROC-like curve: varying threshold\n",
        "thresholds = np.percentile(iso_scores, range(0, 100, 5))\n",
        "precisions, recalls, fprs = [], [], []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    preds = (iso_scores < threshold)\n",
        "    if preds.sum() > 0:\n",
        "        precision = y_true[preds].mean()\n",
        "        recall = (y_true * preds).sum() / y_true.sum()\n",
        "        fpr = ((1-y_true) * preds).sum() / (1-y_true).sum()\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        fprs.append(fpr)\n",
        "\n",
        "axes[1].plot(fprs, recalls, marker='o', linewidth=2, markersize=4)\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate (Recall)')\n",
        "axes[1].set_title('Detection Trade-off Curve', fontweight='bold')\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "1e7b23f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autoencoder-based Anomaly Detection"
      ],
      "id": "ae1f115d-1088-440e-9b33-88befe4600b8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"AUTOENCODER ANOMALY DETECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Build autoencoder architecture\n",
        "input_dim = X_scaled.shape[1]\n",
        "encoding_dim = 4  # Bottleneck dimension\n",
        "\n",
        "# Encoder\n",
        "encoder = keras.Sequential([\n",
        "    layers.Dense(12, activation='relu', input_shape=(input_dim,)),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(encoding_dim, activation='relu', name='encoding')\n",
        "])\n",
        "\n",
        "# Decoder\n",
        "decoder = keras.Sequential([\n",
        "    layers.Dense(8, activation='relu', input_shape=(encoding_dim,)),\n",
        "    layers.Dense(12, activation='relu'),\n",
        "    layers.Dense(input_dim, activation='linear')\n",
        "])\n",
        "\n",
        "# Complete autoencoder\n",
        "autoencoder = keras.Sequential([encoder, decoder])\n",
        "\n",
        "# Compile\n",
        "autoencoder.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse'\n",
        ")\n",
        "\n",
        "print(\"\\nAutoencoder Architecture:\")\n",
        "autoencoder.summary()\n",
        "\n",
        "# Train on normal transactions only (unsupervised approach)\n",
        "X_train_normal = X_scaled[y_true == 0]\n",
        "\n",
        "print(f\"\\nTraining on {len(X_train_normal)} normal transactions...\")\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    X_train_normal, X_train_normal,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Calculate reconstruction error for all transactions\n",
        "reconstructions = autoencoder.predict(X_scaled, verbose=0)\n",
        "reconstruction_errors = np.mean(np.square(X_scaled - reconstructions), axis=1)\n",
        "\n",
        "# Flag anomalies based on reconstruction error threshold (95th percentile)\n",
        "error_threshold = np.percentile(reconstruction_errors, 95)\n",
        "ae_anomalies = reconstruction_errors > error_threshold\n",
        "\n",
        "# Evaluate\n",
        "ae_precision = y_true[ae_anomalies].mean()\n",
        "ae_recall = (y_true * ae_anomalies).sum() / y_true.sum()\n",
        "ae_fpr = ((1-y_true) * ae_anomalies).sum() / (1-y_true).sum()\n",
        "\n",
        "print(f\"\\nAutoencoder Results (95th percentile threshold):\")\n",
        "print(f\"  Error threshold: {error_threshold:.4f}\")\n",
        "print(f\"  Flagged: {ae_anomalies.sum()} transactions ({ae_anomalies.mean()*100:.1f}%)\")\n",
        "print(f\"  Precision: {ae_precision:.3f}\")\n",
        "print(f\"  Recall: {ae_recall:.3f}\")\n",
        "print(f\"  False Positive Rate: {ae_fpr:.3f}\")\n",
        "\n",
        "# Visualize reconstruction errors\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Error distribution\n",
        "axes[0].hist(reconstruction_errors[y_true==0], bins=50, alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0].hist(reconstruction_errors[y_true==1], bins=50, alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0].axvline(error_threshold, color='green', linestyle='--', linewidth=2, label=f'Threshold ({error_threshold:.3f})')\n",
        "axes[0].set_xlabel('Reconstruction Error')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Autoencoder Reconstruction Errors', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Training history\n",
        "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('MSE Loss')\n",
        "axes[1].set_title('Autoencoder Training History', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "d81a1b2c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Detection Methods"
      ],
      "id": "0fd7a096-4a97-4aa5-bdd7-dc34fdc7f0af"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"METHOD COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Method': ['Z-Score', 'Mahalanobis', 'Isolation Forest', 'Autoencoder'],\n",
        "    'Precision': [z_precision, mahal_precision, iso_precision, ae_precision],\n",
        "    'Recall': [z_recall, mahal_recall, iso_recall, ae_recall],\n",
        "    'FPR': [z_fpr, mahal_fpr, iso_fpr, ae_fpr],\n",
        "    'Flagged': [\n",
        "        z_anomalies.sum(),\n",
        "        mahal_anomalies.sum(),\n",
        "        iso_anomalies.sum(),\n",
        "        ae_anomalies.sum()\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\", comparison.to_string(index=False))\n",
        "\n",
        "# Calculate F1 scores\n",
        "comparison['F1'] = 2 * (comparison['Precision'] * comparison['Recall']) / (comparison['Precision'] + comparison['Recall'])\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"F1 Scores (harmonic mean of precision and recall):\")\n",
        "print(\"-\"*70)\n",
        "for idx, row in comparison.iterrows():\n",
        "    print(f\"  {row['Method']:20s}: {row['F1']:.3f}\")\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Precision-Recall comparison\n",
        "x_pos = np.arange(len(comparison))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x_pos - width/2, comparison['Precision'], width, label='Precision', alpha=0.8)\n",
        "axes[0].bar(x_pos + width/2, comparison['Recall'], width, label='Recall', alpha=0.8)\n",
        "axes[0].set_xlabel('Method')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_title('Precision vs Recall by Method', fontweight='bold')\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(comparison['Method'], rotation=45, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# F1 score comparison\n",
        "axes[1].bar(x_pos, comparison['F1'], color='green', alpha=0.8)\n",
        "axes[1].set_xlabel('Method')\n",
        "axes[1].set_ylabel('F1 Score')\n",
        "axes[1].set_title('Overall Performance (F1 Score)', fontweight='bold')\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(comparison['Method'], rotation=45, ha='right')\n",
        "axes[1].grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ Key Insights:\")\n",
        "print(\"  - No single method dominates all metrics\")\n",
        "print(\"  - Precision-recall trade-off varies by method\")\n",
        "print(\"  - Production systems often ensemble multiple methods\")\n",
        "print(\"  - False positive management is critical for operational deployment\")"
      ],
      "id": "d39c409b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 2)\n",
        "\n",
        "Write 250-300 words addressing:\n",
        "\n",
        "1.  **Method Comparison**: Which anomaly detection method performed best\n",
        "    on this dataset? Consider both quantitative metrics (precision,\n",
        "    recall, F1) and qualitative factors (interpretability, computational\n",
        "    cost, ease of deployment).\n",
        "\n",
        "2.  **Trade-offs**: The methods show different precision-recall\n",
        "    trade-offs. For financial fraud detection, which is more\n",
        "    importantâ€”catching all fraud (high recall) or minimizing false\n",
        "    alarms (high precision)? How does this answer depend on context\n",
        "    (transaction amount, customer relationship, regulatory\n",
        "    requirements)?\n",
        "\n",
        "3.  **Adversarial Adaptation**: Criminals who know the detection system\n",
        "    might craft transactions to evade detection. Which method is most\n",
        "    robust to adversarial attacks? Which is most vulnerable? How can\n",
        "    detection systems adapt to evolving fraud tactics?\n",
        "\n",
        "## Exercise 3: Network Analysis for Fraud Detection\n",
        "\n",
        "### Building Transaction Networks\n",
        "\n",
        "Fraud often involves networksâ€”money mules, laundering chains,\n",
        "coordinated account takeovers. Graph analytics reveals patterns\n",
        "invisible to transaction-level analysis."
      ],
      "id": "b091ef03-9345-4aef-b223-2d4c17427c78"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate synthetic transaction network\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create addresses (nodes)\n",
        "n_addresses = 200\n",
        "addresses = [f\"addr_{i:04d}\" for i in range(n_addresses)]\n",
        "\n",
        "# Generate transactions (edges) with different patterns\n",
        "edges = []\n",
        "\n",
        "# Normal transactions: random pairs with preferential attachment\n",
        "for _ in range(300):\n",
        "    # Preferential attachment: popular addresses receive more transactions\n",
        "    source = np.random.choice(addresses)\n",
        "    # Weighted choice favoring certain addresses (simulating exchanges, merchants)\n",
        "    weights = np.array([1 / (i+1) for i in range(n_addresses)])\n",
        "    weights = weights / weights.sum()\n",
        "    target = np.random.choice(addresses, p=weights)\n",
        "    \n",
        "    if source != target:\n",
        "        amount = np.random.lognormal(3, 1.5)\n",
        "        edges.append((source, target, amount, 'normal'))\n",
        "\n",
        "# Fraud pattern 1: Mixing service (one source, many outputs)\n",
        "mixer_source = addresses[0]\n",
        "for i in range(20):\n",
        "    target = np.random.choice(addresses[50:150])\n",
        "    amount = np.random.lognormal(4, 1)\n",
        "    edges.append((mixer_source, target, amount, 'mixing'))\n",
        "\n",
        "# Fraud pattern 2: Layering (sequential chain)\n",
        "chain_start = addresses[150]\n",
        "chain = [chain_start] + list(np.random.choice(addresses[151:170], size=10, replace=False))\n",
        "for i in range(len(chain)-1):\n",
        "    amount = np.random.lognormal(5, 0.5)\n",
        "    edges.append((chain[i], chain[i+1], amount, 'layering'))\n",
        "\n",
        "# Fraud pattern 3: Tight fraud ring (densely connected subgraph)\n",
        "fraud_ring = addresses[170:180]\n",
        "for i, addr1 in enumerate(fraud_ring):\n",
        "    for addr2 in fraud_ring[i+1:]:\n",
        "        if np.random.random() < 0.4:  # 40% connection probability within ring\n",
        "            amount = np.random.lognormal(4, 1)\n",
        "            edges.append((addr1, addr2, amount, 'fraud_ring'))\n",
        "\n",
        "# Convert to DataFrame\n",
        "edge_df = pd.DataFrame(edges, columns=['source', 'target', 'amount', 'pattern'])\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRANSACTION NETWORK\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nNodes (addresses): {n_addresses}\")\n",
        "print(f\"Edges (transactions): {len(edge_df)}\")\n",
        "print(\"\\nPattern distribution:\")\n",
        "print(edge_df['pattern'].value_counts())"
      ],
      "id": "14ae1850"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Metrics and Centrality Analysis"
      ],
      "id": "55341aaf-2e59-4a3a-8220-a08c85a63cc5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for _, row in edge_df.iterrows():\n",
        "    G.add_edge(row['source'], row['target'], \n",
        "               amount=row['amount'], pattern=row['pattern'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NETWORK STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nNodes: {G.number_of_nodes()}\")\n",
        "print(f\"Edges: {G.number_of_edges()}\")\n",
        "print(f\"Density: {nx.density(G):.4f}\")\n",
        "print(f\"Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
        "\n",
        "# Calculate centrality metrics\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "in_degree_centrality = nx.in_degree_centrality(G)\n",
        "out_degree_centrality = nx.out_degree_centrality(G)\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "# Identify suspicious nodes based on centrality\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TOP 10 NODES BY CENTRALITY\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Degree centrality (most connected)\n",
        "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"\\nDegree Centrality (most connected):\")\n",
        "for addr, score in top_degree:\n",
        "    in_deg = G.in_degree(addr)\n",
        "    out_deg = G.out_degree(addr)\n",
        "    print(f\"  {addr}: {score:.4f} (in={in_deg}, out={out_deg})\")\n",
        "\n",
        "# Betweenness centrality (bridges/brokers)\n",
        "top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"\\nBetweenness Centrality (bridges between communities):\")\n",
        "for addr, score in top_betweenness[:5]:  # Show top 5\n",
        "    print(f\"  {addr}: {score:.4f}\")\n",
        "\n",
        "# Out-degree centrality (distributors)\n",
        "top_out = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "print(\"\\nOut-Degree Centrality (high output activity):\")\n",
        "for addr, score in top_out:\n",
        "    print(f\"  {addr}: {score:.4f} (out_degree={G.out_degree(addr)})\")"
      ],
      "id": "461af294"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Community Detection"
      ],
      "id": "5d125085-4708-4c2d-b115-84e3a6ab4f7d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to undirected for community detection\n",
        "G_undirected = G.to_undirected()\n",
        "\n",
        "# Detect communities using Louvain method\n",
        "try:\n",
        "    import community as community_louvain\n",
        "except ImportError:\n",
        "    print(\"Installing python-louvain...\")\n",
        "    !pip install -q python-louvain\n",
        "    import community as community_louvain\n",
        "\n",
        "# Detect communities\n",
        "communities = community_louvain.best_partition(G_undirected)\n",
        "\n",
        "# Analyze community structure\n",
        "community_sizes = {}\n",
        "for node, comm_id in communities.items():\n",
        "    community_sizes[comm_id] = community_sizes.get(comm_id, 0) + 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMMUNITY DETECTION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nNumber of communities detected: {len(community_sizes)}\")\n",
        "print(f\"\\nCommunity sizes:\")\n",
        "for comm_id, size in sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "    print(f\"  Community {comm_id}: {size} nodes\")\n",
        "\n",
        "# Identify suspicious communities (very small tight groups)\n",
        "suspicious_communities = [comm_id for comm_id, size in community_sizes.items() if 5 <= size <= 15]\n",
        "\n",
        "print(f\"\\nSuspicious communities (5-15 nodes): {len(suspicious_communities)}\")\n",
        "\n",
        "# Analyze fraud ring community (addresses 170-180)\n",
        "fraud_ring_communities = [communities[addr] for addr in addresses[170:180]]\n",
        "fraud_ring_comm_id = max(set(fraud_ring_communities), key=fraud_ring_communities.count)\n",
        "\n",
        "print(f\"\\nFraud ring detection:\")\n",
        "print(f\"  Known fraud ring addresses assigned to community {fraud_ring_comm_id}\")\n",
        "print(f\"  Community size: {community_sizes[fraud_ring_comm_id]} nodes\")"
      ],
      "id": "d89d78d4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Visualization"
      ],
      "id": "048d40b8-8eba-4fcf-b8b9-313a6bd8c946"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize subnetwork focusing on high-activity nodes\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Subgraph 1: High degree nodes\n",
        "high_degree_nodes = [node for node, degree in G.degree() if degree >= 5]\n",
        "G_high_degree = G.subgraph(high_degree_nodes)\n",
        "\n",
        "pos1 = nx.spring_layout(G_high_degree, k=0.5, iterations=50, seed=42)\n",
        "node_colors1 = ['red' if node in addresses[0:1] or node in addresses[170:180] \n",
        "                else 'lightblue' for node in G_high_degree.nodes()]\n",
        "\n",
        "nx.draw_networkx_nodes(G_high_degree, pos1, node_color=node_colors1, \n",
        "                       node_size=300, alpha=0.8, ax=axes[0])\n",
        "nx.draw_networkx_edges(G_high_degree, pos1, alpha=0.3, arrows=True, \n",
        "                       arrowsize=10, ax=axes[0])\n",
        "nx.draw_networkx_labels(G_high_degree, pos1, font_size=7, ax=axes[0])\n",
        "\n",
        "axes[0].set_title('High-Activity Nodes Subnetwork\\n(Red = suspicious)', \n",
        "                  fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Subgraph 2: Known fraud ring\n",
        "fraud_ring_extended = addresses[170:180]\n",
        "# Add neighbors for context\n",
        "for addr in fraud_ring_extended:\n",
        "    fraud_ring_extended.extend(list(G.neighbors(addr)))\n",
        "fraud_ring_extended = list(set(fraud_ring_extended))\n",
        "\n",
        "G_fraud_ring = G.subgraph(fraud_ring_extended)\n",
        "\n",
        "pos2 = nx.spring_layout(G_fraud_ring, k=0.8, iterations=50, seed=42)\n",
        "node_colors2 = ['red' if node in addresses[170:180] else 'lightgray' \n",
        "                for node in G_fraud_ring.nodes()]\n",
        "\n",
        "nx.draw_networkx_nodes(G_fraud_ring, pos2, node_color=node_colors2, \n",
        "                       node_size=400, alpha=0.8, ax=axes[1])\n",
        "nx.draw_networkx_edges(G_fraud_ring, pos2, alpha=0.4, arrows=True, \n",
        "                       arrowsize=12, ax=axes[1])\n",
        "nx.draw_networkx_labels(G_fraud_ring, pos2, font_size=8, ax=axes[1])\n",
        "\n",
        "axes[1].set_title('Fraud Ring and Neighbors\\n(Red = fraud ring members)', \n",
        "                  fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate graph metrics for fraud detection\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FRAUD PATTERN DETECTION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if mixer was identified\n",
        "mixer_score = out_degree_centrality[mixer_source]\n",
        "print(f\"\\nMixing service detection:\")\n",
        "print(f\"  Address: {mixer_source}\")\n",
        "print(f\"  Out-degree centrality: {mixer_score:.4f} (rank: {sorted(out_degree_centrality.values(), reverse=True).index(mixer_score)+1})\")\n",
        "print(f\"  Out-degree: {G.out_degree(mixer_source)} (should be high for mixer)\")\n",
        "\n",
        "# Check if layering chain was identified\n",
        "chain_between = [betweenness_centrality[addr] for addr in chain[1:-1]]  # Middle nodes\n",
        "avg_chain_between = np.mean(chain_between)\n",
        "print(f\"\\nLayering chain detection:\")\n",
        "print(f\"  Average betweenness of chain nodes: {avg_chain_between:.4f}\")\n",
        "print(f\"  These nodes should have high betweenness (act as bridges)\")\n",
        "\n",
        "# Check if fraud ring was identified as community\n",
        "fraud_ring_in_community = sum(1 for addr in addresses[170:180] \n",
        "                               if communities[addr] == fraud_ring_comm_id)\n",
        "print(f\"\\nFraud ring detection:\")\n",
        "print(f\"  Fraud ring members in same community: {fraud_ring_in_community}/10\")\n",
        "print(f\"  Community density should be high for fraud rings\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Network analytics revealed patterns invisible to transaction-level analysis!\")"
      ],
      "id": "1ecc0c14"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 3)\n",
        "\n",
        "Write 200-250 words addressing:\n",
        "\n",
        "1.  **Network Patterns**: How do mixing services, layering chains, and\n",
        "    fraud rings manifest differently in network structure? Which\n",
        "    centrality metrics are most effective for detecting each pattern?\n",
        "\n",
        "2.  **Scalability**: Real blockchain networks have millions of nodes and\n",
        "    billions of edges. What technical challenges arise when scaling\n",
        "    these analyses to production systems? What approximations or\n",
        "    sampling strategies might be necessary?\n",
        "\n",
        "3.  **Privacy Trade-offs**: Network analysis on transparent blockchains\n",
        "    enables sophisticated fraud detection but reveals transaction\n",
        "    relationships. How should regulators and system designers balance\n",
        "    fraud detection capabilities against user privacy? Are there\n",
        "    techniques that preserve privacy whilst enabling effective\n",
        "    monitoring?\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Exercise 4: Supervised Fraud Detection & Cost-Sensitive Learning\n",
        "\n",
        "**Learning Objectives:** - Demonstrate the base rate fallacy for rare\n",
        "events (Week 0, Â§0.8.3) - Apply supervised learning with proper\n",
        "validation (stratified CV) - Optimize decision thresholds using\n",
        "cost-sensitive criteria - Validate with temporal cross-validation (no\n",
        "look-ahead bias)\n",
        "\n",
        "> **Connection to [Ch 05: Marketplace\n",
        "> Lending](../chapters/05_alt_finance_marketplace_lending.qmd#sec-credit-type-errors)\n",
        "> & [Ch 08: Fraud\n",
        "> Detection](../chapters/08_blockchain_fraud_detection.qmd#sec-supervised-fraud)**\n",
        ">\n",
        "> This exercise reuses the **exact same framework** from Ch 05 credit\n",
        "> scoring: - Rare events (fraud \\<1%, defaults ~10%) - Type I/II error\n",
        "> tradeoff (false positive vs false negative) - Cost-sensitive threshold\n",
        "> selection - Proper validation strategies\n",
        ">\n",
        "> **Same statistical structure, different application!**\n",
        "\n",
        "### Part A: The Base Rate Fallacy Demonstration\n",
        "\n",
        "**Show why accuracy is useless for rare events:**"
      ],
      "id": "415d0b76-129b-4395-88a5-194f6b99ccba"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (classification_report, roc_auc_score, \n",
        "                              precision_recall_curve, confusion_matrix)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Simulate fraud dataset\n",
        "np.random.seed(42)\n",
        "n_transactions = 10000\n",
        "fraud_rate = 0.005  # 0.5% fraud (50 fraudulent, 9,950 legitimate)\n",
        "\n",
        "# Generate features (transaction amount, time, location proxy, etc.)\n",
        "data = pd.DataFrame({\n",
        "    'amount': np.random.lognormal(5, 2, n_transactions),\n",
        "    'hour': np.random.randint(0, 24, n_transactions),\n",
        "    'days_since_last': np.random.exponential(2, n_transactions),\n",
        "    'merchant_risk_score': np.random.beta(2, 5, n_transactions),\n",
        "})\n",
        "\n",
        "# Generate fraud labels (probability depends on features)\n",
        "fraud_prob = (\n",
        "    0.005  # baseline 0.5%\n",
        "    + 0.0001 * (data['amount'] - data['amount'].mean())  # High amount â†’ higher risk\n",
        "    + 0.001 * (data['hour'] < 6).astype(int)  # Late night â†’ higher risk\n",
        "    - 0.05 * data['merchant_risk_score']  # Low-risk merchant â†’ lower fraud\n",
        ")\n",
        "fraud_prob = 1 / (1 + np.exp(-fraud_prob))  # Logistic transform\n",
        "data['fraud'] = (np.random.random(n_transactions) < fraud_prob).astype(int)\n",
        "\n",
        "print(\"Dataset Overview:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Total transactions: {len(data):,}\")\n",
        "print(f\"Fraudulent: {data['fraud'].sum():,} ({data['fraud'].mean():.2%})\")\n",
        "print(f\"Legitimate: {(data['fraud'] == 0).sum():,} ({(data['fraud'] == 0).mean():.2%})\")\n",
        "\n",
        "# Naive Model: Predict ALL transactions as legitimate\n",
        "y_pred_naive = np.zeros(len(data))\n",
        "y_true = data['fraud']\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy_naive = (y_pred_naive == y_true).sum() / len(y_true)\n",
        "fraud_caught_naive = (y_pred_naive[y_true == 1] == 1).sum()\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(\"NAIVE MODEL: Predict Everything as Legitimate\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Accuracy:        {accuracy_naive:.2%} â† LOOKS AMAZING!\")\n",
        "print(f\"Fraud caught:    {fraud_caught_naive} out of {y_true.sum()}\")\n",
        "print(f\"Recall:          {fraud_caught_naive / y_true.sum():.2%} â† ZERO!\")\n",
        "print(f\"\\nâš ï¸  Base Rate Fallacy: 99.5% 'accurate' but COMPLETELY USELESS!\")\n",
        "print(f\"     The model catches ZERO fraudâ€”every fraudulent transaction succeeds.\")\n",
        "\n",
        "print(\"\\nðŸ’¡ For rare events (fraud <1%), accuracy is meaningless.\")\n",
        "print(\"   We need precision, recall, and F1-score instead.\")"
      ],
      "id": "93cffbf4"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Connection to [Week 0, Â§0.8.3: Base Rate\n",
        "> Fallacy](../chapters/00_foundations.qmd#sec-base-rate)**\n",
        ">\n",
        "> With **0.5% fraud rate**, predicting â€œall legitimateâ€ gives **99.5%\n",
        "> accuracy**!\n",
        ">\n",
        "> **Why accuracy is useless**: The model ignores the rare class entirely\n",
        "> and still scores high.\n",
        ">\n",
        "> **Proper metrics for imbalanced data**: - **Precision**: Of flagged\n",
        "> transactions, what % are fraud? - **Recall**: Of fraudulent\n",
        "> transactions, what % did we catch? - **F1-score**: Harmonic mean\n",
        "> balancing precision and recall\n",
        "\n",
        "### Part B: Balanced Supervised Model with Stratified CV\n",
        "\n",
        "**Proper approach: Use class weights and stratified cross-validation**"
      ],
      "id": "6eb0a86d-07e8-4315-800f-76bde22199e9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Prepare features and labels\n",
        "X = data[['amount', 'hour', 'days_since_last', 'merchant_risk_score']].values\n",
        "y = data['fraud'].values\n",
        "\n",
        "# Train-test split (stratified to maintain fraud rate)\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Balanced model (penalizes missing fraud more)\n",
        "model_balanced = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
        "\n",
        "# 5-fold stratified cross-validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores = cross_val_score(model_balanced, X_train_scaled, y_train, cv=cv, scoring='roc_auc')\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"BALANCED MODEL WITH STRATIFIED CROSS-VALIDATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"5-Fold CV AUC: {cv_scores.mean():.3f} Â± {cv_scores.std():.3f}\")\n",
        "print(f\"Individual folds: {cv_scores}\")\n",
        "\n",
        "# Train on full training set\n",
        "model_balanced.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predictions on test set\n",
        "y_pred_proba = model_balanced.predict_proba(X_test_scaled)[:, 1]\n",
        "y_pred = model_balanced.predict(X_test_scaled)\n",
        "\n",
        "# Evaluate\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "print(f\"  AUC-ROC: {roc_auc_score(y_test, y_pred_proba):.3f}\")\n",
        "print(f\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Legitimate', 'Fraud'], digits=3))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print(f\"\\nConfusion Matrix:\")\n",
        "print(f\"                  Predicted Legit    Predicted Fraud\")\n",
        "print(f\"Actually Legit    {cm[0,0]:<18} {cm[0,1]:<15} (FP: false alarms)\")\n",
        "print(f\"Actually Fraud    {cm[1,0]:<18} {cm[1,1]:<15} (FN: missed fraud)\")\n",
        "\n",
        "fraud_caught_balanced = cm[1,1]\n",
        "fraud_total = cm[1,0] + cm[1,1]\n",
        "recall_balanced = fraud_caught_balanced / fraud_total\n",
        "\n",
        "print(f\"\\nâœ“ Balanced model catches {fraud_caught_balanced}/{fraud_total} fraud ({recall_balanced:.1%} recall)\")\n",
        "print(f\"  vs naive model (0 fraud caught)\")"
      ],
      "id": "a34f9082"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Connection to [Ch 05: Stratified CV & Class\n",
        "> Weights](../chapters/05_alt_finance_marketplace_lending.qmd#sec-cross-validation)**\n",
        ">\n",
        "> **Stratified CV**: Maintains 0.5% fraud rate in each fold (prevents\n",
        "> empty folds)\n",
        ">\n",
        "> **class_weight=â€˜balancedâ€™**: Automatically weights classes inversely\n",
        "> proportional to frequency - Legitimate (99.5%): Weight = 0.5 / 0.995 â‰ˆ\n",
        "> 0.50 - Fraud (0.5%): Weight = 0.5 / 0.005 = **100Ã—** more penalty\n",
        ">\n",
        "> This forces model to pay attention to rare class (fraud).\n",
        "\n",
        "### Part C: Cost-Sensitive Threshold Optimization\n",
        "\n",
        "**Optimize decision threshold to minimize expected cost:**"
      ],
      "id": "c94631ff-2538-4db4-813d-92767d2cbf92"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define costs (business context)\n",
        "cost_fp = 25      # Â£25: Customer support + friction from false alarm\n",
        "cost_fn = 1200    # Â£1,200: Average fraud loss (chargebacks + investigation)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"COST-SENSITIVE THRESHOLD OPTIMIZATION\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Cost assumptions:\")\n",
        "print(f\"  Type I (False Positive):  Â£{cost_fp} per false alarm\")\n",
        "print(f\"  Type II (False Negative): Â£{cost_fn} per missed fraud\")\n",
        "print(f\"  Cost ratio: {cost_fn / cost_fp:.1f}:1 (Type II is {cost_fn/cost_fp:.0f}Ã— worse)\")\n",
        "\n",
        "# Test thresholds from 0.01 to 0.99\n",
        "thresholds_test = np.arange(0.01, 1.00, 0.01)\n",
        "results = []\n",
        "\n",
        "for threshold in thresholds_test:\n",
        "    y_pred_thresh = (y_pred_proba >= threshold).astype(int)\n",
        "    \n",
        "    # Confusion matrix\n",
        "    tn = ((y_pred_thresh == 0) & (y_test == 0)).sum()\n",
        "    fp = ((y_pred_thresh == 1) & (y_test == 0)).sum()\n",
        "    fn = ((y_pred_thresh == 0) & (y_test == 1)).sum()\n",
        "    tp = ((y_pred_thresh == 1) & (y_test == 1)).sum()\n",
        "    \n",
        "    # Calculate costs\n",
        "    total_cost = fp * cost_fp + fn * cost_fn\n",
        "    avg_cost = total_cost / len(y_test)\n",
        "    \n",
        "    # Metrics\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    \n",
        "    results.append({\n",
        "        'threshold': threshold,\n",
        "        'avg_cost': avg_cost,\n",
        "        'fp': fp,\n",
        "        'fn': fn,\n",
        "        'precision': precision,\n",
        "        'recall': recall\n",
        "    })\n",
        "\n",
        "# Find optimal threshold (minimum cost)\n",
        "results_df = pd.DataFrame(results)\n",
        "optimal_result = results_df.loc[results_df['avg_cost'].idxmin()]\n",
        "\n",
        "print(f\"\\nOptimal Threshold Results:\")\n",
        "print(f\"  Threshold:         {optimal_result['threshold']:.3f}\")\n",
        "print(f\"  Average cost:      Â£{optimal_result['avg_cost']:.2f} per transaction\")\n",
        "print(f\"  False positives:   {optimal_result['fp']:.0f}\")\n",
        "print(f\"  False negatives:   {optimal_result['fn']:.0f}\")\n",
        "print(f\"  Precision:         {optimal_result['precision']:.2%}\")\n",
        "print(f\"  Recall:            {optimal_result['recall']:.2%}\")\n",
        "\n",
        "# Compare to default threshold (0.5)\n",
        "default_result = results_df.loc[results_df['threshold'].sub(0.5).abs().idxmin()]\n",
        "cost_savings = default_result['avg_cost'] - optimal_result['avg_cost']\n",
        "\n",
        "print(f\"\\nComparison to Default Threshold (0.5):\")\n",
        "print(f\"  Default cost:      Â£{default_result['avg_cost']:.2f}\")\n",
        "print(f\"  Optimal cost:      Â£{optimal_result['avg_cost']:.2f}\")\n",
        "print(f\"  Savings:           Â£{cost_savings:.2f} per transaction\")\n",
        "\n",
        "# Visualize cost vs threshold\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Panel 1: Total cost vs threshold\n",
        "ax1.plot(results_df['threshold'], results_df['avg_cost'], linewidth=2, color='blue')\n",
        "ax1.axvline(optimal_result['threshold'], color='red', linestyle='--', linewidth=2, \n",
        "            label=f\"Optimal: {optimal_result['threshold']:.2f}\")\n",
        "ax1.axvline(0.5, color='gray', linestyle=':', linewidth=2, alpha=0.7, label='Default: 0.50')\n",
        "ax1.set_xlabel('Decision Threshold', fontsize=12)\n",
        "ax1.set_ylabel('Expected Cost per Transaction (Â£)', fontsize=12)\n",
        "ax1.set_title('Cost vs Threshold: Finding Optimal Decision Boundary', fontsize=13)\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "# Panel 2: Precision-Recall trade-off\n",
        "ax2.plot(results_df['recall'], results_df['precision'], linewidth=2, color='green')\n",
        "ax2.scatter(optimal_result['recall'], optimal_result['precision'], s=200, c='red', \n",
        "            edgecolors='black', zorder=5, label=f\"Optimal (threshold={optimal_result['threshold']:.2f})\")\n",
        "ax2.set_xlabel('Recall (% of fraud caught)', fontsize=12)\n",
        "ax2.set_ylabel('Precision (% flagged that are fraud)', fontsize=12)\n",
        "ax2.set_title('Precision-Recall Curve with Optimal Point', fontsize=13)\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nðŸ’¡ Optimal threshold ({optimal_result['threshold']:.2f}) is MUCH LOWER than default (0.5)\")\n",
        "print(f\"   Because missing fraud (Â£{cost_fn}) >> false alarm (Â£{cost_fp})\")"
      ],
      "id": "1b35a75c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Connection to [Ch 05: Cost-Sensitive Threshold\n",
        "> Selection](../chapters/05_alt_finance_marketplace_lending.qmd#sec-credit-type-errors)**\n",
        ">\n",
        "> **Minimize expected cost**, not maximize accuracy/F1:\n",
        ">\n",
        "> $$\\text{Expected Cost} = n_{\\text{FP}} \\times \\text{Cost}_{\\text{FP}} + n_{\\text{FN}} \\times \\text{Cost}_{\\text{FN}}$$\n",
        ">\n",
        "> When Type II cost \\>\\> Type I cost (typical ratio 50:1), optimal\n",
        "> threshold is **low** (e.g., 0.15): - Flag more transactions (higher\n",
        "> recall, catch more fraud) - Accept more false positives (cheaper\n",
        "> error) - Minimize total cost\n",
        "\n",
        "### Part D: Temporal Cross-Validation for Concept Drift\n",
        "\n",
        "**Test if model degrades over time (fraud patterns evolve):**"
      ],
      "id": "9bda9262-ef7d-4b2a-a724-39e3445de1fa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "\n",
        "# Add temporal dimension (transaction timestamps)\n",
        "data['timestamp'] = pd.date_range(start='2023-01-01', periods=len(data), freq='5min')\n",
        "data = data.sort_values('timestamp')  # Ensure temporal order\n",
        "\n",
        "# Prepare features\n",
        "X_temporal = data[['amount', 'hour', 'days_since_last', 'merchant_risk_score']].values\n",
        "y_temporal = data['fraud'].values\n",
        "\n",
        "# Temporal cross-validation (5 splits, expanding window)\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "cv_scores_temporal = []\n",
        "fold_sizes = []\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"TEMPORAL CROSS-VALIDATION (NO LOOK-AHEAD BIAS)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(tscv.split(X_temporal)):\n",
        "    # Train on past data only\n",
        "    X_train_fold = scaler.fit_transform(X_temporal[train_idx])\n",
        "    X_test_fold = scaler.transform(X_temporal[test_idx])\n",
        "    y_train_fold = y_temporal[train_idx]\n",
        "    y_test_fold = y_temporal[test_idx]\n",
        "    \n",
        "    # Train model\n",
        "    model_fold = LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42)\n",
        "    model_fold.fit(X_train_fold, y_train_fold)\n",
        "    \n",
        "    # Test on future data\n",
        "    y_pred_fold = model_fold.predict_proba(X_test_fold)[:, 1]\n",
        "    auc_fold = roc_auc_score(y_test_fold, y_pred_fold)\n",
        "    \n",
        "    cv_scores_temporal.append(auc_fold)\n",
        "    fold_sizes.append((len(train_idx), len(test_idx)))\n",
        "    \n",
        "    print(f\"Fold {fold+1}: Train on {len(train_idx):>5} txns | Test on {len(test_idx):>5} txns | AUC={auc_fold:.3f}\")\n",
        "\n",
        "# Compare to shuffled CV (WRONG approach)\n",
        "cv_shuffled = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv_scores_shuffled = cross_val_score(\n",
        "    LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42),\n",
        "    scaler.fit_transform(X_temporal), y_temporal,\n",
        "    cv=cv_shuffled, scoring='roc_auc'\n",
        ")\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 70)\n",
        "print(\"COMPARISON: Temporal vs Shuffled Cross-Validation\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Temporal CV (honest):     {np.mean(cv_scores_temporal):.3f} Â± {np.std(cv_scores_temporal):.3f}\")\n",
        "print(f\"Shuffled CV (look-ahead): {cv_scores_shuffled.mean():.3f} Â± {cv_scores_shuffled.std():.3f}\")\n",
        "print(f\"Difference:               {cv_scores_shuffled.mean() - np.mean(cv_scores_temporal):+.3f}\")\n",
        "\n",
        "if cv_scores_shuffled.mean() > np.mean(cv_scores_temporal):\n",
        "    print(f\"\\nâš ï¸  Shuffled CV is OVEROPTIMISTIC (look-ahead bias)!\")\n",
        "    print(f\"     It uses future fraud patterns to predict pastâ€”cheating!\")\n",
        "else:\n",
        "    print(f\"\\nâœ“  Similar performance (no strong concept drift detected)\")\n",
        "\n",
        "# Test for concept drift (is performance declining over time?)\n",
        "import scipy.stats as stats\n",
        "slope, intercept, r, p_value, stderr = stats.linregress(range(len(cv_scores_temporal)), cv_scores_temporal)\n",
        "\n",
        "print(f\"\\nConcept Drift Test:\")\n",
        "print(f\"  Performance slope: {slope:.4f} per fold\")\n",
        "print(f\"  p-value:           {p_value:.4f}\")\n",
        "\n",
        "if p_value < 0.05 and slope < 0:\n",
        "    print(f\"  âš ï¸  Significant DOWNWARD trend â†’ model degrading (concept drift)\")\n",
        "    print(f\"      Fraud patterns evolving, model needs retraining\")\n",
        "elif p_value < 0.05 and slope > 0:\n",
        "    print(f\"  âœ“  Significant UPWARD trend â†’ model improving (learning more data)\")\n",
        "else:\n",
        "    print(f\"  âœ“  No significant trend â†’ stable performance over time\")\n",
        "\n",
        "# Visualize\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "# Panel 1: Fold-by-fold AUC comparison\n",
        "folds = np.arange(1, 6)\n",
        "width = 0.35\n",
        "\n",
        "ax1.bar(folds - width/2, cv_scores_temporal, width, label='Temporal CV', alpha=0.7, color='blue')\n",
        "ax1.bar(folds + width/2, cv_scores_shuffled, width, label='Shuffled CV', alpha=0.7, color='red')\n",
        "ax1.axhline(np.mean(cv_scores_temporal), color='blue', linestyle='--', alpha=0.5)\n",
        "ax1.axhline(cv_scores_shuffled.mean(), color='red', linestyle='--', alpha=0.5)\n",
        "ax1.set_xlabel('Fold', fontsize=12)\n",
        "ax1.set_ylabel('AUC Score', fontsize=12)\n",
        "ax1.set_title('Temporal vs Shuffled Cross-Validation', fontsize=13)\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(alpha=0.3, axis='y')\n",
        "\n",
        "# Panel 2: Concept drift visualization\n",
        "ax2.plot(folds, cv_scores_temporal, marker='o', markersize=10, linewidth=2, color='blue', label='Temporal CV AUC')\n",
        "ax2.plot(folds, intercept + slope * np.arange(len(cv_scores_temporal)), \n",
        "         linestyle='--', linewidth=2, color='red', label=f'Trend (slope={slope:.4f})')\n",
        "ax2.set_xlabel('Fold (Time Order)', fontsize=12)\n",
        "ax2.set_ylabel('AUC Score', fontsize=12)\n",
        "ax2.set_title('Concept Drift Detection: Performance Over Time', fontsize=13)\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nâœ” Temporal cross-validation complete\")"
      ],
      "id": "995bf330"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Connection to [Week 0, Â§0.6: Time-Series\n",
        "> CV](../chapters/00_foundations.qmd#sec-model-selection)**\n",
        ">\n",
        "> **CANNOT shuffle fraud data**â€”time order matters!\n",
        ">\n",
        "> **Temporal CV** (TimeSeriesSplit): - Always train on past, test on\n",
        "> future - Expanding window: Fold 1 trains on 1-2000, tests on\n",
        "> 2001-4000; Fold 2 trains on 1-4000, tests on 4001-6000 - Detects\n",
        "> concept drift: If AUC declines over folds â†’ fraud patterns evolving\n",
        ">\n",
        "> **Shuffled CV CHEATS**: Uses future fraud examples to predict past\n",
        "> (look-ahead bias) â†’ overoptimistic by 0.03-0.05 AUC points!\n",
        "\n",
        "### Reflection Questions (Exercise 4)\n",
        "\n",
        "Write 250-300 words addressing:\n",
        "\n",
        "1.  **Base rate fallacy**: Why does a 99.5% accurate model that catches\n",
        "    zero fraud seem paradoxical? Explain using the fraud rate (0.5%) and\n",
        "    why accuracy is misleading for rare events.\n",
        "\n",
        "2.  **Threshold selection**: The optimal threshold was ~0.15 (not 0.5).\n",
        "    Why? Connect to the cost ratio (Type II = 50Ã— Type I). If false\n",
        "    positive costs increased to Â£100, would optimal threshold rise or\n",
        "    fall? Why?\n",
        "\n",
        "3.  **Temporal vs shuffled CV**: Did shuffled CV overestimate\n",
        "    performance? By how much? Why does this matter for production\n",
        "    deployment?\n",
        "\n",
        "4.  **Framework reuse**: This exercise used the same techniques as Ch 05\n",
        "    (credit scoring). List 3 specific similarities and explain why the\n",
        "    statistical framework transfers across domains.\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Summary and Integration\n",
        "\n",
        "### What Weâ€™ve Learned\n",
        "\n",
        "Through these exercises, youâ€™ve:\n",
        "\n",
        "1.  **Analyzed blockchain transaction data** identifying distinguishing\n",
        "    features of fraudulent versus normal activity\n",
        "\n",
        "2.  **Implemented multiple anomaly detection methods** (statistical,\n",
        "    Isolation Forest, autoencoders) and compared their effectiveness\n",
        "\n",
        "3.  **Evaluated precision-recall trade-offs** understanding that no\n",
        "    method perfectly detects all fraud without false positives\n",
        "\n",
        "4.  **Applied network analysis** detecting fraud patterns invisible to\n",
        "    transaction-level methodsâ€”mixing, layering, fraud rings\n",
        "\n",
        "5.  **Visualized suspicious subnetworks** making abstract analytics\n",
        "    interpretable for fraud analysts\n",
        "\n",
        "6.  **Considered operational deployment** thinking beyond algorithms to\n",
        "    false positive management, interpretability, and adversarial\n",
        "    robustness\n",
        "\n",
        "### Connections to Course Themes\n",
        "\n",
        "-   **Week 7 (Cryptocurrency)**: Blockchain transparency enables\n",
        "    analyses impossible with traditional banking data, but pseudonymity\n",
        "    limits identification\n",
        "\n",
        "-   **Week 6 (Financial Inclusion)**: Fraud detection systems must\n",
        "    balance security with accessâ€”overly aggressive detection excludes\n",
        "    legitimate users, especially marginalized populations\n",
        "\n",
        "-   **Week 3 (Platforms)**: Exchanges and payment platforms need fraud\n",
        "    detection at scale, balancing automated systems with human review\n",
        "\n",
        "-   **Week 2 (APIs)**: Real-world deployment requires integrating fraud\n",
        "    detection with blockchain APIs, streaming transaction feeds, and\n",
        "    alert systems\n",
        "\n",
        "### Critical Evaluation Framework\n",
        "\n",
        "When evaluating fraud detection systems:\n",
        "\n",
        "1.  **Quantitative performance**: Precision, recall, F1 scores, false\n",
        "    positive rates\n",
        "2.  **Operational feasibility**: Computational cost, latency,\n",
        "    scalability, integration complexity\n",
        "3.  **Interpretability**: Can analysts understand why transactions were\n",
        "    flagged?\n",
        "4.  **Adversarial robustness**: How easily can criminals evade\n",
        "    detection?\n",
        "5.  **Fairness**: Does system discriminate against certain users or\n",
        "    transaction types?\n",
        "6.  **Regulatory compliance**: Does system meet AML/KYC requirements?\n",
        "\n",
        "### Assessment Preparation\n",
        "\n",
        "If your assessment involves a short research report or reflective\n",
        "analysis, this lab gives you two strong pathways:\n",
        "\n",
        "-   Analyse transaction patterns in cryptocurrency or payment data using\n",
        "    anomaly detection and network analysis\n",
        "-   Critically evaluate fraud-detection claims by comparing promised\n",
        "    benefits to empirical evidence, limitations, and realistic\n",
        "    improvements\n",
        "\n",
        "### Further Exploration\n",
        "\n",
        "If interested in extending your analysis:\n",
        "\n",
        "-   **Temporal dynamics**: How do fraud patterns evolve? Can models\n",
        "    detect pattern shifts?\n",
        "-   **Deep learning on graphs**: Graph neural networks (GNNs) for\n",
        "    end-to-end fraud detection\n",
        "-   **Privacy-preserving detection**: Federated learning, differential\n",
        "    privacy for fraud detection without exposing transaction details\n",
        "-   **Adversarial ML**: How do criminals evade detection? Can we make\n",
        "    systems more robust?\n",
        "-   **Cross-chain analysis**: Detecting fraud spanning multiple\n",
        "    blockchains (bridges, wrapped tokens)\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Excellent work! Youâ€™ve implemented production-grade fraud detection\n",
        "techniques, connecting technical methods to operational realities and\n",
        "regulatory requirements.**"
      ],
      "id": "b3e73834-48d6-429b-b606-81df3c8d481b"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "fin510",
      "display_name": "FIN510 Python",
      "language": "python",
      "path": "/Users/quinference/Library/Jupyter/kernels/fin510"
    }
  }
}