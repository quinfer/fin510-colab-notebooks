{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 11: Market Prediction â€” Exploratory Analysis\n",
        "\n",
        "<figure>\n",
        "<a\n",
        "href=\"https://colab.research.google.com/github/quinfer/fin510-colab-notebooks/blob/main/labs/lab11_prediction.ipynb\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n",
        "<figcaption>Open in Colab</figcaption>\n",
        "</figure>\n",
        "\n",
        "## Objective\n",
        "\n",
        "This lab develops understanding of market prediction principles through\n",
        "**exploratory exercises**. Youâ€™ll investigate walk-forward validation,\n",
        "regularization, evaluation metrics, and overfittingâ€”without producing\n",
        "submission-ready outputs.\n",
        "\n",
        "**Important**: This is not a template for Coursework 2 Option B. The\n",
        "scaffold notebook ([open in\n",
        "Colab](https://colab.research.google.com/github/quinfer/fin510-colab-notebooks/blob/main/labs/coursework2_scaffold.ipynb))\n",
        "provides that. This lab teaches you **how to think** about prediction so\n",
        "you can interpret scaffold outputs intelligently and write critical\n",
        "analysis.\n",
        "\n",
        "**Time estimate**: 60-90 minutes\n",
        "\n",
        "## Learning Goals\n",
        "\n",
        "By the end of this lab, you should be able to:\n",
        "\n",
        "-   Explain why walk-forward validation prevents look-ahead bias\n",
        "-   Demonstrate how look-ahead bias inflates performance\n",
        "-   Compare OLS vs ridge when predictors correlate\n",
        "-   Interpret RÂ² OOS and directional accuracy\n",
        "-   Identify overfitting through in-sample vs out-of-sample gaps\n",
        "-   Ask critical questions about prediction model results\n",
        "\n",
        "## Setup"
      ],
      "id": "9330ca2f-c254-4582-b388-279914f50414"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.float_format', '{:.4f}'.format)\n",
        "np.random.seed(42)  # Reproducibility\n",
        "\n",
        "print(\"âœ“ Libraries loaded successfully\")"
      ],
      "id": "ba7e91a3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Understanding Walk-Forward Validation\n",
        "\n",
        "### Exercise 1.1: Simulating Walk-Forward Process\n",
        "\n",
        "Letâ€™s create a simple dataset and manually implement walk-forward\n",
        "validation to see how it works step-by-step."
      ],
      "id": "174007e2-dae5-4166-ab93-84bd9f7e4398"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate simulated monthly data\n",
        "n_months = 240  # 20 years\n",
        "dates = pd.date_range('2004-01-01', periods=n_months, freq='MS')\n",
        "\n",
        "# Simulate market returns and two factor predictors\n",
        "np.random.seed(42)\n",
        "market = np.random.normal(0.008, 0.04, n_months)\n",
        "factor1 = np.random.normal(0.006, 0.03, n_months)\n",
        "factor2 = np.random.normal(0.004, 0.035, n_months)\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'market': market,\n",
        "    'factor1': factor1,\n",
        "    'factor2': factor2\n",
        "})\n",
        "\n",
        "# Create target: next month's market return\n",
        "data['market_next'] = data['market'].shift(-1)\n",
        "\n",
        "# Remove last row (no future target)\n",
        "data = data[:-1].copy()\n",
        "\n",
        "print(f\"Data shape: {data.shape}\")\n",
        "print(f\"Date range: {data['date'].min()} to {data['date'].max()}\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "data.head()"
      ],
      "id": "23dea3fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  What is the â€œtargetâ€ variable weâ€™re trying to predict?\n",
        "2.  Why do we shift market returns by -1 to create the target?\n",
        "3.  What would happen if we forgot to remove the last row?\n",
        "\n",
        "### Exercise 1.2: Manual Walk-Forward Implementation\n",
        "\n",
        "Now implement walk-forward validation manually for first few iterations\n",
        "to see the process."
      ],
      "id": "95b6e72d-25a1-44a6-8a1a-af70f67dd361"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Settings\n",
        "train_window = 120  # 10 years training\n",
        "test_start = 120   # Start forecasting at month 120\n",
        "\n",
        "# Storage for results\n",
        "predictions = []\n",
        "actuals = []\n",
        "train_periods = []\n",
        "\n",
        "# Walk-forward loop (first 5 iterations for demonstration)\n",
        "for t in range(test_start, test_start + 5):\n",
        "    # Training data: rolling window\n",
        "    train_start = t - train_window\n",
        "    train_end = t\n",
        "    \n",
        "    X_train = data.iloc[train_start:train_end][['factor1', 'factor2']].values\n",
        "    y_train = data.iloc[train_start:train_end]['market_next'].values\n",
        "    \n",
        "    # Test data: next month only\n",
        "    X_test = data.iloc[t:t+1][['factor1', 'factor2']].values\n",
        "    y_test = data.iloc[t]['market_next']\n",
        "    \n",
        "    # Train model\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Forecast\n",
        "    y_pred = model.predict(X_test)[0]\n",
        "    \n",
        "    # Store results\n",
        "    predictions.append(y_pred)\n",
        "    actuals.append(y_test)\n",
        "    train_periods.append(f\"{data.iloc[train_start]['date'].year}-{data.iloc[train_end-1]['date'].year}\")\n",
        "    \n",
        "    print(f\"\\nForecast {t-train_window+1}:\")\n",
        "    print(f\"  Training period: {train_periods[-1]}\")\n",
        "    print(f\"  Forecast date: {data.iloc[t]['date']}\")\n",
        "    print(f\"  Predicted: {y_pred*100:.2f}%\")\n",
        "    print(f\"  Actual: {y_test*100:.2f}%\")\n",
        "    print(f\"  Error: {(y_pred - y_test)*100:.2f}%\")"
      ],
      "id": "013e45ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  How does the training window change as we move forward in time?\n",
        "2.  At each forecast date, has the model â€œseenâ€ the target value during\n",
        "    training?\n",
        "3.  Why do we retrain the model at each step rather than training once?\n",
        "4.  What prevents look-ahead bias in this process?\n",
        "\n",
        "## Part 2: Demonstrating Look-Ahead Bias\n",
        "\n",
        "### Exercise 2.1: Comparing Honest vs Biased Testing\n",
        "\n",
        "Letâ€™s explicitly compare walk-forward validation (honest) vs training on\n",
        "full sample (look-ahead bias)."
      ],
      "id": "4eae3a53-b170-41dc-8819-2ba927f2ead2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def walk_forward_prediction(data, train_window=120):\n",
        "    \"\"\"\n",
        "    Honest walk-forward prediction.\n",
        "    \"\"\"\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    dates = []\n",
        "    \n",
        "    for t in range(train_window, len(data)):\n",
        "        # Training: only past data\n",
        "        train_start = t - train_window\n",
        "        X_train = data.iloc[train_start:t][['factor1', 'factor2']].values\n",
        "        y_train = data.iloc[train_start:t]['market_next'].values\n",
        "        \n",
        "        # Test: current observation\n",
        "        X_test = data.iloc[t:t+1][['factor1', 'factor2']].values\n",
        "        y_test = data.iloc[t]['market_next']\n",
        "        \n",
        "        # Train and predict\n",
        "        model = LinearRegression()\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)[0]\n",
        "        \n",
        "        predictions.append(y_pred)\n",
        "        actuals.append(y_test)\n",
        "        dates.append(data.iloc[t]['date'])\n",
        "    \n",
        "    return np.array(predictions), np.array(actuals), dates\n",
        "\n",
        "def biased_prediction(data, train_window=120):\n",
        "    \"\"\"\n",
        "    Biased approach: train on full sample, then \"predict\" test period.\n",
        "    This introduces look-ahead bias.\n",
        "    \"\"\"\n",
        "    # Train on ENTIRE dataset (including test period - THIS IS WRONG)\n",
        "    X_all = data[['factor1', 'factor2']].values\n",
        "    y_all = data['market_next'].values\n",
        "    \n",
        "    model = LinearRegression()\n",
        "    model.fit(X_all, y_all)\n",
        "    \n",
        "    # \"Predict\" test period (but model already saw this data during training)\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    dates = []\n",
        "    \n",
        "    for t in range(train_window, len(data)):\n",
        "        X_test = data.iloc[t:t+1][['factor1', 'factor2']].values\n",
        "        y_test = data.iloc[t]['market_next']\n",
        "        \n",
        "        y_pred = model.predict(X_test)[0]\n",
        "        \n",
        "        predictions.append(y_pred)\n",
        "        actuals.append(y_test)\n",
        "        dates.append(data.iloc[t]['date'])\n",
        "    \n",
        "    return np.array(predictions), np.array(actuals), dates\n",
        "\n",
        "# Run both approaches\n",
        "pred_honest, actual_honest, dates_honest = walk_forward_prediction(data)\n",
        "pred_biased, actual_biased, dates_biased = biased_prediction(data)\n",
        "\n",
        "# Calculate RÂ² for both\n",
        "def calc_r2_oos(y_true, y_pred):\n",
        "    \"\"\"Calculate out-of-sample RÂ².\"\"\"\n",
        "    ss_res = ((y_true - y_pred) ** 2).sum()\n",
        "    ss_tot = ((y_true - y_true.mean()) ** 2).sum()\n",
        "    return 1 - (ss_res / ss_tot)\n",
        "\n",
        "r2_honest = calc_r2_oos(actual_honest, pred_honest)\n",
        "r2_biased = calc_r2_oos(actual_biased, pred_biased)\n",
        "\n",
        "print(\"=== Comparison: Honest vs Biased Testing ===\\n\")\n",
        "print(f\"Walk-Forward (Honest):  RÂ² OOS = {r2_honest*100:.2f}%\")\n",
        "print(f\"Full-Sample (Biased):   RÂ² OOS = {r2_biased*100:.2f}%\")\n",
        "print(f\"\\nDifference: {(r2_biased - r2_honest)*100:.2f} percentage points\")\n",
        "if abs(r2_honest) > 1e-4:\n",
        "    print(f\"Inflation factor: {r2_biased/r2_honest:.2f}x\")\n",
        "else:\n",
        "    print(\"Inflation factor: not well-defined because honest RÂ² is ~0\")"
      ],
      "id": "68098031"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  Which approach has higher RÂ² OOS? Why?\n",
        "2.  Is the â€œRÂ² OOSâ€ from biased approach truly out-of-sample?\n",
        "3.  How much does look-ahead bias inflate performance in this example?\n",
        "4.  If you deployed the biased model in real-world, would it achieve the\n",
        "    high RÂ²?\n",
        "5.  Why is this bias subtle and easy to introduce accidentally?\n",
        "\n",
        "### Exercise 2.2: Visualising the Difference\n",
        "\n",
        "Letâ€™s visualize prediction accuracy over time for both approaches."
      ],
      "id": "f708a02c-ef97-49cf-9e08-8ae0995567fe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot predictions vs actuals for both methods\n",
        "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "# Honest walk-forward\n",
        "ax1.scatter(dates_honest, actual_honest * 100, alpha=0.5, label='Actual', s=30)\n",
        "ax1.scatter(dates_honest, pred_honest * 100, alpha=0.5, label='Predicted', s=30)\n",
        "ax1.set_ylabel('Return (%)', fontsize=11)\n",
        "ax1.set_title(f'Honest Walk-Forward Validation (RÂ² OOS = {r2_honest*100:.2f}%)', \n",
        "              fontsize=12, fontweight='bold')\n",
        "ax1.legend(fontsize=10)\n",
        "ax1.grid(alpha=0.3)\n",
        "ax1.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
        "\n",
        "# Biased full-sample\n",
        "ax2.scatter(dates_biased, actual_biased * 100, alpha=0.5, label='Actual', s=30)\n",
        "ax2.scatter(dates_biased, pred_biased * 100, alpha=0.5, label='Predicted', s=30, color='red')\n",
        "ax2.set_xlabel('Date', fontsize=11)\n",
        "ax2.set_ylabel('Return (%)', fontsize=11)\n",
        "ax2.set_title(f'Biased Full-Sample Training (Apparent RÂ² = {r2_biased*100:.2f}%)', \n",
        "              fontsize=12, fontweight='bold')\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(alpha=0.3)\n",
        "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ“Š Observation:\")\n",
        "print(\"   Biased approach shows tighter clustering (predictions closer to actuals)\")\n",
        "print(\"   This is artificialâ€”model 'saw' the future during training\")\n",
        "print(\"   Honest approach shows more scatter (realistic forecasting difficulty)\")"
      ],
      "id": "ae80aa41"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  Visually, which approach shows predictions closer to actuals?\n",
        "2.  Does tighter clustering mean better true forecasting ability?\n",
        "3.  In real deployment, which performance would you actually achieve?\n",
        "\n",
        "**Key insight**: Look-ahead bias makes predictions look artificially\n",
        "good. Only walk-forward validation reveals true forecasting ability.\n",
        "\n",
        "## Part 3: OLS vs Ridge Regression\n",
        "\n",
        "### Exercise 3.1: Creating Multicollinearity\n",
        "\n",
        "Letâ€™s create correlated predictors to see when ridge outperforms OLS."
      ],
      "id": "6bbe6791-961b-495d-b460-81f37546e300"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate highly correlated factors\n",
        "n = 240\n",
        "factor_a = np.random.normal(0, 0.03, n)\n",
        "factor_b = 0.7 * factor_a + 0.3 * np.random.normal(0, 0.03, n)  # Correlation ~0.7\n",
        "factor_c = 0.5 * factor_a + 0.5 * np.random.normal(0, 0.03, n)  # Correlation ~0.5\n",
        "\n",
        "# Target: weak linear relationship with factors (realistic signal strength)\n",
        "market_ret = (0.02 * factor_a + 0.01 * factor_b + 0.015 * factor_c + \n",
        "              np.random.normal(0.008, 0.04, n))\n",
        "\n",
        "# Create DataFrame\n",
        "data_multi = pd.DataFrame({\n",
        "    'market_next': market_ret,\n",
        "    'factor_a': factor_a,\n",
        "    'factor_b': factor_b,\n",
        "    'factor_c': factor_c\n",
        "})\n",
        "\n",
        "# Check correlations\n",
        "print(\"=== Predictor Correlations ===\\n\")\n",
        "print(data_multi[['factor_a', 'factor_b', 'factor_c']].corr().round(3))"
      ],
      "id": "e98a4d61"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  What are the correlations between predictors?\n",
        "2.  Why would high correlation create problems for OLS?\n",
        "3.  In factor prediction, do you expect factors to be correlated or\n",
        "    independent?\n",
        "\n",
        "### Exercise 3.2: Comparing OLS vs Ridge with Walk-Forward\n",
        "\n",
        "Now test both models using walk-forward validation."
      ],
      "id": "7cc7f01d-5c93-4257-90bf-4748ced52816"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "def walk_forward_compare_models(data, train_window=120):\n",
        "    \"\"\"\n",
        "    Compare OLS vs Ridge using walk-forward validation.\n",
        "    \"\"\"\n",
        "    pred_ols = []\n",
        "    pred_ridge = []\n",
        "    actuals = []\n",
        "    \n",
        "    for t in range(train_window, len(data)):\n",
        "        # Training data\n",
        "        train_start = t - train_window\n",
        "        X_train = data.iloc[train_start:t][['factor_a', 'factor_b', 'factor_c']].values\n",
        "        y_train = data.iloc[train_start:t]['market_next'].values\n",
        "        \n",
        "        # Test data\n",
        "        X_test = data.iloc[t:t+1][['factor_a', 'factor_b', 'factor_c']].values\n",
        "        y_test = data.iloc[t]['market_next']\n",
        "        \n",
        "        # OLS\n",
        "        model_ols = LinearRegression()\n",
        "        model_ols.fit(X_train, y_train)\n",
        "        pred_ols.append(model_ols.predict(X_test)[0])\n",
        "        \n",
        "        # Ridge (lambda=1.0)\n",
        "        model_ridge = Ridge(alpha=1.0)\n",
        "        model_ridge.fit(X_train, y_train)\n",
        "        pred_ridge.append(model_ridge.predict(X_test)[0])\n",
        "        \n",
        "        actuals.append(y_test)\n",
        "    \n",
        "    return np.array(pred_ols), np.array(pred_ridge), np.array(actuals)\n",
        "\n",
        "# Run comparison\n",
        "pred_ols, pred_ridge, actuals = walk_forward_compare_models(data_multi)\n",
        "\n",
        "# Calculate RÂ² OOS for both\n",
        "r2_ols = calc_r2_oos(actuals, pred_ols)\n",
        "r2_ridge = calc_r2_oos(actuals, pred_ridge)\n",
        "\n",
        "# Calculate directional accuracy\n",
        "def directional_accuracy(y_true, y_pred):\n",
        "    \"\"\"Calculate fraction of correct sign predictions.\"\"\"\n",
        "    return (np.sign(y_true) == np.sign(y_pred)).mean() * 100\n",
        "\n",
        "dir_acc_ols = directional_accuracy(actuals, pred_ols)\n",
        "dir_acc_ridge = directional_accuracy(actuals, pred_ridge)\n",
        "\n",
        "print(\"=== OLS vs Ridge Comparison (Walk-Forward) ===\\n\")\n",
        "print(f\"OLS:\")\n",
        "print(f\"  RÂ² OOS: {r2_ols*100:.2f}%\")\n",
        "print(f\"  Directional Accuracy: {dir_acc_ols:.2f}%\")\n",
        "print(f\"\\nRidge (Î»=1.0):\")\n",
        "print(f\"  RÂ² OOS: {r2_ridge*100:.2f}%\")\n",
        "print(f\"  Directional Accuracy: {dir_acc_ridge:.2f}%\")\n",
        "print(f\"\\nDifference:\")\n",
        "print(f\"  RÂ² improvement: {(r2_ridge - r2_ols)*100:.2f} pp\")\n",
        "print(f\"  Direction improvement: {dir_acc_ridge - dir_acc_ols:.2f} pp\")"
      ],
      "id": "2a7d5ab2"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  Which model performs better: OLS or ridge?\n",
        "2.  Is the improvement large or modest?\n",
        "3.  Why might ridge help when predictors are correlated?\n",
        "4.  What is the tradeoff ridge makes (hint: bias vs variance)?\n",
        "5.  If predictors were uncorrelated, would ridge still help?\n",
        "\n",
        "### Exercise 3.3: Effect of Regularization Strength\n",
        "\n",
        "Letâ€™s see how different Î» values affect performance."
      ],
      "id": "8fca1f4d-0248-44e0-8992-13a99f9b5580"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell assumes you have already run the OLS vs Ridge comparison above\n",
        "# (defines 'actuals' and 'calc_r2_oos').\n",
        "\n",
        "# Test multiple lambda values\n",
        "lambdas = [0.01, 0.1, 0.5, 1.0, 5.0, 10.0]\n",
        "results = []\n",
        "\n",
        "for lam in lambdas:\n",
        "    pred_ridge_temp = []\n",
        "    \n",
        "    for t in range(120, len(data_multi)):\n",
        "        train_start = t - 120\n",
        "        X_train = data_multi.iloc[train_start:t][['factor_a', 'factor_b', 'factor_c']].values\n",
        "        y_train = data_multi.iloc[train_start:t]['market_next'].values\n",
        "        X_test = data_multi.iloc[t:t+1][['factor_a', 'factor_b', 'factor_c']].values\n",
        "        \n",
        "        model = Ridge(alpha=lam)\n",
        "        model.fit(X_train, y_train)\n",
        "        pred_ridge_temp.append(model.predict(X_test)[0])\n",
        "    \n",
        "    r2 = calc_r2_oos(actuals, np.array(pred_ridge_temp))\n",
        "    results.append({'lambda': lam, 'R2_OOS': r2 * 100})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "print(\"=== Ridge Performance Across Î» Values ===\\n\")\n",
        "print(results_df.round(3))\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(results_df['lambda'], results_df['R2_OOS'], marker='o', linewidth=2, markersize=8, \n",
        "         label='Ridge (varies with Î»)')\n",
        "plt.axhline(y=r2_ols*100, color='red', linestyle='--', linewidth=2, label=f'OLS (RÂ²={r2_ols*100:.2f}%)')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('Regularization Strength (Î»)', fontsize=12)\n",
        "plt.ylabel('RÂ² OOS (%)', fontsize=12)\n",
        "plt.title('Ridge Performance vs Regularization Strength', fontsize=14, fontweight='bold')\n",
        "plt.legend(fontsize=11)\n",
        "plt.grid(alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "2c371751"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  What happens to RÂ² OOS as Î» increases?\n",
        "2.  Is there an optimal Î»? How would you find it?\n",
        "3.  What happens with Î»â†’0 (approaches OLS)?\n",
        "4.  What happens with Î»â†’âˆž (extreme shrinkage)?\n",
        "5.  How would you choose optimal Î» for Coursework 2?\n",
        "\n",
        "**Key insight**: Ridge provides stability when predictors correlate, but\n",
        "improvement is typically modest (1-2pp). Donâ€™t expect ridge to double\n",
        "your RÂ² OOS.\n",
        "\n",
        "## Part 4: Evaluation Metrics Deep Dive\n",
        "\n",
        "### Exercise 4.1: Understanding RÂ² OOS Components\n",
        "\n",
        "Letâ€™s decompose what RÂ² OOS actually measures."
      ],
      "id": "dbe6178d-c68a-4f1d-a58c-7726b47bfbbf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# This cell assumes you have already run the OLS vs Ridge section above\n",
        "# (defines 'actuals' and 'pred_ridge').\n",
        "\n",
        "# Using ridge predictions from earlier\n",
        "mean_pred = np.full_like(actuals, actuals.mean())  # Naive benchmark\n",
        "\n",
        "# Calculate errors\n",
        "error_model = actuals - pred_ridge\n",
        "error_benchmark = actuals - mean_pred\n",
        "\n",
        "# Sum of squared errors\n",
        "sse_model = (error_model ** 2).sum()\n",
        "sse_benchmark = (error_benchmark ** 2).sum()\n",
        "\n",
        "# RÂ² OOS\n",
        "r2_manual = 1 - (sse_model / sse_benchmark)\n",
        "\n",
        "print(\"=== RÂ² OOS Decomposition ===\\n\")\n",
        "print(f\"Actual return statistics:\")\n",
        "print(f\"  Mean: {actuals.mean()*100:.2f}% monthly\")\n",
        "print(f\"  Std Dev: {actuals.std()*100:.2f}%\")\n",
        "print(f\"\\nPrediction errors:\")\n",
        "print(f\"  Model SSE: {sse_model:.6f}\")\n",
        "print(f\"  Benchmark (mean) SSE: {sse_benchmark:.6f}\")\n",
        "print(f\"\\nRÂ² OOS calculation:\")\n",
        "print(f\"  RÂ² OOS = 1 - (Model SSE / Benchmark SSE)\")\n",
        "print(f\"  RÂ² OOS = 1 - ({sse_model:.6f} / {sse_benchmark:.6f})\")\n",
        "print(f\"  RÂ² OOS = {r2_manual*100:.2f}%\")\n",
        "print(f\"\\nInterpretation:\")\n",
        "print(f\"  Model reduces prediction error by {r2_manual*100:.1f}% vs naive mean\")"
      ],
      "id": "a18cf0ee"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  What is the â€œbenchmarkâ€ weâ€™re comparing against?\n",
        "2.  If model SSE \\> benchmark SSE, what is RÂ² OOS?\n",
        "3.  Why is historical mean a reasonable benchmark?\n",
        "4.  Can RÂ² OOS be negative? What does that mean?\n",
        "5.  Given monthly return volatility ~4% and mean ~0.8%, whatâ€™s the\n",
        "    maximum possible RÂ²?\n",
        "\n",
        "### Exercise 4.2: Directional Accuracy Analysis\n",
        "\n",
        "Letâ€™s examine directional accuracy in detail."
      ],
      "id": "c8d1f52a-7cef-4915-8491-e0294dd9b9cc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate detailed directional statistics\n",
        "signs_actual = np.sign(actuals)\n",
        "signs_pred = np.sign(pred_ridge)\n",
        "\n",
        "correct = (signs_actual == signs_pred)\n",
        "n_correct = correct.sum()\n",
        "n_total = len(actuals)\n",
        "\n",
        "# Breakdown by actual sign\n",
        "positive_months = (actuals > 0)\n",
        "negative_months = (actuals < 0)\n",
        "\n",
        "acc_positive = correct[positive_months].mean() * 100\n",
        "acc_negative = correct[negative_months].mean() * 100\n",
        "\n",
        "print(\"=== Directional Accuracy Analysis ===\\n\")\n",
        "print(f\"Overall directional accuracy: {dir_acc_ridge:.1f}%\")\n",
        "print(f\"  Correct predictions: {n_correct} / {n_total}\")\n",
        "print(f\"  Benchmark (random): 50.0%\")\n",
        "print(f\"\\nBreakdown:\")\n",
        "print(f\"  Positive months: {positive_months.sum()} occurrences\")\n",
        "print(f\"    Accuracy when predicting positive: {acc_positive:.1f}%\")\n",
        "print(f\"  Negative months: {negative_months.sum()} occurrences\")\n",
        "print(f\"    Accuracy when predicting negative: {acc_negative:.1f}%\")\n",
        "\n",
        "# Statistical significance (binomial test approximation - requires scipy)\n",
        "from scipy.stats import binom\n",
        "p_value = 1 - binom.cdf(n_correct - 1, n_total, 0.5)\n",
        "\n",
        "print(f\"\\nStatistical test:\")\n",
        "print(f\"  Null hypothesis: Directional accuracy = 50% (random guessing)\")\n",
        "print(f\"  p-value: {p_value:.4f}\")\n",
        "if p_value < 0.05:\n",
        "    print(f\"  âœ“ Reject null: Accuracy significantly > 50% (p < 0.05)\")\n",
        "else:\n",
        "    print(f\"  âœ— Cannot reject null: Accuracy not significantly different from 50%\")"
      ],
      "id": "08ecd282"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  Is directional accuracy significantly better than 50%?\n",
        "2.  Does model predict positive months better than negative months (or\n",
        "    vice versa)?\n",
        "3.  If directional accuracy is 54%, is that economically meaningful?\n",
        "4.  How many correct predictions would you need to be significant at 5%\n",
        "    level?\n",
        "5.  Which matters more for market timing: RÂ² OOS or directional\n",
        "    accuracy?\n",
        "\n",
        "## Part 5: Detecting Overfitting\n",
        "\n",
        "### Exercise 5.1: In-Sample vs Out-of-Sample Comparison\n",
        "\n",
        "Letâ€™s explicitly demonstrate overfitting by comparing in-sample and\n",
        "out-of-sample performance."
      ],
      "id": "eefc5a1a-5a20-4dc3-855f-1fe5a798787f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Using correlated factor data from earlier\n",
        "train_size = 120\n",
        "X_train = data_multi.iloc[:train_size][['factor_a', 'factor_b', 'factor_c']].values\n",
        "y_train = data_multi.iloc[:train_size]['market_next'].values\n",
        "\n",
        "# Fit models\n",
        "model_ols = LinearRegression()\n",
        "model_ols.fit(X_train, y_train)\n",
        "\n",
        "model_ridge = Ridge(alpha=1.0)\n",
        "model_ridge.fit(X_train, y_train)\n",
        "\n",
        "# In-sample predictions (training data)\n",
        "pred_train_ols = model_ols.predict(X_train)\n",
        "pred_train_ridge = model_ridge.predict(X_train)\n",
        "\n",
        "# Out-of-sample predictions (walk-forward on test data)\n",
        "pred_ols_oos, pred_ridge_oos, actuals_oos = walk_forward_compare_models(data_multi, train_window=120)\n",
        "\n",
        "# Calculate RÂ² for both\n",
        "r2_train_ols = r2_score(y_train, pred_train_ols)\n",
        "r2_train_ridge = r2_score(y_train, pred_train_ridge)\n",
        "r2_test_ols = calc_r2_oos(actuals_oos, pred_ols_oos)\n",
        "r2_test_ridge = calc_r2_oos(actuals_oos, pred_ridge_oos)\n",
        "\n",
        "print(\"=== In-Sample vs Out-of-Sample Performance ===\\n\")\n",
        "print(\"OLS:\")\n",
        "print(f\"  In-sample RÂ²: {r2_train_ols*100:.2f}%\")\n",
        "print(f\"  Out-of-sample RÂ²: {r2_test_ols*100:.2f}%\")\n",
        "print(f\"  Gap (overfitting): {(r2_train_ols - r2_test_ols)*100:.2f} pp\")\n",
        "print(f\"\\nRidge (Î»=1.0):\")\n",
        "print(f\"  In-sample RÂ²: {r2_train_ridge*100:.2f}%\")\n",
        "print(f\"  Out-of-sample RÂ²: {r2_test_ridge*100:.2f}%\")\n",
        "print(f\"  Gap (overfitting): {(r2_train_ridge - r2_test_ridge)*100:.2f} pp\")\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "models = ['OLS', 'Ridge']\n",
        "in_sample = [r2_train_ols*100, r2_train_ridge*100]\n",
        "out_sample = [r2_test_ols*100, r2_test_ridge*100]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "ax.bar(x - width/2, in_sample, width, label='In-Sample', alpha=0.8)\n",
        "ax.bar(x + width/2, out_sample, width, label='Out-of-Sample', alpha=0.8)\n",
        "\n",
        "ax.set_ylabel('RÂ² (%)', fontsize=12)\n",
        "ax.set_title('In-Sample vs Out-of-Sample RÂ² (Overfitting Gap)', fontsize=14, fontweight='bold')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(axis='y', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "819283e8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  Which model has a larger overfitting gap: OLS or ridge?\n",
        "2.  Why does ridge have a smaller gap?\n",
        "3.  If in-sample RÂ² = 15% but OOS RÂ² = 2%, what does that suggest?\n",
        "4.  Is some overfitting gap inevitable?\n",
        "5.  What size gap would make you worried about overfitting?\n",
        "\n",
        "### Exercise 5.2: Overfitting with Too Many Predictors\n",
        "\n",
        "Letâ€™s demonstrate what happens when we add too many predictors relative\n",
        "to sample size."
      ],
      "id": "35657be3-1586-4a6e-91a1-9d2d40e584a8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate many (mostly irrelevant) predictors\n",
        "n = 120  # Small sample\n",
        "n_relevant = 3\n",
        "n_noise = 15  # Many noise predictors\n",
        "\n",
        "# True predictors (small effect)\n",
        "X_relevant = np.random.normal(0, 0.03, (n, n_relevant))\n",
        "# Noise predictors\n",
        "X_noise = np.random.normal(0, 0.03, (n, n_noise))\n",
        "\n",
        "# Combine\n",
        "X_all = np.hstack([X_relevant, X_noise])\n",
        "\n",
        "# Target: only depends on first 3 predictors + noise\n",
        "y = (0.02 * X_relevant[:, 0] + 0.01 * X_relevant[:, 1] + \n",
        "     0.015 * X_relevant[:, 2] + np.random.normal(0.008, 0.04, n))\n",
        "\n",
        "# Split: first 80 for training, rest for testing\n",
        "train_size = 80\n",
        "X_train = X_all[:train_size]\n",
        "y_train = y[:train_size]\n",
        "X_test = X_all[train_size:]\n",
        "y_test = y[train_size:]\n",
        "\n",
        "# Fit OLS\n",
        "model_overfit = LinearRegression()\n",
        "model_overfit.fit(X_train, y_train)\n",
        "\n",
        "# In-sample and out-of-sample RÂ²\n",
        "r2_in = model_overfit.score(X_train, y_train)\n",
        "pred_test = model_overfit.predict(X_test)\n",
        "r2_out = calc_r2_oos(y_test, pred_test)\n",
        "\n",
        "print(\"=== Overfitting with Too Many Predictors ===\\n\")\n",
        "print(f\"Data:\")\n",
        "print(f\"  Training observations: {train_size}\")\n",
        "print(f\"  Number of predictors: {X_all.shape[1]}\")\n",
        "print(f\"  Observations per predictor: {train_size / X_all.shape[1]:.1f}\")\n",
        "print(f\"\\nPerformance:\")\n",
        "print(f\"  In-sample RÂ²: {r2_in*100:.2f}%\")\n",
        "print(f\"  Out-of-sample RÂ²: {r2_out*100:.2f}%\")\n",
        "print(f\"  Overfitting gap: {(r2_in - r2_out)*100:.2f} pp\")\n",
        "print(f\"\\nâš ï¸ Warning: {X_all.shape[1]} predictors with only {train_size} observations\")\n",
        "print(f\"   Severe overfittingâ€”model fits training noise, not signal\")"
      ],
      "id": "73a6b6eb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Discussion questions:**\n",
        "\n",
        "1.  How many observations per predictor do we have?\n",
        "2.  What happens to in-sample RÂ² with many predictors?\n",
        "3.  What happens to out-of-sample RÂ² with many predictors?\n",
        "4.  Whatâ€™s the overfitting gap?\n",
        "5.  For monthly data with 120-month training window, how many predictors\n",
        "    is reasonable?\n",
        "\n",
        "**Key insight**: With limited data, keep predictors â‰¤ 10. More\n",
        "predictors â†’ overfitting, even with regularization.\n",
        "\n",
        "## Part 6: Connecting to Coursework 2\n",
        "\n",
        "### What Youâ€™ve Learned vs.Â What Youâ€™ll Apply\n",
        "\n",
        "**Todayâ€™s lab explored concepts:**\n",
        "\n",
        "-   Walk-forward validation prevents look-ahead bias (honest OOS\n",
        "    testing)\n",
        "-   Look-ahead bias inflates performance dramatically (10+ pp)\n",
        "-   Ridge helps when predictors correlate, but improvement is modest\n",
        "    (1-2pp)\n",
        "-   RÂ² OOS = 2-3% is meaningful for monthly returns (signal is weak)\n",
        "-   Directional accuracy \\> 55% indicates timing skill\n",
        "-   Large in-sample vs OOS gap reveals overfitting (\\>5pp is concerning)\n",
        "\n",
        "**For Coursework 2 Option B, youâ€™ll:**\n",
        "\n",
        "1.  Use scaffold notebook to run walk-forward validation on JKP factor\n",
        "    data\n",
        "2.  Interpret RÂ² OOS results using understanding developed today\n",
        "3.  Compare OLS vs ridge and explain why one performs better\n",
        "4.  Write critical analysis discussing overfitting, realism of results,\n",
        "    limitations\n",
        "5.  Engage with Gu et al.Â (2020) on prediction literature\n",
        "6.  Assess whether prediction model would be exploitable after costs\n",
        "\n",
        "### Critical Analysis Questions to Ask\n",
        "\n",
        "When interpreting your Coursework 2 Option B results, ask:\n",
        "\n",
        "**Methodological questions:**\n",
        "\n",
        "-   Does walk-forward validation prevent look-ahead bias?\n",
        "-   Is training window size appropriate (10 years = 120 months typical)?\n",
        "-   Did I optimize any parameters using test data (introduces bias)?\n",
        "\n",
        "**Performance questions:**\n",
        "\n",
        "-   Is RÂ² OOS positive? (If no, model failed)\n",
        "-   Is RÂ² OOS realistic for monthly returns? (2-3% is meaningful, 10%+\n",
        "    suspicious)\n",
        "-   How does RÂ² OOS compare to Gu et al.Â (2020) benchmarks?\n",
        "-   Is directional accuracy significantly \\> 50%?\n",
        "\n",
        "**Model comparison questions:**\n",
        "\n",
        "-   Why did ridge beat/lose to OLS? (Multicollinearity? Sample size?)\n",
        "-   Is improvement economically meaningful or within sampling error?\n",
        "-   Are coefficients stable across training windows?\n",
        "\n",
        "**Overfitting questions:**\n",
        "\n",
        "-   Whatâ€™s the in-sample vs OOS gap? (\\>5pp indicates overfitting)\n",
        "-   Does RÂ² OOS decline in later test periods? (Suggests initial luck)\n",
        "-   How many predictors relative to training sample size? (Rule of\n",
        "    thumb: â‰¤10)\n",
        "\n",
        "**Economic questions:**\n",
        "\n",
        "-   Would strategy be profitable after transaction costs? (~3-4% annual\n",
        "    drag from monthly rebalancing)\n",
        "-   Whatâ€™s the certainty equivalent return gain? (Extension for advanced\n",
        "    students)\n",
        "-   Would practitioner implement this model in real portfolio?\n",
        "\n",
        "**Limitations to acknowledge:**\n",
        "\n",
        "-   Limited training sample (only 10-15 years)\n",
        "-   No regime switching (assumes stable relationships)\n",
        "-   Simplified asset allocation (binary long/short)\n",
        "-   Transaction cost estimates are rough\n",
        "-   Model may have overfit despite regularization\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1.  **Read Gu et al.Â (2020)**: â€œEmpirical Asset Pricing via Machine\n",
        "    Learningâ€ â€” Essential benchmark for prediction research\n",
        "2.  **Run scaffold notebook**: See what actual JKP factor prediction\n",
        "    outputs look like\n",
        "3.  **Choose factors wisely**: Use 5-8 factors (not 20) to avoid\n",
        "    overfitting\n",
        "4.  **Draft interpretation**: Practice writing critical analysis\n",
        "    paragraphs\n",
        "5.  **Office hours**: Ask conceptual questions about interpretation\n",
        "\n",
        "## Summary\n",
        "\n",
        "Todayâ€™s lab developed principles for market prediction:\n",
        "\n",
        "-   **Walk-forward validation** is essentialâ€”only way to prevent\n",
        "    look-ahead bias\n",
        "-   **Look-ahead bias** inflates performance 2-10ppâ€”makes useless models\n",
        "    look good\n",
        "-   **Ridge regression** stabilizes predictions when factors correlate,\n",
        "    but improvement is modest\n",
        "-   **RÂ² OOS = 2-3%** is meaningful for monthly returnsâ€”donâ€™t expect 20%\n",
        "-   **Directional accuracy \\> 55%** indicates timing skill, but must be\n",
        "    statistically tested\n",
        "-   **In-sample vs OOS gap** reveals overfittingâ€”gap \\>5pp is concerning\n",
        "-   **Critical interpretation** asks questions, contextualizes results,\n",
        "    acknowledges limitations\n",
        "\n",
        "**These principles enable critical analysis**â€”the 35% component of\n",
        "Coursework 2 Option B. Scaffold provides outputs; understanding provides\n",
        "interpretation. Focus your effort on thinking deeply about what results\n",
        "mean, not on perfecting code.\n",
        "\n",
        "**Week 10 + Week 11 together**: You now have principles for both\n",
        "coursework options (replication and prediction). Choose based on\n",
        "interestâ€”both are equally challenging at the interpretation level. Both\n",
        "require honest testing, realistic expectations, and critical thinking."
      ],
      "id": "765239fc-b5fd-41b3-b36f-ecc1550ddc2a"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "fin510",
      "display_name": "FIN510 Python",
      "language": "python",
      "path": "/Users/quinference/Library/Jupyter/kernels/fin510"
    }
  }
}