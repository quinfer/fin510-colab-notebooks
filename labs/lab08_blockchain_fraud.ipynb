{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Lab 8: Blockchain Transaction Analysis & Fraud Detection\"\n",
        "subtitle: \"Anomaly detection and network analytics for financial surveillance\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "execute:\n",
        "  echo: true\n",
        "  eval: false\n",
        "  warning: false\n",
        "  message: false\n",
        "---\n",
        "\n",
        "::: callout-note\n",
        "### Expected Time\n",
        "\n",
        "- FIN510: Exercises 1-2 â‰ˆ 75 min\n",
        "- FIN720: All exercises â‰ˆ 110 min\n",
        "- Directed learning extensions â‰ˆ 60 min\n",
        ":::\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/quinfer/fin510-colab-notebooks/blob/main/labs/lab08_blockchain_fraud.ipynb)\n",
        "\n",
        "## Before You Code: The Big Picture\n",
        "\n",
        "Blockchain's transparency is a double-edged sword: **all transactions are public**, but **identities are pseudonymous**. Can we detect fraud and money laundering at scale using data science?\n",
        "\n",
        "::: {.callout-note}\n",
        "## The Blockchain Transparency Paradox\n",
        "\n",
        "**The Promise:**\n",
        "1. **Transparency**: Every transaction recorded immutably on public ledger\n",
        "2. **Traceability**: Follow the money from source to destination\n",
        "3. **Auditability**: Regulators can inspect entire transaction history\n",
        "\n",
        "**The Reality:**\n",
        "- **Pseudonymity**: Addresses â‰  identities (hard to link to real people)\n",
        "- **Mixing services**: Tumblers obscure transaction trails\n",
        "- **Privacy coins**: Monero, Zcash use cryptography to hide amounts/recipients\n",
        "- **Volume**: Billions of transactions â†’ needle-in-haystack problem\n",
        "\n",
        "**The Scale of Crypto Crime:**\n",
        "- $14 billion in crypto scams/hacks in 2021 (Chainalysis 2022)\n",
        "- ~2.7% of crypto transaction volume illicit in 2022\n",
        "- Ransomware payments: $602M in 2021 (up 78% YoY)\n",
        "- North Korea's Lazarus Group: $1.7B stolen in 2022 (largest year ever)\n",
        "\n",
        "**Regulatory Response:**\n",
        "- **Travel Rule**: FATF requires identity sharing for transfers >$1000\n",
        "- **AML/KYC**: Exchanges must screen users, report suspicious activity\n",
        "- **OFAC sanctions**: Block transactions to/from sanctioned addresses\n",
        ":::\n",
        "\n",
        "### What You'll Build Today\n",
        "\n",
        "By the end of this lab, you will have:\n",
        "\n",
        "- âœ… Statistical anomaly detection (Z-scores, Mahalanobis distance)\n",
        "- âœ… Machine learning methods (Isolation Forest, Autoencoders)\n",
        "- âœ… Network analysis to detect fraud rings\n",
        "- âœ… Trade-off analysis (detection rate vs. false positives)\n",
        "- âœ… Understanding of real-world compliance challenges\n",
        "\n",
        "**Time estimate:** 75 minutes (FIN510) | 110 minutes (FIN720 with all exercises)\n",
        "\n",
        "::: {.callout-important}\n",
        "## Why This Matters\n",
        "Financial crime surveillance is a $100B+ industry (Bloomberg 2023). Traditional banks use rules-based systems (slow, high false positives). Your skills in anomaly detection and network analysis are directly applicable to FinCrime teams at banks, exchanges, and regulators.\n",
        ":::\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "- Analyze blockchain transaction data and identify suspicious patterns\n",
        "- Implement statistical anomaly detection methods for fraud screening\n",
        "- Apply machine learning algorithms (Isolation Forest, Autoencoders) for anomaly detection\n",
        "- Conduct network analysis to detect fraud rings and money laundering\n",
        "- Visualize transaction flows and suspicious subnetworks\n",
        "- Evaluate trade-offs between detection accuracy and false positive rates\n",
        "- Connect technical detection methods to regulatory compliance (AML/KYC)\n",
        "\n",
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from datetime import datetime, timedelta\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine learning\n",
        "try:\n",
        "    from sklearn.ensemble import IsolationForest\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    from sklearn.decomposition import PCA\n",
        "except ImportError:\n",
        "    print(\"Installing scikit-learn...\")\n",
        "    !pip install -q scikit-learn\n",
        "\n",
        "# Network analysis\n",
        "try:\n",
        "    import networkx as nx\n",
        "except ImportError:\n",
        "    print(\"Installing networkx...\")\n",
        "    !pip install -q networkx\n",
        "\n",
        "# Deep learning (for autoencoders)\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "    from tensorflow import keras\n",
        "    from tensorflow.keras import layers\n",
        "except ImportError:\n",
        "    print(\"Installing tensorflow...\")\n",
        "    !pip install -q tensorflow\n",
        "\n",
        "# Visualization settings\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "print(\"âœ“ Setup complete - ready for fraud detection analysis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: Transaction Data Analysis and Pattern Identification\n",
        "\n",
        "### Understanding Blockchain Transaction Structure\n",
        "\n",
        "Blockchain transactions differ fundamentally from traditional banking transactions. Rather than simple transfers between accounts, Bitcoin transactions consume \"unspent transaction outputs\" (UTXOs) and create new ones. Understanding this structure is essential for effective fraud detectionâ€”patterns that seem normal in account-based systems might indicate suspicious activity in UTXO-based systems.\n",
        "\n",
        "We'll analyze simulated Bitcoin-like transaction data capturing realistic patterns whilst avoiding issues with actual blockchain data (size, privacy, changing patterns). The simulation includes normal transactions and injected fraud examples.\n",
        "\n",
        "### Loading and Exploring Transaction Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate synthetic blockchain transaction data with fraud examples\n",
        "np.random.seed(42)\n",
        "\n",
        "# Normal transaction parameters\n",
        "n_normal = 9500\n",
        "normal_amounts = np.random.lognormal(mean=3, sigma=1.5, size=n_normal)\n",
        "normal_inputs = np.random.poisson(lam=1.5, size=n_normal) + 1\n",
        "normal_outputs = np.random.poisson(lam=1.8, size=n_normal) + 1\n",
        "normal_fees = normal_amounts * 0.001 + np.random.normal(0, 0.0001, n_normal)\n",
        "\n",
        "# Fraud transaction parameters (unusual patterns)\n",
        "n_fraud = 500\n",
        "fraud_amounts = np.random.lognormal(mean=6, sigma=2, size=n_fraud)  # Larger amounts\n",
        "fraud_inputs = np.random.poisson(lam=5, size=n_fraud) + 1  # More inputs (mixing)\n",
        "fraud_outputs = np.random.poisson(lam=8, size=n_fraud) + 1  # More outputs (distribution)\n",
        "fraud_fees = fraud_amounts * 0.005 + np.random.normal(0, 0.0005, n_fraud)  # Higher fees\n",
        "\n",
        "# Combine into DataFrame\n",
        "transactions = pd.DataFrame({\n",
        "    'amount': np.concatenate([normal_amounts, fraud_amounts]),\n",
        "    'n_inputs': np.concatenate([normal_inputs, fraud_inputs]),\n",
        "    'n_outputs': np.concatenate([normal_outputs, fraud_outputs]),\n",
        "    'fee': np.concatenate([normal_fees, fraud_fees]),\n",
        "    'is_fraud': np.concatenate([np.zeros(n_normal), np.ones(n_fraud)])\n",
        "})\n",
        "\n",
        "# Add temporal features\n",
        "base_time = datetime(2024, 1, 1)\n",
        "transactions['timestamp'] = [\n",
        "    base_time + timedelta(seconds=int(x)) \n",
        "    for x in np.random.uniform(0, 86400*30, len(transactions))\n",
        "]\n",
        "transactions = transactions.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "# Add derived features\n",
        "transactions['hour'] = transactions['timestamp'].dt.hour\n",
        "transactions['fee_rate'] = transactions['fee'] / transactions['amount']\n",
        "transactions['total_io'] = transactions['n_inputs'] + transactions['n_outputs']\n",
        "\n",
        "# Display summary\n",
        "print(\"=\"*70)\n",
        "print(\"BLOCKCHAIN TRANSACTION DATASET\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nTotal transactions: {len(transactions):,}\")\n",
        "print(f\"Fraud transactions: {int(transactions['is_fraud'].sum()):,} ({transactions['is_fraud'].mean()*100:.1f}%)\")\n",
        "print(f\"Normal transactions: {int((~transactions['is_fraud'].astype(bool)).sum()):,}\")\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"STATISTICAL SUMMARY\")\n",
        "print(\"-\"*70)\n",
        "print(transactions[['amount', 'n_inputs', 'n_outputs', 'fee', 'fee_rate']].describe())\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"SAMPLE TRANSACTIONS\")\n",
        "print(\"-\"*70)\n",
        "print(transactions.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Visualizing Transaction Patterns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "\n",
        "# Amount distribution (log scale)\n",
        "axes[0, 0].hist(np.log10(transactions[transactions['is_fraud']==0]['amount']), \n",
        "                bins=50, alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0, 0].hist(np.log10(transactions[transactions['is_fraud']==1]['amount']), \n",
        "                bins=50, alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0, 0].set_xlabel('Log10(Amount)')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "axes[0, 0].set_title('Transaction Amount Distribution', fontweight='bold')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(alpha=0.3)\n",
        "\n",
        "# Input count distribution\n",
        "axes[0, 1].hist(transactions[transactions['is_fraud']==0]['n_inputs'], \n",
        "                bins=range(0, 20), alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0, 1].hist(transactions[transactions['is_fraud']==1]['n_inputs'], \n",
        "                bins=range(0, 20), alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0, 1].set_xlabel('Number of Inputs')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].set_title('Transaction Inputs Distribution', fontweight='bold')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(alpha=0.3)\n",
        "\n",
        "# Output count distribution\n",
        "axes[0, 2].hist(transactions[transactions['is_fraud']==0]['n_outputs'], \n",
        "                bins=range(0, 25), alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0, 2].hist(transactions[transactions['is_fraud']==1]['n_outputs'], \n",
        "                bins=range(0, 25), alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0, 2].set_xlabel('Number of Outputs')\n",
        "axes[0, 2].set_ylabel('Frequency')\n",
        "axes[0, 2].set_title('Transaction Outputs Distribution', fontweight='bold')\n",
        "axes[0, 2].legend()\n",
        "axes[0, 2].grid(alpha=0.3)\n",
        "\n",
        "# Fee rate distribution\n",
        "axes[1, 0].hist(transactions[transactions['is_fraud']==0]['fee_rate']*100, \n",
        "                bins=50, alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[1, 0].hist(transactions[transactions['is_fraud']==1]['fee_rate']*100, \n",
        "                bins=50, alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[1, 0].set_xlabel('Fee Rate (%)')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Transaction Fee Rate Distribution', fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(alpha=0.3)\n",
        "\n",
        "# Hourly transaction volume\n",
        "hourly_counts = transactions.groupby('hour').size()\n",
        "hourly_fraud = transactions[transactions['is_fraud']==1].groupby('hour').size()\n",
        "axes[1, 1].bar(hourly_counts.index, hourly_counts.values, alpha=0.7, label='Total', color='blue')\n",
        "axes[1, 1].bar(hourly_fraud.index, hourly_fraud.values, alpha=0.9, label='Fraud', color='red')\n",
        "axes[1, 1].set_xlabel('Hour of Day')\n",
        "axes[1, 1].set_ylabel('Transaction Count')\n",
        "axes[1, 1].set_title('Hourly Transaction Patterns', fontweight='bold')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(alpha=0.3)\n",
        "\n",
        "# Scatter: Amount vs Total I/O\n",
        "axes[1, 2].scatter(transactions[transactions['is_fraud']==0]['amount'], \n",
        "                   transactions[transactions['is_fraud']==0]['total_io'],\n",
        "                   alpha=0.5, s=10, label='Normal', color='blue')\n",
        "axes[1, 2].scatter(transactions[transactions['is_fraud']==1]['amount'], \n",
        "                   transactions[transactions['is_fraud']==1]['total_io'],\n",
        "                   alpha=0.7, s=20, label='Fraud', color='red', marker='^')\n",
        "axes[1, 2].set_xlabel('Amount')\n",
        "axes[1, 2].set_ylabel('Total Inputs + Outputs')\n",
        "axes[1, 2].set_title('Amount vs Transaction Complexity', fontweight='bold')\n",
        "axes[1, 2].set_xscale('log')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate separation between normal and fraud\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FRAUD vs NORMAL COMPARISON\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for col in ['amount', 'n_inputs', 'n_outputs', 'fee_rate', 'total_io']:\n",
        "    normal_mean = transactions[transactions['is_fraud']==0][col].mean()\n",
        "    fraud_mean = transactions[transactions['is_fraud']==1][col].mean()\n",
        "    pct_diff = ((fraud_mean - normal_mean) / normal_mean) * 100\n",
        "    \n",
        "    print(f\"\\n{col}:\")\n",
        "    print(f\"  Normal mean: {normal_mean:.4f}\")\n",
        "    print(f\"  Fraud mean:  {fraud_mean:.4f}\")\n",
        "    print(f\"  Difference:  {pct_diff:+.1f}%\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 1)\n",
        "\n",
        "Write 200-250 words addressing:\n",
        "\n",
        "1. **Pattern Identification**: What transaction features distinguish fraud from normal activity in this dataset? How might real-world fraud patterns differ from these simulated examples?\n",
        "\n",
        "2. **Detection Challenges**: Why might simple rule-based detection (e.g., \"flag all transactions >$X\") miss sophisticated fraud whilst generating many false positives?\n",
        "\n",
        "3. **Temporal Patterns**: The hourly distribution shows some variation. How might criminals deliberately time transactions to avoid detection? What temporal features might improve fraud detection?\n",
        "\n",
        "## Exercise 2: Statistical and Machine Learning Anomaly Detection\n",
        "\n",
        "### Statistical Outlier Detection\n",
        "\n",
        "We'll start with simple statistical methods establishing baseline performance before implementing more sophisticated machine learning approaches."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Prepare features for anomaly detection\n",
        "feature_cols = ['amount', 'n_inputs', 'n_outputs', 'fee', 'fee_rate', 'total_io', 'hour']\n",
        "X = transactions[feature_cols].values\n",
        "y_true = transactions['is_fraud'].values\n",
        "\n",
        "# Standardize features (zero mean, unit variance)\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Method 1: Z-score based detection (univariate)\n",
        "def zscore_anomaly_detection(data, threshold=3):\n",
        "    \"\"\"\n",
        "    Detect anomalies using Z-scores (standard deviations from mean).\n",
        "    \n",
        "    Classic statistical method: flag transactions with extreme values on\n",
        "    any feature. Fast and interpretable, but assumes independence.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_samples, n_features)\n",
        "        Scaled transaction features (zero mean, unit variance)\n",
        "    threshold : float, default=3\n",
        "        Number of standard deviations to use as cutoff\n",
        "        Common choices: 2.5 (broader), 3 (standard), 4 (conservative)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    ndarray, shape (n_samples,)\n",
        "        Boolean array: True = anomaly, False = normal\n",
        "        \n",
        "    Notes\n",
        "    -----\n",
        "    **How it works:**\n",
        "    - Compute Z-score for each feature: z = (x - Î¼) / Ïƒ\n",
        "    - Flag transaction if |z| > threshold on ANY feature\n",
        "    - Uses \"any\" aggregation (union rule), not \"all\" (intersection)\n",
        "    \n",
        "    **Pros:**\n",
        "    - Fast: O(n Ã— d) where n = samples, d = features\n",
        "    - Interpretable: Can explain why each transaction flagged\n",
        "    - No training required: Works on new data instantly\n",
        "    \n",
        "    **Cons:**\n",
        "    - Assumes features independent (ignores correlations)\n",
        "    - Sensitive to outliers in training data (affects Î¼, Ïƒ)\n",
        "    - High false positive rate (flags ~0.3% if threshold=3)\n",
        "    \n",
        "    **Real-World Usage:**\n",
        "    - First-line screening for manual review queue\n",
        "    - Rule-based systems in traditional banks\n",
        "    - Often combined with domain rules (e.g., \"amount > $10k AND ...\")\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> X_scaled = scaler.fit_transform(transactions[features])\n",
        "    >>> anomalies = zscore_anomaly_detection(X_scaled, threshold=3)\n",
        "    >>> print(f\"Flagged {anomalies.sum()} / {len(anomalies)} transactions\")\n",
        "    Flagged 45 / 10000 transactions (0.45%)\n",
        "    \"\"\"\n",
        "    z_scores = np.abs(stats.zscore(data, axis=0, nan_policy='omit'))\n",
        "    # Flag if ANY feature exceeds threshold\n",
        "    anomalies = (z_scores > threshold).any(axis=1)\n",
        "    return anomalies\n",
        "\n",
        "# Method 2: Percentile-based detection (multivariate using Mahalanobis distance)\n",
        "def mahalanobis_anomaly_detection(data, threshold_percentile=95):\n",
        "    \"\"\"\n",
        "    Detect anomalies using Mahalanobis distance (multivariate outlier detection).\n",
        "    \n",
        "    Accounts for feature correlations by measuring distance in transformed space\n",
        "    where features are uncorrelated. More sophisticated than Z-scores.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    data : ndarray, shape (n_samples, n_features)\n",
        "        Scaled transaction features (zero mean, unit variance)\n",
        "    threshold_percentile : float, default=95\n",
        "        Percentile cutoff (e.g., 95 = flag top 5% most distant)\n",
        "        Higher percentile = more conservative (fewer flags)\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    tuple of (anomalies, distances)\n",
        "        anomalies : ndarray, shape (n_samples,), boolean flags\n",
        "        distances : ndarray, shape (n_samples,), Mahalanobis distances\n",
        "        \n",
        "    Notes\n",
        "    -----\n",
        "    **How it works:**\n",
        "    - Compute covariance matrix Î£ capturing feature correlations\n",
        "    - Mahalanobis distance: d = sqrt((x - Î¼)áµ€ Î£â»Â¹ (x - Î¼))\n",
        "    - Flag transactions with distances > percentile threshold\n",
        "    \n",
        "    **Why better than Z-scores:**\n",
        "    - Accounts for correlations (e.g., high amount + low fee = suspicious)\n",
        "    - Single distance metric (not \"any\" rule over features)\n",
        "    - Scale-invariant (like Z-scores, but multivariate)\n",
        "    \n",
        "    **Pros:**\n",
        "    - Captures multivariate patterns Z-scores miss\n",
        "    - Probabilistic interpretation (Chi-squared distribution)\n",
        "    - Lower false positive rate than Z-scores (for same recall)\n",
        "    \n",
        "    **Cons:**\n",
        "    - Assumes Gaussian distribution (fails on heavy tails)\n",
        "    - Requires covariance matrix inversion (unstable if d > n)\n",
        "    - Computationally slower: O(n Ã— dÂ²) vs O(n Ã— d) for Z-scores\n",
        "    \n",
        "    **Real-World Usage:**\n",
        "    - Second-line screening after Z-score pre-filter\n",
        "    - Works well when fraud exhibits correlated anomalies\n",
        "    - Used by payment processors (Visa, Mastercard)\n",
        "    \n",
        "    **Mathematical Note:**\n",
        "    Under Gaussian assumption, squared Mahalanobis distance follows Ï‡Â²(d)\n",
        "    distribution. Can use this for principled p-values instead of percentiles.\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> X_scaled = scaler.fit_transform(transactions[features])\n",
        "    >>> anomalies, distances = mahalanobis_anomaly_detection(X_scaled, threshold_percentile=95)\n",
        "    >>> print(f\"Flagged {anomalies.sum()} transactions\")\n",
        "    >>> print(f\"Max distance: {distances.max():.2f}\")\n",
        "    Flagged 500 transactions (top 5%)\n",
        "    Max distance: 12.34\n",
        "    \"\"\"\n",
        "    # Calculate covariance matrix\n",
        "    cov_matrix = np.cov(data, rowvar=False)\n",
        "    inv_cov = np.linalg.pinv(cov_matrix)\n",
        "    mean = np.mean(data, axis=0)\n",
        "    \n",
        "    # Calculate Mahalanobis distance for each point\n",
        "    diff = data - mean\n",
        "    mahal_dist = np.sqrt(np.sum(diff @ inv_cov * diff, axis=1))\n",
        "    \n",
        "    # Flag based on percentile threshold\n",
        "    threshold = np.percentile(mahal_dist, threshold_percentile)\n",
        "    anomalies = mahal_dist > threshold\n",
        "    \n",
        "    return anomalies, mahal_dist\n",
        "\n",
        "# Apply statistical methods\n",
        "print(\"=\"*70)\n",
        "print(\"STATISTICAL ANOMALY DETECTION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Z-score method\n",
        "z_anomalies = zscore_anomaly_detection(X_scaled, threshold=3)\n",
        "z_precision = y_true[z_anomalies].mean()\n",
        "z_recall = (y_true * z_anomalies).sum() / y_true.sum()\n",
        "z_fpr = ((1-y_true) * z_anomalies).sum() / (1-y_true).sum()\n",
        "\n",
        "print(f\"\\nZ-Score Method (threshold=3Ïƒ):\")\n",
        "print(f\"  Flagged: {z_anomalies.sum()} transactions ({z_anomalies.mean()*100:.1f}%)\")\n",
        "print(f\"  Precision: {z_precision:.3f} (of flagged, what % are fraud?)\")\n",
        "print(f\"  Recall: {z_recall:.3f} (of fraud, what % detected?)\")\n",
        "print(f\"  False Positive Rate: {z_fpr:.3f}\")\n",
        "\n",
        "# Mahalanobis method\n",
        "mahal_anomalies, mahal_dist = mahalanobis_anomaly_detection(X_scaled, threshold_percentile=95)\n",
        "mahal_precision = y_true[mahal_anomalies].mean()\n",
        "mahal_recall = (y_true * mahal_anomalies).sum() / y_true.sum()\n",
        "mahal_fpr = ((1-y_true) * mahal_anomalies).sum() / (1-y_true).sum()\n",
        "\n",
        "print(f\"\\nMahalanobis Distance Method (95th percentile threshold):\")\n",
        "print(f\"  Flagged: {mahal_anomalies.sum()} transactions ({mahal_anomalies.mean()*100:.1f}%)\")\n",
        "print(f\"  Precision: {mahal_precision:.3f}\")\n",
        "print(f\"  Recall: {mahal_recall:.3f}\")\n",
        "print(f\"  False Positive Rate: {mahal_fpr:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Isolation Forest Anomaly Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Isolation Forest - tree-based anomaly detection\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ISOLATION FOREST ANOMALY DETECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Train Isolation Forest\n",
        "iso_forest = IsolationForest(\n",
        "    n_estimators=100,\n",
        "    contamination=0.05,  # Expected proportion of anomalies\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "iso_forest.fit(X_scaled)\n",
        "\n",
        "# Predict anomalies (-1 for anomaly, 1 for normal)\n",
        "iso_predictions = iso_forest.predict(X_scaled)\n",
        "iso_anomalies = (iso_predictions == -1)\n",
        "\n",
        "# Calculate scores (more negative = more anomalous)\n",
        "iso_scores = iso_forest.score_samples(X_scaled)\n",
        "\n",
        "# Evaluate performance\n",
        "iso_precision = y_true[iso_anomalies].mean()\n",
        "iso_recall = (y_true * iso_anomalies).sum() / y_true.sum()\n",
        "iso_fpr = ((1-y_true) * iso_anomalies).sum() / (1-y_true).sum()\n",
        "\n",
        "print(f\"\\nIsolation Forest Results:\")\n",
        "print(f\"  Flagged: {iso_anomalies.sum()} transactions ({iso_anomalies.mean()*100:.1f}%)\")\n",
        "print(f\"  Precision: {iso_precision:.3f}\")\n",
        "print(f\"  Recall: {iso_recall:.3f}\")\n",
        "print(f\"  False Positive Rate: {iso_fpr:.3f}\")\n",
        "\n",
        "# Visualize anomaly scores\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Score distribution\n",
        "axes[0].hist(iso_scores[y_true==0], bins=50, alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0].hist(iso_scores[y_true==1], bins=50, alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0].set_xlabel('Anomaly Score (more negative = more anomalous)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Isolation Forest Anomaly Scores', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# ROC-like curve: varying threshold\n",
        "thresholds = np.percentile(iso_scores, range(0, 100, 5))\n",
        "precisions, recalls, fprs = [], [], []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    preds = (iso_scores < threshold)\n",
        "    if preds.sum() > 0:\n",
        "        precision = y_true[preds].mean()\n",
        "        recall = (y_true * preds).sum() / y_true.sum()\n",
        "        fpr = ((1-y_true) * preds).sum() / (1-y_true).sum()\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        fprs.append(fpr)\n",
        "\n",
        "axes[1].plot(fprs, recalls, marker='o', linewidth=2, markersize=4)\n",
        "axes[1].set_xlabel('False Positive Rate')\n",
        "axes[1].set_ylabel('True Positive Rate (Recall)')\n",
        "axes[1].set_title('Detection Trade-off Curve', fontweight='bold')\n",
        "axes[1].grid(alpha=0.3)\n",
        "axes[1].plot([0, 1], [0, 1], 'k--', alpha=0.3, label='Random')\n",
        "axes[1].legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Autoencoder-based Anomaly Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"AUTOENCODER ANOMALY DETECTION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Build autoencoder architecture\n",
        "input_dim = X_scaled.shape[1]\n",
        "encoding_dim = 4  # Bottleneck dimension\n",
        "\n",
        "# Encoder\n",
        "encoder = keras.Sequential([\n",
        "    layers.Dense(12, activation='relu', input_shape=(input_dim,)),\n",
        "    layers.Dense(8, activation='relu'),\n",
        "    layers.Dense(encoding_dim, activation='relu', name='encoding')\n",
        "])\n",
        "\n",
        "# Decoder\n",
        "decoder = keras.Sequential([\n",
        "    layers.Dense(8, activation='relu', input_shape=(encoding_dim,)),\n",
        "    layers.Dense(12, activation='relu'),\n",
        "    layers.Dense(input_dim, activation='linear')\n",
        "])\n",
        "\n",
        "# Complete autoencoder\n",
        "autoencoder = keras.Sequential([encoder, decoder])\n",
        "\n",
        "# Compile\n",
        "autoencoder.compile(\n",
        "    optimizer='adam',\n",
        "    loss='mse'\n",
        ")\n",
        "\n",
        "print(\"\\nAutoencoder Architecture:\")\n",
        "autoencoder.summary()\n",
        "\n",
        "# Train on normal transactions only (unsupervised approach)\n",
        "X_train_normal = X_scaled[y_true == 0]\n",
        "\n",
        "print(f\"\\nTraining on {len(X_train_normal)} normal transactions...\")\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    X_train_normal, X_train_normal,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.2,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Calculate reconstruction error for all transactions\n",
        "reconstructions = autoencoder.predict(X_scaled, verbose=0)\n",
        "reconstruction_errors = np.mean(np.square(X_scaled - reconstructions), axis=1)\n",
        "\n",
        "# Flag anomalies based on reconstruction error threshold (95th percentile)\n",
        "error_threshold = np.percentile(reconstruction_errors, 95)\n",
        "ae_anomalies = reconstruction_errors > error_threshold\n",
        "\n",
        "# Evaluate\n",
        "ae_precision = y_true[ae_anomalies].mean()\n",
        "ae_recall = (y_true * ae_anomalies).sum() / y_true.sum()\n",
        "ae_fpr = ((1-y_true) * ae_anomalies).sum() / (1-y_true).sum()\n",
        "\n",
        "print(f\"\\nAutoencoder Results (95th percentile threshold):\")\n",
        "print(f\"  Error threshold: {error_threshold:.4f}\")\n",
        "print(f\"  Flagged: {ae_anomalies.sum()} transactions ({ae_anomalies.mean()*100:.1f}%)\")\n",
        "print(f\"  Precision: {ae_precision:.3f}\")\n",
        "print(f\"  Recall: {ae_recall:.3f}\")\n",
        "print(f\"  False Positive Rate: {ae_fpr:.3f}\")\n",
        "\n",
        "# Visualize reconstruction errors\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Error distribution\n",
        "axes[0].hist(reconstruction_errors[y_true==0], bins=50, alpha=0.7, label='Normal', color='blue', edgecolor='black')\n",
        "axes[0].hist(reconstruction_errors[y_true==1], bins=50, alpha=0.7, label='Fraud', color='red', edgecolor='black')\n",
        "axes[0].axvline(error_threshold, color='green', linestyle='--', linewidth=2, label=f'Threshold ({error_threshold:.3f})')\n",
        "axes[0].set_xlabel('Reconstruction Error')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].set_title('Autoencoder Reconstruction Errors', fontweight='bold')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Training history\n",
        "axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
        "axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
        "axes[1].set_xlabel('Epoch')\n",
        "axes[1].set_ylabel('MSE Loss')\n",
        "axes[1].set_title('Autoencoder Training History', fontweight='bold')\n",
        "axes[1].legend()\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Comparing Detection Methods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Summary comparison\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"METHOD COMPARISON SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    'Method': ['Z-Score', 'Mahalanobis', 'Isolation Forest', 'Autoencoder'],\n",
        "    'Precision': [z_precision, mahal_precision, iso_precision, ae_precision],\n",
        "    'Recall': [z_recall, mahal_recall, iso_recall, ae_recall],\n",
        "    'FPR': [z_fpr, mahal_fpr, iso_fpr, ae_fpr],\n",
        "    'Flagged': [\n",
        "        z_anomalies.sum(),\n",
        "        mahal_anomalies.sum(),\n",
        "        iso_anomalies.sum(),\n",
        "        ae_anomalies.sum()\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\", comparison.to_string(index=False))\n",
        "\n",
        "# Calculate F1 scores\n",
        "comparison['F1'] = 2 * (comparison['Precision'] * comparison['Recall']) / (comparison['Precision'] + comparison['Recall'])\n",
        "\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"F1 Scores (harmonic mean of precision and recall):\")\n",
        "print(\"-\"*70)\n",
        "for idx, row in comparison.iterrows():\n",
        "    print(f\"  {row['Method']:20s}: {row['F1']:.3f}\")\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Precision-Recall comparison\n",
        "x_pos = np.arange(len(comparison))\n",
        "width = 0.35\n",
        "\n",
        "axes[0].bar(x_pos - width/2, comparison['Precision'], width, label='Precision', alpha=0.8)\n",
        "axes[0].bar(x_pos + width/2, comparison['Recall'], width, label='Recall', alpha=0.8)\n",
        "axes[0].set_xlabel('Method')\n",
        "axes[0].set_ylabel('Score')\n",
        "axes[0].set_title('Precision vs Recall by Method', fontweight='bold')\n",
        "axes[0].set_xticks(x_pos)\n",
        "axes[0].set_xticklabels(comparison['Method'], rotation=45, ha='right')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3, axis='y')\n",
        "\n",
        "# F1 score comparison\n",
        "axes[1].bar(x_pos, comparison['F1'], color='green', alpha=0.8)\n",
        "axes[1].set_xlabel('Method')\n",
        "axes[1].set_ylabel('F1 Score')\n",
        "axes[1].set_title('Overall Performance (F1 Score)', fontweight='bold')\n",
        "axes[1].set_xticks(x_pos)\n",
        "axes[1].set_xticklabels(comparison['Method'], rotation=45, ha='right')\n",
        "axes[1].grid(alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nðŸ’¡ Key Insights:\")\n",
        "print(\"  - No single method dominates all metrics\")\n",
        "print(\"  - Precision-recall trade-off varies by method\")\n",
        "print(\"  - Production systems often ensemble multiple methods\")\n",
        "print(\"  - False positive management is critical for operational deployment\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 2)\n",
        "\n",
        "Write 250-300 words addressing:\n",
        "\n",
        "1. **Method Comparison**: Which anomaly detection method performed best on this dataset? Consider both quantitative metrics (precision, recall, F1) and qualitative factors (interpretability, computational cost, ease of deployment).\n",
        "\n",
        "2. **Trade-offs**: The methods show different precision-recall trade-offs. For financial fraud detection, which is more importantâ€”catching all fraud (high recall) or minimizing false alarms (high precision)? How does this answer depend on context (transaction amount, customer relationship, regulatory requirements)?\n",
        "\n",
        "3. **Adversarial Adaptation**: Criminals who know the detection system might craft transactions to evade detection. Which method is most robust to adversarial attacks? Which is most vulnerable? How can detection systems adapt to evolving fraud tactics?\n",
        "\n",
        "## Exercise 3: Network Analysis for Fraud Detection\n",
        "\n",
        "### Building Transaction Networks\n",
        "\n",
        "Fraud often involves networksâ€”money mules, laundering chains, coordinated account takeovers. Graph analytics reveals patterns invisible to transaction-level analysis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate synthetic transaction network\n",
        "np.random.seed(42)\n",
        "\n",
        "# Create addresses (nodes)\n",
        "n_addresses = 200\n",
        "addresses = [f\"addr_{i:04d}\" for i in range(n_addresses)]\n",
        "\n",
        "# Generate transactions (edges) with different patterns\n",
        "edges = []\n",
        "\n",
        "# Normal transactions: random pairs with preferential attachment\n",
        "for _ in range(300):\n",
        "    # Preferential attachment: popular addresses receive more transactions\n",
        "    source = np.random.choice(addresses)\n",
        "    # Weighted choice favoring certain addresses (simulating exchanges, merchants)\n",
        "    weights = np.array([1 / (i+1) for i in range(n_addresses)])\n",
        "    weights = weights / weights.sum()\n",
        "    target = np.random.choice(addresses, p=weights)\n",
        "    \n",
        "    if source != target:\n",
        "        amount = np.random.lognormal(3, 1.5)\n",
        "        edges.append((source, target, amount, 'normal'))\n",
        "\n",
        "# Fraud pattern 1: Mixing service (one source, many outputs)\n",
        "mixer_source = addresses[0]\n",
        "for i in range(20):\n",
        "    target = np.random.choice(addresses[50:150])\n",
        "    amount = np.random.lognormal(4, 1)\n",
        "    edges.append((mixer_source, target, amount, 'mixing'))\n",
        "\n",
        "# Fraud pattern 2: Layering (sequential chain)\n",
        "chain_start = addresses[150]\n",
        "chain = [chain_start] + list(np.random.choice(addresses[151:170], size=10, replace=False))\n",
        "for i in range(len(chain)-1):\n",
        "    amount = np.random.lognormal(5, 0.5)\n",
        "    edges.append((chain[i], chain[i+1], amount, 'layering'))\n",
        "\n",
        "# Fraud pattern 3: Tight fraud ring (densely connected subgraph)\n",
        "fraud_ring = addresses[170:180]\n",
        "for i, addr1 in enumerate(fraud_ring):\n",
        "    for addr2 in fraud_ring[i+1:]:\n",
        "        if np.random.random() < 0.4:  # 40% connection probability within ring\n",
        "            amount = np.random.lognormal(4, 1)\n",
        "            edges.append((addr1, addr2, amount, 'fraud_ring'))\n",
        "\n",
        "# Convert to DataFrame\n",
        "edge_df = pd.DataFrame(edges, columns=['source', 'target', 'amount', 'pattern'])\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRANSACTION NETWORK\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nNodes (addresses): {n_addresses}\")\n",
        "print(f\"Edges (transactions): {len(edge_df)}\")\n",
        "print(\"\\nPattern distribution:\")\n",
        "print(edge_df['pattern'].value_counts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Metrics and Centrality Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Build directed graph\n",
        "G = nx.DiGraph()\n",
        "\n",
        "for _, row in edge_df.iterrows():\n",
        "    G.add_edge(row['source'], row['target'], \n",
        "               amount=row['amount'], pattern=row['pattern'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"NETWORK STATISTICS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nNodes: {G.number_of_nodes()}\")\n",
        "print(f\"Edges: {G.number_of_edges()}\")\n",
        "print(f\"Density: {nx.density(G):.4f}\")\n",
        "print(f\"Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
        "\n",
        "# Calculate centrality metrics\n",
        "degree_centrality = nx.degree_centrality(G)\n",
        "in_degree_centrality = nx.in_degree_centrality(G)\n",
        "out_degree_centrality = nx.out_degree_centrality(G)\n",
        "betweenness_centrality = nx.betweenness_centrality(G)\n",
        "\n",
        "# Identify suspicious nodes based on centrality\n",
        "print(\"\\n\" + \"-\"*70)\n",
        "print(\"TOP 10 NODES BY CENTRALITY\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "# Degree centrality (most connected)\n",
        "top_degree = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"\\nDegree Centrality (most connected):\")\n",
        "for addr, score in top_degree:\n",
        "    in_deg = G.in_degree(addr)\n",
        "    out_deg = G.out_degree(addr)\n",
        "    print(f\"  {addr}: {score:.4f} (in={in_deg}, out={out_deg})\")\n",
        "\n",
        "# Betweenness centrality (bridges/brokers)\n",
        "top_betweenness = sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "print(\"\\nBetweenness Centrality (bridges between communities):\")\n",
        "for addr, score in top_betweenness[:5]:  # Show top 5\n",
        "    print(f\"  {addr}: {score:.4f}\")\n",
        "\n",
        "# Out-degree centrality (distributors)\n",
        "top_out = sorted(out_degree_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
        "print(\"\\nOut-Degree Centrality (high output activity):\")\n",
        "for addr, score in top_out:\n",
        "    print(f\"  {addr}: {score:.4f} (out_degree={G.out_degree(addr)})\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Community Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Convert to undirected for community detection\n",
        "G_undirected = G.to_undirected()\n",
        "\n",
        "# Detect communities using Louvain method\n",
        "try:\n",
        "    import community as community_louvain\n",
        "except ImportError:\n",
        "    print(\"Installing python-louvain...\")\n",
        "    !pip install -q python-louvain\n",
        "    import community as community_louvain\n",
        "\n",
        "# Detect communities\n",
        "communities = community_louvain.best_partition(G_undirected)\n",
        "\n",
        "# Analyze community structure\n",
        "community_sizes = {}\n",
        "for node, comm_id in communities.items():\n",
        "    community_sizes[comm_id] = community_sizes.get(comm_id, 0) + 1\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"COMMUNITY DETECTION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nNumber of communities detected: {len(community_sizes)}\")\n",
        "print(f\"\\nCommunity sizes:\")\n",
        "for comm_id, size in sorted(community_sizes.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "    print(f\"  Community {comm_id}: {size} nodes\")\n",
        "\n",
        "# Identify suspicious communities (very small tight groups)\n",
        "suspicious_communities = [comm_id for comm_id, size in community_sizes.items() if 5 <= size <= 15]\n",
        "\n",
        "print(f\"\\nSuspicious communities (5-15 nodes): {len(suspicious_communities)}\")\n",
        "\n",
        "# Analyze fraud ring community (addresses 170-180)\n",
        "fraud_ring_communities = [communities[addr] for addr in addresses[170:180]]\n",
        "fraud_ring_comm_id = max(set(fraud_ring_communities), key=fraud_ring_communities.count)\n",
        "\n",
        "print(f\"\\nFraud ring detection:\")\n",
        "print(f\"  Known fraud ring addresses assigned to community {fraud_ring_comm_id}\")\n",
        "print(f\"  Community size: {community_sizes[fraud_ring_comm_id]} nodes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Visualize subnetwork focusing on high-activity nodes\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
        "\n",
        "# Subgraph 1: High degree nodes\n",
        "high_degree_nodes = [node for node, degree in G.degree() if degree >= 5]\n",
        "G_high_degree = G.subgraph(high_degree_nodes)\n",
        "\n",
        "pos1 = nx.spring_layout(G_high_degree, k=0.5, iterations=50, seed=42)\n",
        "node_colors1 = ['red' if node in addresses[0:1] or node in addresses[170:180] \n",
        "                else 'lightblue' for node in G_high_degree.nodes()]\n",
        "\n",
        "nx.draw_networkx_nodes(G_high_degree, pos1, node_color=node_colors1, \n",
        "                       node_size=300, alpha=0.8, ax=axes[0])\n",
        "nx.draw_networkx_edges(G_high_degree, pos1, alpha=0.3, arrows=True, \n",
        "                       arrowsize=10, ax=axes[0])\n",
        "nx.draw_networkx_labels(G_high_degree, pos1, font_size=7, ax=axes[0])\n",
        "\n",
        "axes[0].set_title('High-Activity Nodes Subnetwork\\n(Red = suspicious)', \n",
        "                  fontweight='bold')\n",
        "axes[0].axis('off')\n",
        "\n",
        "# Subgraph 2: Known fraud ring\n",
        "fraud_ring_extended = addresses[170:180]\n",
        "# Add neighbors for context\n",
        "for addr in fraud_ring_extended:\n",
        "    fraud_ring_extended.extend(list(G.neighbors(addr)))\n",
        "fraud_ring_extended = list(set(fraud_ring_extended))\n",
        "\n",
        "G_fraud_ring = G.subgraph(fraud_ring_extended)\n",
        "\n",
        "pos2 = nx.spring_layout(G_fraud_ring, k=0.8, iterations=50, seed=42)\n",
        "node_colors2 = ['red' if node in addresses[170:180] else 'lightgray' \n",
        "                for node in G_fraud_ring.nodes()]\n",
        "\n",
        "nx.draw_networkx_nodes(G_fraud_ring, pos2, node_color=node_colors2, \n",
        "                       node_size=400, alpha=0.8, ax=axes[1])\n",
        "nx.draw_networkx_edges(G_fraud_ring, pos2, alpha=0.4, arrows=True, \n",
        "                       arrowsize=12, ax=axes[1])\n",
        "nx.draw_networkx_labels(G_fraud_ring, pos2, font_size=8, ax=axes[1])\n",
        "\n",
        "axes[1].set_title('Fraud Ring and Neighbors\\n(Red = fraud ring members)', \n",
        "                  fontweight='bold')\n",
        "axes[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate graph metrics for fraud detection\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FRAUD PATTERN DETECTION SUMMARY\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Check if mixer was identified\n",
        "mixer_score = out_degree_centrality[mixer_source]\n",
        "print(f\"\\nMixing service detection:\")\n",
        "print(f\"  Address: {mixer_source}\")\n",
        "print(f\"  Out-degree centrality: {mixer_score:.4f} (rank: {sorted(out_degree_centrality.values(), reverse=True).index(mixer_score)+1})\")\n",
        "print(f\"  Out-degree: {G.out_degree(mixer_source)} (should be high for mixer)\")\n",
        "\n",
        "# Check if layering chain was identified\n",
        "chain_between = [betweenness_centrality[addr] for addr in chain[1:-1]]  # Middle nodes\n",
        "avg_chain_between = np.mean(chain_between)\n",
        "print(f\"\\nLayering chain detection:\")\n",
        "print(f\"  Average betweenness of chain nodes: {avg_chain_between:.4f}\")\n",
        "print(f\"  These nodes should have high betweenness (act as bridges)\")\n",
        "\n",
        "# Check if fraud ring was identified as community\n",
        "fraud_ring_in_community = sum(1 for addr in addresses[170:180] \n",
        "                               if communities[addr] == fraud_ring_comm_id)\n",
        "print(f\"\\nFraud ring detection:\")\n",
        "print(f\"  Fraud ring members in same community: {fraud_ring_in_community}/10\")\n",
        "print(f\"  Community density should be high for fraud rings\")\n",
        "\n",
        "print(\"\\nðŸ’¡ Network analytics revealed patterns invisible to transaction-level analysis!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 3)\n",
        "\n",
        "Write 200-250 words addressing:\n",
        "\n",
        "1. **Network Patterns**: How do mixing services, layering chains, and fraud rings manifest differently in network structure? Which centrality metrics are most effective for detecting each pattern?\n",
        "\n",
        "2. **Scalability**: Real blockchain networks have millions of nodes and billions of edges. What technical challenges arise when scaling these analyses to production systems? What approximations or sampling strategies might be necessary?\n",
        "\n",
        "3. **Privacy Trade-offs**: Network analysis on transparent blockchains enables sophisticated fraud detection but reveals transaction relationships. How should regulators and system designers balance fraud detection capabilities against user privacy? Are there techniques that preserve privacy whilst enabling effective monitoring?\n",
        "\n",
        "## Summary and Integration\n",
        "\n",
        "### What We've Learned\n",
        "\n",
        "Through these exercises, you've:\n",
        "\n",
        "1. **Analyzed blockchain transaction data** identifying distinguishing features of fraudulent versus normal activity\n",
        "\n",
        "2. **Implemented multiple anomaly detection methods** (statistical, Isolation Forest, autoencoders) and compared their effectiveness\n",
        "\n",
        "3. **Evaluated precision-recall trade-offs** understanding that no method perfectly detects all fraud without false positives\n",
        "\n",
        "4. **Applied network analysis** detecting fraud patterns invisible to transaction-level methodsâ€”mixing, layering, fraud rings\n",
        "\n",
        "5. **Visualized suspicious subnetworks** making abstract analytics interpretable for fraud analysts\n",
        "\n",
        "6. **Considered operational deployment** thinking beyond algorithms to false positive management, interpretability, and adversarial robustness\n",
        "\n",
        "### Connections to Course Themes\n",
        "\n",
        "- **Week 7 (Cryptocurrency)**: Blockchain transparency enables analyses impossible with traditional banking data, but pseudonymity limits identification\n",
        "\n",
        "- **Week 6 (Financial Inclusion)**: Fraud detection systems must balance security with accessâ€”overly aggressive detection excludes legitimate users, especially marginalized populations\n",
        "\n",
        "- **Week 3 (Platforms)**: Exchanges and payment platforms need fraud detection at scale, balancing automated systems with human review\n",
        "\n",
        "- **Week 2 (APIs)**: Real-world deployment requires integrating fraud detection with blockchain APIs, streaming transaction feeds, and alert systems\n",
        "\n",
        "### Critical Evaluation Framework\n",
        "\n",
        "When evaluating fraud detection systems:\n",
        "\n",
        "1. **Quantitative performance**: Precision, recall, F1 scores, false positive rates\n",
        "2. **Operational feasibility**: Computational cost, latency, scalability, integration complexity\n",
        "3. **Interpretability**: Can analysts understand why transactions were flagged?\n",
        "4. **Adversarial robustness**: How easily can criminals evade detection?\n",
        "5. **Fairness**: Does system discriminate against certain users or transaction types?\n",
        "6. **Regulatory compliance**: Does system meet AML/KYC requirements?\n",
        "\n",
        "### Assessment Preparation\n",
        "\n",
        "**FIN510 Coursework 2**: You can analyze transaction patterns in cryptocurrency or traditional payment data, applying anomaly detection or network analysis techniques from this lab.\n",
        "\n",
        "**FIN720**: Critical evaluation of blockchain fraud detection claims makes excellent reflective analysisâ€”compare promised benefits to empirical evidence, identify limitations, propose improvements based on academic research.\n",
        "\n",
        "### Further Exploration\n",
        "\n",
        "If interested in extending your analysis:\n",
        "\n",
        "- **Temporal dynamics**: How do fraud patterns evolve? Can models detect pattern shifts?\n",
        "- **Deep learning on graphs**: Graph neural networks (GNNs) for end-to-end fraud detection\n",
        "- **Privacy-preserving detection**: Federated learning, differential privacy for fraud detection without exposing transaction details\n",
        "- **Adversarial ML**: How do criminals evade detection? Can we make systems more robust?\n",
        "- **Cross-chain analysis**: Detecting fraud spanning multiple blockchains (bridges, wrapped tokens)\n",
        "\n",
        "---\n",
        "\n",
        "**Excellent work! You've implemented production-grade fraud detection techniques, connecting technical methods to operational realities and regulatory requirements.**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nlp_env",
      "language": "python",
      "display_name": "Python (nlp_env)",
      "path": "/Users/quinference/Library/Jupyter/kernels/nlp_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}