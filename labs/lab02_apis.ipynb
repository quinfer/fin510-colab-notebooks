{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Data Acquisition & APIs\n",
        "\n",
        "Build a Reliable Pipeline with Free APIs (Colab Version)\n",
        "\n",
        "> **Lab Versions**\n",
        ">\n",
        "> This is the **Colab version** using free APIs (yfinance). For\n",
        "> Bloomberg Terminal access in the Financial Innovation Lab, see [Lab 2:\n",
        "> Bloomberg Version](lab02_bloomberg.qmd).\n",
        ">\n",
        "> **Expected Time:**\n",
        ">\n",
        "> -   Core lab: â‰ˆ 60 minutes\n",
        "> -   Extensions (directed learning): +30â€“60 minutes\n",
        "\n",
        "<figure>\n",
        "<a\n",
        "href=\"https://colab.research.google.com/github/quinfer/fin510-colab-notebooks/blob/main/labs/lab02_apis.ipynb\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n",
        "<figcaption>Open in Colab</figcaption>\n",
        "</figure>\n",
        "\n",
        "> **Bloomberg Terminal Access**\n",
        ">\n",
        "> Ulster students have access to ~20 Bloomberg terminals in the\n",
        "> Financial Innovation Lab. If youâ€™re interested in comparing\n",
        "> professional-grade data with free APIs, check out the [Bloomberg\n",
        "> Terminal version of this lab](lab02_bloomberg.qmd).\n",
        "\n",
        "## Setup (Colabâ€‘only installs)"
      ],
      "id": "bf3c91d0-54bb-405c-8519-071f494b4d2e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    import yfinance, pandas, pandas_datareader\n",
        "except Exception:\n",
        "    !pip -q install yfinance pandas pandas-datareader"
      ],
      "id": "ee29f7b0"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Before You Code: The Big Picture\n",
        "\n",
        "Real-world financial data science starts with data acquisition. APIs are\n",
        "your gateway to market data, but theyâ€™re unreliableâ€”rate limits,\n",
        "outages, data quality issues. Professional systems need **resilience**:\n",
        "retry logic, fallback sources, validation, and logging.\n",
        "\n",
        "> **The Three Pillars of Reliable Data Pipelines**\n",
        ">\n",
        "> **1. Resilience** â†’ Handle API failures gracefully (retries,\n",
        "> fallbacks, synthetic data)  \n",
        "> **2. Validation** â†’ Check data quality before analysis (missing\n",
        "> values, outliers, gaps)  \n",
        "> **3. Provenance** â†’ Log data sources and transformations\n",
        "> (reproducibility, debugging)\n",
        ">\n",
        "> These arenâ€™t optional extrasâ€”theyâ€™re the difference between research\n",
        "> toys and production systems.\n",
        "\n",
        "### What Youâ€™ll Build Today\n",
        "\n",
        "By the end of this lab, you will have:\n",
        "\n",
        "-   âœ… Robust data fetching with retry logic and fallback sources\n",
        "-   âœ… Data quality validation pipeline (missing values, outliers, range\n",
        "    checks)\n",
        "-   âœ… Provenance logging (what data, from where, when, what issues\n",
        "    found)\n",
        "-   âœ… Clean return series ready for analysis\n",
        "\n",
        "**Time estimate:** â‰ˆ 60 minutes (plus optional extensions)\n",
        "\n",
        "> **Why This Matters**\n",
        ">\n",
        "> In projects and assessments, youâ€™ll often build models (or trading\n",
        "> rules) on top of a data pipeline. If your pipeline has silent bugs\n",
        "> (wrong dates, missing values, look-ahead bias), your entire analysis\n",
        "> is invalidated. Build good habits now.\n",
        "\n",
        "## Objectives\n",
        "\n",
        "-   Pull assets with yfinance; validate and log\n",
        "-   Handle missing values and outâ€‘ofâ€‘range returns\n",
        "-   Understand look-ahead bias and proper time alignment\n",
        "\n",
        "## Task 1 â€” Download and Validate\n",
        "\n",
        "This task implements a **resilient data pipeline**: try yfinance first,\n",
        "fall back to Stooq if it fails, use synthetic data as last resort. This\n",
        "three-tier approach ensures your analysis never stops due to API\n",
        "failures.\n",
        "\n",
        "> **ðŸ“š Professional Practice: Resilient API Design**\n",
        ">\n",
        "> **Why we need fallbacks:** - Free APIs have rate limits (yfinance:\n",
        "> ~2000 calls/hour) - APIs go down (maintenance, outages, deprecated\n",
        "> endpoints) - Network issues are common in cloud environments\n",
        ">\n",
        "> **Three-tier strategy:** 1. **Primary**: yfinance (best for US\n",
        "> equities, free, widely used) 2. **Fallback**: Stooq via\n",
        "> pandas-datareader (alternative free source) 3. **Last resort**:\n",
        "> Synthetic data (ensures code always runs for testing)\n",
        ">\n",
        "> This pattern appears in production systems at every scale.\n",
        "\n",
        "### Step 1: Define robust fetching functions"
      ],
      "id": "e42f65ce-8988-48f5-b559-7d6074b5d573"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, time, random\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "symbols = ['AAPL', 'MSFT', 'SPY']\n",
        "\n",
        "def get_close_from_yf(symbols, period='2y', tries=3):\n",
        "    \"\"\"\n",
        "    Fetch adjusted closing prices from Yahoo Finance with retry logic.\n",
        "    \n",
        "    Implements exponential backoff to handle rate limits gracefully. Returns\n",
        "    adjusted prices (splits/dividends applied) suitable for return calculations.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    symbols : list of str\n",
        "        Stock tickers (e.g., ['AAPL', 'MSFT'])\n",
        "    period : str, default='2y'\n",
        "        Data period: '1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y', 'ytd', 'max'\n",
        "    tries : int, default=3\n",
        "        Number of retry attempts before raising error\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Adjusted closing prices with DatetimeIndex and symbol columns\n",
        "        \n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If all retry attempts fail\n",
        "        \n",
        "    Notes\n",
        "    -----\n",
        "    - Uses `auto_adjust=True` to get split/dividend-adjusted prices\n",
        "    - Implements exponential backoff: waits 2, 4, 6 seconds between retries\n",
        "    - Handles yfinance MultiIndex columns (multiple symbols) vs single-symbol format\n",
        "    - Returns empty rows are filtered (only keeps days with at least one price)\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> prices = get_close_from_yf(['AAPL', 'MSFT'], period='1y')\n",
        "    >>> prices.shape\n",
        "    (252, 2)  # Roughly 252 trading days in a year\n",
        "    >>> prices.iloc[-1]  # Most recent closing prices\n",
        "    AAPL    182.50\n",
        "    MSFT    415.20\n",
        "    \"\"\"\n",
        "    last_err = None\n",
        "    \n",
        "    for attempt in range(tries):\n",
        "        try:\n",
        "            # Download with adjusted prices (splits/dividends applied)\n",
        "            df = yf.download(symbols, period=period, auto_adjust=True, \n",
        "                           progress=False, group_by='ticker', threads=True)\n",
        "            \n",
        "            # yfinance returns MultiIndex columns when fetching multiple symbols\n",
        "            if isinstance(df.columns, pd.MultiIndex):\n",
        "                # Extract 'Close' price for each symbol\n",
        "                closes = pd.concat(\n",
        "                    {sym: df[sym]['Close'] for sym in symbols if sym in df.columns.levels[0]}, \n",
        "                    axis=1\n",
        "                )\n",
        "                closes.columns = [c if isinstance(c, str) else c[0] for c in closes.columns]\n",
        "            else:\n",
        "                # Single symbol returns simple columns\n",
        "                closes = df['Close'].to_frame(symbols[0])\n",
        "            \n",
        "            # Only return if we got at least one non-empty row\n",
        "            if closes.dropna(how='all').shape[0] > 0:\n",
        "                return closes\n",
        "                \n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "        \n",
        "        # Exponential backoff with jitter to avoid thundering herd\n",
        "        time.sleep(2 * (attempt + 1) + random.random())\n",
        "    \n",
        "    raise RuntimeError(f\"yfinance download failed after {tries} tries: {last_err}\")\n",
        "\n",
        "\n",
        "def get_close_from_stooq(symbols, years=2):\n",
        "    \"\"\"\n",
        "    Fetch closing prices from Stooq via pandas-datareader (fallback source).\n",
        "    \n",
        "    Stooq provides historical price data for global markets. This function\n",
        "    serves as fallback when yfinance fails. Fetches data sequentially to\n",
        "    respect rate limits.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    symbols : list of str\n",
        "        Stock tickers (e.g., ['AAPL', 'MSFT'])\n",
        "    years : int, default=2\n",
        "        Number of years of historical data to fetch\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Closing prices with DatetimeIndex (sorted) and symbol columns\n",
        "        \n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If no symbols successfully fetched\n",
        "        \n",
        "    Notes\n",
        "    -----\n",
        "    - Fetches symbols sequentially (not parallel) to respect rate limits\n",
        "    - Waits 0.4 seconds between requests\n",
        "    - Silently skips symbols that fail (rather than stopping entire process)\n",
        "    - Returns only successfully fetched symbols\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> prices = get_close_from_stooq(['AAPL'], years=1)\n",
        "    >>> prices.index.name\n",
        "    'Date'\n",
        "    \"\"\"\n",
        "    from datetime import datetime, timedelta\n",
        "    from pandas_datareader import data as web\n",
        "    \n",
        "    # Define date range\n",
        "    end = datetime.today()\n",
        "    start = end - timedelta(days=365 * years + 14)  # Extra days for weekends/holidays\n",
        "    \n",
        "    series = []\n",
        "    for sym in symbols:\n",
        "        try:\n",
        "            # Fetch from Stooq and extract 'Close' column\n",
        "            s = web.DataReader(sym, 'stooq', start, end)['Close'].sort_index()\n",
        "            s.name = sym\n",
        "            series.append(s)\n",
        "            \n",
        "            # Respectful rate limiting\n",
        "            time.sleep(0.4)\n",
        "        except Exception:\n",
        "            # Skip symbols that fail rather than stopping entire fetch\n",
        "            pass\n",
        "    \n",
        "    if not series:\n",
        "        raise RuntimeError(\"stooq fallback returned no data\")\n",
        "    \n",
        "    return pd.concat(series, axis=1)\n",
        "\n",
        "\n",
        "def synthetic_prices(symbols, periods=252, mu=0.0004, sigma=0.012):\n",
        "    \"\"\"\n",
        "    Generate synthetic price series using geometric Brownian motion.\n",
        "    \n",
        "    This is a last-resort fallback when all real APIs fail. Useful for\n",
        "    testing and development when APIs are unavailable. NOT FOR ANALYSIS.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    symbols : list of str\n",
        "        Symbol names for columns (can be anything)\n",
        "    periods : int, default=252\n",
        "        Number of business days to generate (~1 year of trading days)\n",
        "    mu : float, default=0.0004\n",
        "        Daily drift (mean return): 0.0004 â‰ˆ 10% annual\n",
        "    sigma : float, default=0.012\n",
        "        Daily volatility: 0.012 â‰ˆ 19% annual volatility\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Synthetic prices starting at 100, with business day index\n",
        "        \n",
        "    Notes\n",
        "    -----\n",
        "    - Uses fixed seed (42) for reproducibility\n",
        "    - Generates prices via: P(t) = 100 * exp(cumsum(returns))\n",
        "    - Returns ~ Normal(mu, sigma) independently across symbols\n",
        "    - Index: business days ending at today\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> synth = synthetic_prices(['SYN1', 'SYN2'], periods=10)\n",
        "    >>> synth.shape\n",
        "    (10, 2)\n",
        "    >>> (synth.iloc[-1] / synth.iloc[0]).mean()  # Typical growth\n",
        "    1.04  # Roughly 4% over 10 days\n",
        "    \n",
        "    Warnings\n",
        "    --------\n",
        "    DO NOT use for actual analysis. This is for code testing only.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(42)  # Fixed seed for reproducibility\n",
        "    \n",
        "    # Business day index ending today\n",
        "    dates = pd.bdate_range(end=pd.Timestamp.today().normalize(), periods=periods)\n",
        "    \n",
        "    # Generate returns: Normal(mu, sigma)\n",
        "    shocks = rng.normal(mu, sigma, size=(len(dates), len(symbols)))\n",
        "    \n",
        "    # Convert to prices: P(t) = P(0) * exp(cumsum(returns))\n",
        "    levels = 100 * np.exp(np.cumsum(shocks, axis=0))\n",
        "    \n",
        "    return pd.DataFrame(levels, index=dates, columns=symbols)"
      ],
      "id": "7e3aa24c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Execute three-tier fetching strategy"
      ],
      "id": "d5c84fa5-aa2e-475b-b291-a26bd7ac258b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Try primary source (yfinance) ===\n",
        "try:\n",
        "    prices = get_close_from_yf(symbols)\n",
        "    source = 'yfinance'\n",
        "    print(f\"âœ” Successfully fetched from yfinance\")\n",
        "    \n",
        "except Exception as e1:\n",
        "    print(f\"âš  yfinance failed: {e1}\")\n",
        "    \n",
        "    # === Try fallback source (Stooq) ===\n",
        "    try:\n",
        "        prices = get_close_from_stooq(symbols)\n",
        "        source = 'stooq (pandas-datareader)'\n",
        "        print(f\"âœ” Successfully fetched from Stooq fallback\")\n",
        "        \n",
        "    except Exception as e2:\n",
        "        print(f\"âš  Stooq failed: {e2}\")\n",
        "        \n",
        "        # === Last resort: synthetic data ===\n",
        "        prices = synthetic_prices(symbols)\n",
        "        source = f'synthetic (fallback due to API failures)'\n",
        "        print(f\"âš  Using synthetic data (NOT for real analysis!)\")\n",
        "\n",
        "# Display first and last few rows\n",
        "print(f\"\\nData shape: {prices.shape}\")\n",
        "print(f\"\\nFirst 3 rows:\")\n",
        "print(prices.head(3))\n",
        "print(f\"\\nLast 3 rows:\")\n",
        "print(prices.tail(3))"
      ],
      "id": "522a6b48"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **What Just Happened?**\n",
        ">\n",
        "> The try-except cascade ensures you **always get data**, even if APIs\n",
        "> fail. In production, youâ€™d add alerts when fallbacks activate so\n",
        "> engineers can investigate the primary failure.\n",
        "\n",
        "### Step 3: Validate data quality"
      ],
      "id": "d104755f-074e-42e0-bf56-5f9c64b25024"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Build provenance log ===\n",
        "log = {}\n",
        "log['source'] = source\n",
        "log['symbols_requested'] = len(symbols)\n",
        "log['symbols_received'] = len(prices.columns)\n",
        "log['date_range'] = f\"{prices.index[0]} to {prices.index[-1]}\"\n",
        "log['trading_days'] = len(prices)\n",
        "\n",
        "# === Check for missing prices ===\n",
        "log['missing_prices'] = int(prices.isna().sum().sum())\n",
        "log['missing_pct'] = f\"{(prices.isna().sum().sum() / prices.size * 100):.2f}%\"\n",
        "\n",
        "# === Calculate returns and check quality ===\n",
        "rets = prices.pct_change()\n",
        "log['missing_returns'] = int(rets.isna().sum().sum())\n",
        "\n",
        "# === Flag outliers (|return| > 20% = likely data error or halt) ===\n",
        "log['out_of_range'] = int((rets.abs() > 0.2).sum().sum())\n",
        "\n",
        "# Display log\n",
        "import json\n",
        "print(\"\\n=== Data Quality Log ===\")\n",
        "print(json.dumps(log, indent=2))\n",
        "\n",
        "# === Quality gate ===\n",
        "if prices.dropna(how='all').shape[0] > 0:\n",
        "    print(f\"\\nâœ” Data source: {source}\")\n",
        "    print(f\"âœ” Download and validation checks passed\")\n",
        "else:\n",
        "    print(f\"\\nâš  Warning: no data returned from any source\")"
      ],
      "id": "330fc49a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Quality Checks Explained**\n",
        ">\n",
        "> -   **Missing prices**: Gaps in time series (holidays, halts,\n",
        ">     delisting)\n",
        "> -   **Missing returns**: First row is always NaN (no prior price to\n",
        ">     compare)\n",
        "> -   **Out-of-range**: \\|return\\| \\> 20% often indicates data errors,\n",
        ">     stock splits, or trading halts\n",
        ">\n",
        "> **Rule of thumb:** \\<1% missing is acceptable, \\>5% requires\n",
        "> investigation\n",
        ">\n",
        "> **Checkpoint:** Look at your log. Which quality issues did you find?\n",
        "> How would you handle them differently for a production system\n",
        "> vs.Â academic analysis?\n",
        "\n",
        "## Task 2 â€” Clean and Save\n",
        "\n",
        "Raw data always has issues. Professional practice: **clean\n",
        "conservatively** and **document decisions**.\n",
        "\n",
        "> **Cleaning Strategy**\n",
        ">\n",
        "> 1.  **Drop NaN rows** - Canâ€™t calculate returns without prices\n",
        "> 2.  **Clip outliers** - Cap extreme values at Â±20% (likely errors or\n",
        ">     halts)\n",
        "> 3.  **Save intermediate output** - Enables reproducibility and\n",
        ">     debugging\n",
        "> 4.  **Document transformations** - What did you change and why?\n",
        "\n",
        "### Step 1: Clean the data"
      ],
      "id": "a60ec0fe-a4bb-43fb-813c-dae88d940c10"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Remove rows where all returns are missing ===\n",
        "clean = rets.dropna()\n",
        "\n",
        "print(f\"Original shape: {rets.shape}\")\n",
        "print(f\"After dropna: {clean.shape}\")\n",
        "print(f\"Rows removed: {rets.shape[0] - clean.shape[0]}\")\n",
        "\n",
        "# === Clip extreme values (conservative approach) ===\n",
        "# Instead of deleting outliers, cap them at reasonable limits\n",
        "# -20% to +20% captures 99%+ of normal daily returns\n",
        "clean_clipped = clean.clip(lower=-0.2, upper=0.2)\n",
        "\n",
        "# Count how many values were clipped\n",
        "clipped_low = (clean < -0.2).sum().sum()\n",
        "clipped_high = (clean > 0.2).sum().sum()\n",
        "\n",
        "print(f\"\\n=== Outlier Treatment ===\")\n",
        "print(f\"Values clipped at lower bound (-20%): {clipped_low}\")\n",
        "print(f\"Values clipped at upper bound (+20%): {clipped_high}\")\n",
        "\n",
        "# Display sample\n",
        "print(f\"\\nCleaned returns (last 5 days):\")\n",
        "print(clean_clipped.tail())"
      ],
      "id": "c480ef98"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Why Clip Instead of Delete?**\n",
        ">\n",
        "> -   **Deleting** outliers shortens your time series (breaks\n",
        ">     continuity)\n",
        "> -   **Clipping** preserves all dates while limiting extreme values\n",
        "> -   For academic analysis, document which approach you use and why\n",
        "\n",
        "### Step 2: Save cleaned data"
      ],
      "id": "d97ffa82-ca21-41a3-ac6a-f3662a7616e2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Save to CSV for later use ===\n",
        "clean_clipped.to_csv('returns_clean.csv')\n",
        "\n",
        "# === Verify file was created ===\n",
        "import os\n",
        "if os.path.exists('returns_clean.csv'):\n",
        "    file_size = os.path.getsize('returns_clean.csv')\n",
        "    print(f\"âœ” Saved returns_clean.csv ({file_size:,} bytes)\")\n",
        "else:\n",
        "    print(\"âš  Warning: File not created\")\n",
        "\n",
        "# === Document the cleaning process ===\n",
        "cleaning_log = {\n",
        "    'rows_original': rets.shape[0],\n",
        "    'rows_after_dropna': clean.shape[0],\n",
        "    'values_clipped_low': int(clipped_low),\n",
        "    'values_clipped_high': int(clipped_high),\n",
        "    'clip_bounds': '[-20%, +20%]',\n",
        "    'output_file': 'returns_clean.csv'\n",
        "}\n",
        "\n",
        "print(f\"\\n=== Cleaning Summary ===\")\n",
        "print(json.dumps(cleaning_log, indent=2))"
      ],
      "id": "92208b56"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Deliverable:** Write a short note (100-150 words) describing: - What\n",
        "issues you found (missing values, outliers) - How you handled them\n",
        "(dropna, clipping) - Why these choices are appropriate for financial\n",
        "return data - What trade-offs you made (e.g., information loss\n",
        "vs.Â robustness)\n",
        "\n",
        "> **Troubleshooting**\n",
        ">\n",
        "> -   API download empty: try fewer symbols or shorter period.\n",
        "> -   Many outliers: inspect corporate actions/adjustments; consider\n",
        ">     `auto_adjust=True`.\n",
        "> -   CSV not found: ensure current working directory permissions in\n",
        ">     Colab.\n",
        "\n",
        "> **Further Reading (Hilpisch 2019)**\n",
        ">\n",
        "> -   See: [Hilpisch Code Resources](../resources/hilpisch-code.qmd) â€”\n",
        ">     Week 2\n",
        "> -   Chapter 13 (ML pipelines) shows endâ€‘toâ€‘end workflows (features â†’\n",
        ">     pipeline â†’ evaluation) you can mirror with timeâ€‘aware splits.\n",
        "\n",
        "## Miniâ€‘Task â€” JKP Sample (Factor dataset primer)\n",
        "\n",
        "This short exercise previews a factor dataset (JKP). Load a small sample\n",
        "CSV, compute quick stats, and (optionally) run a oneâ€‘line CAPM alpha."
      ],
      "id": "147b9366-385e-4011-bf35-11ac713ead32"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# JKP sample (course mirror) â€” small monthly slice with MKT, SMB, HML, MOM\n",
        "import pandas as pd, os\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Prefer local file during site build; fall back to raw GitHub if needed\n",
        "local_path = os.path.join('..','resources','jkp-sample.csv')\n",
        "if os.path.exists(local_path):\n",
        "    jkp = pd.read_csv(local_path, parse_dates=['date']).set_index('date').sort_index()\n",
        "else:\n",
        "    # Use public notebooks repo URL for Colab\n",
        "    url = \"https://raw.githubusercontent.com/quinfer/fin510-colab-notebooks/main/resources/jkp-sample.csv\"\n",
        "    jkp = pd.read_csv(url, parse_dates=['date']).set_index('date').sort_index()\n",
        "\n",
        "# Summary stats and quick cumulative return for MOM\n",
        "summary = jkp[['MKT','SMB','HML','MOM']].describe().round(3)\n",
        "cum = (1 + jkp['MOM']).cumprod() - 1\n",
        "summary.tail(3), cum.tail()\n",
        "\n",
        "# Optional: CAPM alpha (no HAC here â€” use HAC in the assessment)\n",
        "ls = jkp['MOM'].dropna()\n",
        "mkt = jkp['MKT'].reindex(ls.index)\n",
        "capm = sm.OLS(ls, sm.add_constant(mkt)).fit()\n",
        "float(capm.params['const']), float(capm.tvalues['const'])"
      ],
      "id": "25dfbe7b"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Notes - In the assessment you will use a larger CSV downloaded from the\n",
        "JKP portal and apply HAC standard errors. - Keep scope tight (few\n",
        "factors, limited window) and focus on quality of evidence.\n",
        "\n",
        "## Quick Leakage Check (Practice)"
      ],
      "id": "103e839e-f44d-4a4e-bd88-b7afaca80425"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure prediction tasks shift the target correctly\n",
        "import pandas as pd\n",
        "\n",
        "# Intentionally wrong design (no shift) for demonstration\n",
        "X_wrong = jkp[['MKT','SMB','HML','MOM']].dropna()\n",
        "y_next   = jkp['MOM'].shift(-1)               # next-month target\n",
        "\n",
        "# Overlap of indices indicates potential leakage if you don't drop/shift properly\n",
        "overlap = X_wrong.index.intersection(y_next.dropna().index)\n",
        "print(\"Potential leakage rows with wrong design:\", len(overlap))\n",
        "\n",
        "# Correct design: predictors at t, target at t+1 â†’ align and drop NA\n",
        "X = jkp[['MKT','SMB','HML']].shift(0)\n",
        "y = jkp['MOM'].shift(-1)\n",
        "df = pd.concat([X, y.rename('y')], axis=1).dropna()\n",
        "print(\"Rows after proper shift/drop:\", len(df))"
      ],
      "id": "9eb31ece"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "fin510",
      "display_name": "FIN510 Python",
      "language": "python",
      "path": "/Users/quinference/Library/Jupyter/kernels/fin510"
    }
  }
}