{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Lab 10: Production ML Pipelines & Rigorous Backtesting\"\n",
        "subtitle: \"From research prototype to production-ready factor models\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "    number-sections: true\n",
        "execute:\n",
        "  echo: true\n",
        "  warning: false\n",
        "  message: false\n",
        "---\n",
        "\n",
        "::: callout-note\n",
        "### Expected Time\n",
        "\n",
        "- FIN510: Exercises 1-2 ≈ 75 min\n",
        "- FIN720: All exercises ≈ 100 min\n",
        "- Directed learning extensions ≈ 60 min\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "This lab extends concepts from the JKP factor replication lab. Familiarity with factor models and portfolio construction is helpful but not required.\n",
        ":::\n",
        "\n",
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/quinfer/fin510-colab-notebooks/blob/main/labs/lab10_pipelines.ipynb)\n",
        "\n",
        "## Before You Code: The Big Picture\n",
        "\n",
        "**Research code ≠ Production code.** Your Jupyter notebook with 85% backtest Sharpe ratio? It won't survive first contact with live markets. Here's how to build **production-grade** ML pipelines that actually work.\n",
        "\n",
        "::: {.callout-note}\n",
        "## The Research-to-Production Gap\n",
        "\n",
        "**Why Models Fail in Production:**\n",
        "1. **Look-ahead bias**: Used future data to make past predictions\n",
        "2. **Overfitting**: Optimized on test data, no true holdout set\n",
        "3. **Data drift**: Training distribution ≠ production distribution\n",
        "4. **Multiple testing**: Tried 100 features, reported the 5 that worked\n",
        "5. **Leakage**: Features contain information not available at prediction time\n",
        "\n",
        "**The Evidence:**\n",
        "- **70% of ML projects fail to deploy** (Gartner 2021)\n",
        "- **90% of deployed models underperform expectations** (Algorithmia 2020)\n",
        "- Average Sharpe ratio decline: 0.5 → 0.1 from backtest to live (industry estimates)\n",
        "\n",
        "**What Separates Winners from Losers:**\n",
        "- **Rigorous validation**: CPCV, PBO, embargo periods, multiple testing corrections\n",
        "- **Temporal correctness**: Strict point-in-time data, no future information\n",
        "- **Production monitoring**: Drift detection, model versioning, automated rollback\n",
        "- **Documentation**: Reproducible, auditable, explainable\n",
        ":::\n",
        "\n",
        "### What You'll Build Today\n",
        "\n",
        "By the end of this lab, you will have:\n",
        "\n",
        "- ✅ End-to-end ML pipeline (ingestion → features → training → monitoring)\n",
        "- ✅ Temporal correctness (no look-ahead bias, point-in-time features)\n",
        "- ✅ Multiple testing corrections (Bonferroni, FDR)\n",
        "- ✅ Combinatorial Purged Cross-Validation (gold standard for finance)\n",
        "- ✅ Production monitoring (drift detection, performance tracking)\n",
        "\n",
        "**Time estimate:** 75 minutes (FIN510) | 100 minutes (FIN720 with all exercises)\n",
        "\n",
        "::: {.callout-important}\n",
        "## Why This Matters\n",
        "**This is Coursework 2 best practices.** If you implement these patterns—CPCV, PBO, embargo periods, multiple testing corrections—you'll stand out. Most students submit naive backtests. You'll submit **production-grade** work that could actually be deployed.\n",
        ":::\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "- Design and implement end-to-end ML pipelines for financial applications\n",
        "- Engineer features with temporal correctness preventing look-ahead bias\n",
        "- Apply multiple testing corrections (Bonferroni, FDR) to prevent false discoveries\n",
        "- Implement combinatorial purged cross-validation for robust backtesting\n",
        "- Calculate probability of backtest overfitting quantifying risk\n",
        "- Monitor model performance and detect data drift in production\n",
        "- Track model versions and implement rollback capabilities\n",
        "- Evaluate production readiness using comprehensive validation\n",
        "\n",
        "## Setup and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from scipy import stats\n",
        "from itertools import combinations\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization settings\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# For statistical tests\n",
        "try:\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "except ImportError:\n",
        "    print(\"Installing statsmodels...\")\n",
        "    !pip install -q statsmodels\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "print(\"✓ Setup complete - ready for production ML pipeline development\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: ML Pipeline Implementation\n",
        "\n",
        "### Understanding Pipeline Architecture\n",
        "\n",
        "Production ML systems aren't standalone scripts—they're pipelines with clearly defined components, interfaces, and orchestration. We'll implement a simple but realistic pipeline for factor-based investing, demonstrating principles applicable to any ML application.\n",
        "\n",
        "### Data Ingestion with Versioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class DataIngestionPipeline:\n",
        "    \"\"\"\n",
        "    Data ingestion component with versioning and validation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_source=\"synthetic\"):\n",
        "        self.data_source = data_source\n",
        "        self.version = None\n",
        "        self.ingestion_timestamp = None\n",
        "        \n",
        "    def ingest_factor_data(self, n_periods=120, n_assets=50):\n",
        "        \"\"\"\n",
        "        Ingest or generate factor data with metadata\n",
        "        \n",
        "        In production, this would:\n",
        "        - Query databases or APIs\n",
        "        - Handle retries and failures\n",
        "        - Validate schemas\n",
        "        - Version the extracted data\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # Generate synthetic factor data (simulating market data)\n",
        "        dates = pd.date_range(end=datetime.now(), periods=n_periods, freq='M')\n",
        "        \n",
        "        # Factors: Value, Momentum, Quality, Size, Low Vol\n",
        "        factor_names = ['value', 'momentum', 'quality', 'size', 'low_vol']\n",
        "        \n",
        "        data = []\n",
        "        for asset in range(n_assets):\n",
        "            asset_id = f\"asset_{asset:03d}\"\n",
        "            \n",
        "            for date in dates:\n",
        "                # Generate factor exposures with some persistence\n",
        "                row = {'date': date, 'asset_id': asset_id}\n",
        "                \n",
        "                for factor in factor_names:\n",
        "                    # Factors have autocorrelation (realistic)\n",
        "                    if len(data) > 0 and any(d['asset_id'] == asset_id for d in data):\n",
        "                        prev_vals = [d[factor] for d in data if d['asset_id'] == asset_id]\n",
        "                        prev = prev_vals[-1] if prev_vals else 0\n",
        "                        row[factor] = 0.7 * prev + 0.3 * np.random.randn()\n",
        "                    else:\n",
        "                        row[factor] = np.random.randn()\n",
        "                \n",
        "                # Generate forward returns (target variable)\n",
        "                # Returns correlated with factors (but not perfectly)\n",
        "                factor_vals = [row[f] for f in factor_names]\n",
        "                true_factor_loadings = [0.05, 0.03, 0.04, -0.02, -0.01]  # True relationships\n",
        "                \n",
        "                expected_return = sum(f * l for f, l in zip(factor_vals, true_factor_loadings))\n",
        "                row['return_1m'] = expected_return + 0.10 * np.random.randn()  # Add noise\n",
        "                \n",
        "                data.append(row)\n",
        "        \n",
        "        df = pd.DataFrame(data)\n",
        "        \n",
        "        # Add metadata\n",
        "        self.version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.ingestion_timestamp = datetime.now()\n",
        "        \n",
        "        # Validation\n",
        "        self._validate_data(df)\n",
        "        \n",
        "        print(f\"✓ Data ingested: {len(df)} records\")\n",
        "        print(f\"  Version: {self.version}\")\n",
        "        print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "        print(f\"  Assets: {df['asset_id'].nunique()}\")\n",
        "        print(f\"  Factors: {', '.join(factor_names)}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _validate_data(self, df):\n",
        "        \"\"\"Validate data quality\"\"\"\n",
        "        # Check for required columns\n",
        "        required_cols = ['date', 'asset_id', 'return_1m']\n",
        "        missing = [c for c in required_cols if c not in df.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing required columns: {missing}\")\n",
        "        \n",
        "        # Check for nulls\n",
        "        null_counts = df.isnull().sum()\n",
        "        if null_counts.sum() > 0:\n",
        "            print(f\"  ⚠️  Warning: Found {null_counts.sum()} null values\")\n",
        "        \n",
        "        # Check for duplicates\n",
        "        dupes = df.duplicated(['date', 'asset_id']).sum()\n",
        "        if dupes > 0:\n",
        "            raise ValueError(f\"Found {dupes} duplicate (date, asset_id) pairs\")\n",
        "        \n",
        "        print(f\"  ✓ Data validation passed\")\n",
        "\n",
        "\n",
        "# Demonstrate data ingestion\n",
        "print(\"=\"*70)\n",
        "print(\"DATA INGESTION PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "ingestion_pipeline = DataIngestionPipeline()\n",
        "factor_data = ingestion_pipeline.ingest_factor_data(n_periods=120, n_assets=50)\n",
        "\n",
        "print(\"\\nSample data:\")\n",
        "print(factor_data.head(10))\n",
        "\n",
        "print(\"\\nData statistics:\")\n",
        "print(factor_data[['value', 'momentum', 'quality', 'size', 'low_vol', 'return_1m']].describe())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering with Temporal Correctness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class FeatureEngineeringPipeline:\n",
        "    \"\"\"\n",
        "    Feature engineering ensuring temporal correctness (no look-ahead bias)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.feature_definitions = {}\n",
        "        self.version = \"1.0.0\"\n",
        "        \n",
        "    def engineer_features(self, df, lookback_periods=[3, 6, 12]):\n",
        "        \"\"\"\n",
        "        Engineer temporal features with strict point-in-time correctness\n",
        "        \n",
        "        Key principle: Only use data available at prediction time\n",
        "        \"\"\"\n",
        "        df = df.copy().sort_values(['asset_id', 'date'])\n",
        "        \n",
        "        # Original factors (already point-in-time correct)\n",
        "        features = ['value', 'momentum', 'quality', 'size', 'low_vol']\n",
        "        \n",
        "        # Engineer lagged aggregations (moving averages, volatilities)\n",
        "        for asset_id, asset_df in df.groupby('asset_id'):\n",
        "            for period in lookback_periods:\n",
        "                for factor in ['value', 'momentum', 'quality']:\n",
        "                    # Moving average (using only past data)\n",
        "                    col_name = f'{factor}_ma{period}'\n",
        "                    df.loc[df['asset_id'] == asset_id, col_name] = (\n",
        "                        asset_df[factor].rolling(window=period, min_periods=1).mean()\n",
        "                    )\n",
        "                    \n",
        "                    # Volatility (using only past data)\n",
        "                    col_name = f'{factor}_vol{period}'\n",
        "                    df.loc[df['asset_id'] == asset_id, col_name] = (\n",
        "                        asset_df[factor].rolling(window=period, min_periods=2).std()\n",
        "                    )\n",
        "        \n",
        "        # Fill NaN from rolling windows (first periods)\n",
        "        engineered_features = [c for c in df.columns if '_ma' in c or '_vol' in c]\n",
        "        df[engineered_features] = df[engineered_features].fillna(0)\n",
        "        \n",
        "        print(f\"✓ Features engineered: {len(engineered_features)} new features\")\n",
        "        print(f\"  Lookback periods: {lookback_periods}\")\n",
        "        print(f\"  Total features: {len(features) + len(engineered_features)}\")\n",
        "        \n",
        "        # Verify no look-ahead bias\n",
        "        self._verify_temporal_correctness(df)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _verify_temporal_correctness(self, df):\n",
        "        \"\"\"\n",
        "        Verify features don't use future information\n",
        "        \n",
        "        Check: For prediction at time t, all features use data <= t\n",
        "        \"\"\"\n",
        "        # Sample verification: check if any feature perfectly predicts returns\n",
        "        # (would indicate leakage)\n",
        "        \n",
        "        feature_cols = [c for c in df.columns if c not in ['date', 'asset_id', 'return_1m']]\n",
        "        \n",
        "        # Calculate correlation with future returns\n",
        "        max_corr = 0\n",
        "        max_corr_feature = None\n",
        "        \n",
        "        for col in feature_cols:\n",
        "            corr = abs(df[col].corr(df['return_1m']))\n",
        "            if corr > max_corr:\n",
        "                max_corr = corr\n",
        "                max_corr_feature = col\n",
        "        \n",
        "        if max_corr > 0.9:  # Suspiciously high correlation\n",
        "            print(f\"  ⚠️  Warning: Feature {max_corr_feature} has correlation {max_corr:.3f} with returns\")\n",
        "            print(f\"     This might indicate look-ahead bias!\")\n",
        "        else:\n",
        "            print(f\"  ✓ Temporal correctness verified (max correlation: {max_corr:.3f})\")\n",
        "\n",
        "\n",
        "# Demonstrate feature engineering\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE ENGINEERING PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "feature_pipeline = FeatureEngineeringPipeline()\n",
        "factor_data_with_features = feature_pipeline.engineer_features(factor_data, lookback_periods=[3, 6, 12])\n",
        "\n",
        "print(\"\\nEngineered features sample:\")\n",
        "feature_cols = [c for c in factor_data_with_features.columns if '_ma' in c or '_vol' in c]\n",
        "print(factor_data_with_features[['date', 'asset_id'] + feature_cols[:6]].head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training with Versioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class ModelTrainingPipeline:\n",
        "    \"\"\"\n",
        "    Model training with versioning and metadata tracking\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.metadata = {}\n",
        "        self.version_counter = 0\n",
        "        \n",
        "    def train_factor_model(self, df, train_end_date, features=None):\n",
        "        \"\"\"\n",
        "        Train factor model with proper train/test split\n",
        "        \n",
        "        Args:\n",
        "            df: Full dataset\n",
        "            train_end_date: Last date for training (anything after is test)\n",
        "            features: List of feature columns (if None, use all numeric except target)\n",
        "        \"\"\"\n",
        "        # Split data temporally (no random split - that would leak!)\n",
        "        train_df = df[df['date'] <= train_end_date].copy()\n",
        "        test_df = df[df['date'] > train_end_date].copy()\n",
        "        \n",
        "        # Select features\n",
        "        if features is None:\n",
        "            exclude_cols = ['date', 'asset_id', 'return_1m']\n",
        "            features = [c for c in df.columns if c not in exclude_cols]\n",
        "        \n",
        "        X_train = train_df[features]\n",
        "        y_train = train_df['return_1m']\n",
        "        X_test = test_df[features]\n",
        "        y_test = test_df['return_1m']\n",
        "        \n",
        "        # Scale features (fit on train only!)\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        # Train model\n",
        "        model = Ridge(alpha=1.0)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Evaluate\n",
        "        train_score = model.score(X_train_scaled, y_train)\n",
        "        test_score = model.score(X_test_scaled, y_test)\n",
        "        \n",
        "        train_pred = model.predict(X_train_scaled)\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        train_mse = np.mean((y_train - train_pred) ** 2)\n",
        "        test_mse = np.mean((y_test - test_pred) ** 2)\n",
        "        \n",
        "        # Version and store\n",
        "        version_id = f\"v{self.version_counter}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        self.version_counter += 1\n",
        "        \n",
        "        self.models[version_id] = model\n",
        "        self.scalers[version_id] = scaler\n",
        "        self.metadata[version_id] = {\n",
        "            'train_end_date': train_end_date,\n",
        "            'features': features,\n",
        "            'n_train': len(train_df),\n",
        "            'n_test': len(test_df),\n",
        "            'train_r2': train_score,\n",
        "            'test_r2': test_score,\n",
        "            'train_mse': train_mse,\n",
        "            'test_mse': test_mse,\n",
        "            'model_type': 'Ridge',\n",
        "            'hyperparameters': {'alpha': 1.0}\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n✓ Model trained: {version_id}\")\n",
        "        print(f\"  Training period: {train_df['date'].min()} to {train_end_date}\")\n",
        "        print(f\"  Test period: {test_df['date'].min()} to {test_df['date'].max()}\")\n",
        "        print(f\"  Training samples: {len(train_df):,}\")\n",
        "        print(f\"  Test samples: {len(test_df):,}\")\n",
        "        print(f\"  Features: {len(features)}\")\n",
        "        print(f\"\\n  Performance:\")\n",
        "        print(f\"    Train R²: {train_score:.4f}\")\n",
        "        print(f\"    Test R²:  {test_score:.4f}\")\n",
        "        print(f\"    Train MSE: {train_mse:.6f}\")\n",
        "        print(f\"    Test MSE:  {test_mse:.6f}\")\n",
        "        \n",
        "        return version_id, model, scaler\n",
        "    \n",
        "    def get_model_metadata(self, version_id):\n",
        "        \"\"\"Retrieve model metadata for versioning and auditing\"\"\"\n",
        "        return self.metadata.get(version_id, {})\n",
        "\n",
        "\n",
        "# Demonstrate model training\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL TRAINING PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "training_pipeline = ModelTrainingPipeline()\n",
        "\n",
        "# Train model with 80-20 temporal split\n",
        "all_dates = sorted(factor_data_with_features['date'].unique())\n",
        "split_idx = int(len(all_dates) * 0.8)\n",
        "train_end_date = all_dates[split_idx]\n",
        "\n",
        "version_id, model, scaler = training_pipeline.train_factor_model(\n",
        "    factor_data_with_features,\n",
        "    train_end_date\n",
        ")\n",
        "\n",
        "# Show model coefficients (feature importance)\n",
        "feature_cols = [c for c in factor_data_with_features.columns \n",
        "                if c not in ['date', 'asset_id', 'return_1m']]\n",
        "coefficients = model.coef_\n",
        "\n",
        "print(f\"\\nTop 10 features by absolute coefficient:\")\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'coefficient': coefficients\n",
        "}).sort_values('coefficient', key=abs, ascending=False)\n",
        "\n",
        "print(feature_importance.head(10).to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 1)\n",
        "\n",
        "Write 200-250 words addressing:\n",
        "\n",
        "1. **Pipeline Benefits**: How does organizing ML code into pipeline components (ingestion, features, training) improve maintainability compared to monolithic scripts? What challenges does this introduce?\n",
        "\n",
        "2. **Temporal Correctness**: Why is temporal correctness critical for financial ML? What would happen if we accidentally used future information in features? How can we systematically verify temporal correctness?\n",
        "\n",
        "3. **Model Versioning**: What information should model versions track for production systems? How does versioning enable debugging, auditing, and rollback capabilities?\n",
        "\n",
        "## Exercise 2: Rigorous Backtesting with Multiple Testing Correction\n",
        "\n",
        "### The Multiple Testing Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def demonstrate_multiple_testing_problem(n_random_features=100, n_samples=1000, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Demonstrate how testing many random features produces false discoveries\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Generate random features (no predictive power)\n",
        "    X_random = np.random.randn(n_samples, n_random_features)\n",
        "    y = np.random.randn(n_samples)  # Random returns\n",
        "    \n",
        "    # Test each feature for significance\n",
        "    p_values = []\n",
        "    correlations = []\n",
        "    \n",
        "    for i in range(n_random_features):\n",
        "        corr = np.corrcoef(X_random[:, i], y)[0, 1]\n",
        "        # T-test for correlation significance\n",
        "        t_stat = corr * np.sqrt(n_samples - 2) / np.sqrt(1 - corr**2)\n",
        "        p_val = 2 * (1 - stats.t.cdf(abs(t_stat), n_samples - 2))\n",
        "        \n",
        "        p_values.append(p_val)\n",
        "        correlations.append(abs(corr))\n",
        "    \n",
        "    # Count \"significant\" features (without correction)\n",
        "    significant_uncorrected = sum(p < alpha for p in p_values)\n",
        "    \n",
        "    # Apply Bonferroni correction\n",
        "    alpha_bonferroni = alpha / n_random_features\n",
        "    significant_bonferroni = sum(p < alpha_bonferroni for p in p_values)\n",
        "    \n",
        "    # Apply Benjamini-Hochberg FDR correction\n",
        "    rejected, p_corrected, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
        "    significant_fdr = sum(rejected)\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"MULTIPLE TESTING DEMONSTRATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nTesting {n_random_features} random features (no true predictive power)\")\n",
        "    print(f\"Sample size: {n_samples}\")\n",
        "    print(f\"Significance level: α = {alpha}\")\n",
        "    \n",
        "    print(f\"\\nResults WITHOUT correction:\")\n",
        "    print(f\"  'Significant' features: {significant_uncorrected}\")\n",
        "    print(f\"  Expected false positives: {n_random_features * alpha:.1f}\")\n",
        "    print(f\"  → {significant_uncorrected/n_random_features*100:.1f}% of features appear significant!\")\n",
        "    \n",
        "    print(f\"\\nResults WITH Bonferroni correction:\")\n",
        "    print(f\"  Significant features: {significant_bonferroni}\")\n",
        "    print(f\"  → Properly controls false positives\")\n",
        "    \n",
        "    print(f\"\\nResults WITH FDR correction:\")\n",
        "    print(f\"  Significant features: {significant_fdr}\")\n",
        "    print(f\"  → Balances power and error control\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # P-value distribution\n",
        "    axes[0].hist(p_values, bins=20, edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(alpha, color='red', linestyle='--', linewidth=2, label=f'α = {alpha}')\n",
        "    axes[0].axvline(alpha_bonferroni, color='green', linestyle='--', linewidth=2, \n",
        "                    label=f'Bonferroni = {alpha_bonferroni:.4f}')\n",
        "    axes[0].set_xlabel('P-value')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('P-value Distribution (Random Features)', fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Sorted p-values with FDR threshold\n",
        "    sorted_p = sorted(p_values)\n",
        "    ranks = np.arange(1, len(sorted_p) + 1)\n",
        "    fdr_threshold = alpha * ranks / n_random_features\n",
        "    \n",
        "    axes[1].plot(ranks, sorted_p, 'o', markersize=4, alpha=0.6, label='P-values')\n",
        "    axes[1].plot(ranks, fdr_threshold, 'r--', linewidth=2, label='FDR threshold')\n",
        "    axes[1].set_xlabel('Rank')\n",
        "    axes[1].set_ylabel('P-value')\n",
        "    axes[1].set_title('Benjamini-Hochberg FDR Procedure', fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return p_values, significant_uncorrected, significant_bonferroni, significant_fdr\n",
        "\n",
        "\n",
        "# Run demonstration\n",
        "p_vals, n_uncorr, n_bonf, n_fdr = demonstrate_multiple_testing_problem()\n",
        "\n",
        "print(\"\\n💡 Key Insight: Testing many features guarantees false discoveries\")\n",
        "print(\"   Multiple testing correction is ESSENTIAL for valid research!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing Combinatorial Purged Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def combinatorial_purged_cv(df, features, n_splits=5, embargo_pct=0.05):\n",
        "    \"\"\"\n",
        "    Implement Bailey & López de Prado's Combinatorial Purged Cross-Validation\n",
        "    \n",
        "    Steps:\n",
        "    1. Create multiple train/test splits\n",
        "    2. Purge training samples near test samples (prevent leakage)\n",
        "    3. Add embargo period (account for label lag)\n",
        "    4. Train model on each split\n",
        "    5. Compute performance metrics\n",
        "    6. Calculate Probability of Backtest Overfitting (PBO)\n",
        "    \n",
        "    Args:\n",
        "        df: Dataset with features and returns\n",
        "        features: List of feature column names\n",
        "        n_splits: Number of CV splits\n",
        "        embargo_pct: Percentage of data to embargo (gap between train/test)\n",
        "    \"\"\"\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "    dates = sorted(df['date'].unique())\n",
        "    n_dates = len(dates)\n",
        "    \n",
        "    # Create split indices\n",
        "    split_size = n_dates // n_splits\n",
        "    embargo_periods = int(split_size * embargo_pct)\n",
        "    \n",
        "    # Generate combinations of splits for train/test\n",
        "    # Use subset of all possible combinations (computational limit)\n",
        "    split_indices = list(range(n_splits))\n",
        "    n_combinations = min(10, 2 ** (n_splits - 1))  # Limit combinations\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMBINATORIAL PURGED CROSS-VALIDATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"  Splits: {n_splits}\")\n",
        "    print(f\"  Embargo: {embargo_pct*100:.0f}% ({embargo_periods} periods)\")\n",
        "    print(f\"  Testing {n_combinations} train/test combinations\")\n",
        "    \n",
        "    for combo_idx in range(n_combinations):\n",
        "        # Randomly select test splits\n",
        "        np.random.seed(combo_idx)\n",
        "        test_splits = np.random.choice(split_indices, size=max(1, n_splits // 3), replace=False)\n",
        "        train_splits = [s for s in split_indices if s not in test_splits]\n",
        "        \n",
        "        # Convert splits to date ranges\n",
        "        test_dates = set()\n",
        "        for split_idx in test_splits:\n",
        "            start_idx = split_idx * split_size\n",
        "            end_idx = min((split_idx + 1) * split_size, n_dates)\n",
        "            test_dates.update(dates[start_idx:end_idx])\n",
        "        \n",
        "        train_dates = set()\n",
        "        for split_idx in train_splits:\n",
        "            start_idx = split_idx * split_size\n",
        "            end_idx = min((split_idx + 1) * split_size, n_dates)\n",
        "            \n",
        "            # Purge: remove dates close to test dates\n",
        "            split_dates = dates[start_idx:end_idx]\n",
        "            for date in split_dates:\n",
        "                # Check if date is too close to any test date\n",
        "                too_close = False\n",
        "                for test_date in test_dates:\n",
        "                    date_diff = abs((date - test_date).days)\n",
        "                    if date_diff < embargo_periods * 30:  # Assuming monthly data\n",
        "                        too_close = True\n",
        "                        break\n",
        "                \n",
        "                if not too_close:\n",
        "                    train_dates.add(date)\n",
        "        \n",
        "        # Create train/test sets\n",
        "        train_df = df[df['date'].isin(train_dates)]\n",
        "        test_df = df[df['date'].isin(test_dates)]\n",
        "        \n",
        "        if len(train_df) < 100 or len(test_df) < 50:\n",
        "            continue  # Skip if insufficient data\n",
        "        \n",
        "        # Train model\n",
        "        X_train = train_df[features]\n",
        "        y_train = train_df['return_1m']\n",
        "        X_test = test_df[features]\n",
        "        y_test = test_df['return_1m']\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        model = Ridge(alpha=1.0)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        train_pred = model.predict(X_train_scaled)\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        # Calculate Sharpe ratios (common financial metric)\n",
        "        train_sharpe = np.mean(train_pred) / (np.std(train_pred) + 1e-6) * np.sqrt(12)\n",
        "        test_sharpe = np.mean(test_pred) / (np.std(test_pred) + 1e-6) * np.sqrt(12)\n",
        "        \n",
        "        results.append({\n",
        "            'combo': combo_idx,\n",
        "            'train_sharpe': train_sharpe,\n",
        "            'test_sharpe': test_sharpe,\n",
        "            'train_size': len(train_df),\n",
        "            'test_size': len(test_df)\n",
        "        })\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Calculate Probability of Backtest Overfitting (PBO)\n",
        "    median_train_sharpe = results_df['train_sharpe'].median()\n",
        "    pbo = (results_df['test_sharpe'] < median_train_sharpe).mean()\n",
        "    \n",
        "    print(f\"\\n\" + \"-\"*70)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"\\nCombinations tested: {len(results_df)}\")\n",
        "    print(f\"Median train Sharpe: {median_train_sharpe:.4f}\")\n",
        "    print(f\"Mean test Sharpe: {results_df['test_sharpe'].mean():.4f}\")\n",
        "    print(f\"\\nProbability of Backtest Overfitting (PBO): {pbo:.4f}\")\n",
        "    \n",
        "    if pbo > 0.5:\n",
        "        print(f\"  ⚠️  HIGH OVERFITTING RISK! (PBO > 0.5)\")\n",
        "        print(f\"     Strategy likely captured noise, not signal\")\n",
        "    elif pbo > 0.3:\n",
        "        print(f\"  ⚠️  MODERATE OVERFITTING RISK (PBO > 0.3)\")\n",
        "        print(f\"     Proceed with caution, validate further\")\n",
        "    else:\n",
        "        print(f\"  ✓ LOW OVERFITTING RISK (PBO < 0.3)\")\n",
        "        print(f\"     Strategy appears robust\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Train vs Test Sharpe\n",
        "    axes[0].scatter(results_df['train_sharpe'], results_df['test_sharpe'], alpha=0.6, s=50)\n",
        "    axes[0].plot([results_df['train_sharpe'].min(), results_df['train_sharpe'].max()],\n",
        "                 [results_df['train_sharpe'].min(), results_df['train_sharpe'].max()],\n",
        "                 'r--', label='45° line')\n",
        "    axes[0].axvline(median_train_sharpe, color='green', linestyle='--', label='Median train')\n",
        "    axes[0].set_xlabel('Train Sharpe Ratio')\n",
        "    axes[0].set_ylabel('Test Sharpe Ratio')\n",
        "    axes[0].set_title('Train vs Test Performance', fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Distribution of test Sharpe\n",
        "    axes[1].hist(results_df['test_sharpe'], bins=15, edgecolor='black', alpha=0.7)\n",
        "    axes[1].axvline(median_train_sharpe, color='green', linestyle='--', linewidth=2,\n",
        "                    label=f'Median train = {median_train_sharpe:.3f}')\n",
        "    axes[1].axvline(results_df['test_sharpe'].mean(), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Mean test = {results_df[\"test_sharpe\"].mean():.3f}')\n",
        "    axes[1].set_xlabel('Test Sharpe Ratio')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].set_title(f'Test Performance Distribution (PBO={pbo:.3f})', fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results_df, pbo\n",
        "\n",
        "\n",
        "# Run combinatorial purged CV\n",
        "feature_cols = [c for c in factor_data_with_features.columns \n",
        "                if c not in ['date', 'asset_id', 'return_1m']]\n",
        "\n",
        "cv_results, pbo = combinatorial_purged_cv(\n",
        "    factor_data_with_features,\n",
        "    feature_cols,\n",
        "    n_splits=5,\n",
        "    embargo_pct=0.05\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 2)\n",
        "\n",
        "Write 250-300 words addressing:\n",
        "\n",
        "1. **Multiple Testing Impact**: In the demonstration, ~5 features appeared significant by chance. How would this affect a researcher who tests 100 features and publishes the \"significant\" ones? What are the consequences for financial markets if overfit strategies are widely adopted?\n",
        "\n",
        "2. **PBO Interpretation**: The Probability of Backtest Overfitting measures what fraction of test periods underperform the median training performance. Why is this a useful metric for detecting overfitting? What PBO threshold should trigger concern?\n",
        "\n",
        "3. **Purging and Embargo**: Why do we purge training samples near test periods and add embargo gaps? What would happen if we skipped these steps? How do these relate to the temporal structure of financial data?\n",
        "\n",
        "## Exercise 3: Production Monitoring and Drift Detection\n",
        "\n",
        "### Simulating Production Deployment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "class ProductionMonitor:\n",
        "    \"\"\"\n",
        "    Monitor model performance and detect drift in production\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, scaler, features, baseline_data):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.features = features\n",
        "        \n",
        "        # Calculate baseline statistics for drift detection\n",
        "        self.baseline_mean = baseline_data[features].mean()\n",
        "        self.baseline_std = baseline_data[features].std()\n",
        "        \n",
        "        # Performance tracking\n",
        "        self.performance_history = []\n",
        "        \n",
        "    def predict_and_monitor(self, df):\n",
        "        \"\"\"\n",
        "        Make predictions and monitor for drift/degradation\n",
        "        \"\"\"\n",
        "        X = df[self.features]\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        predictions = self.model.predict(X_scaled)\n",
        "        \n",
        "        # Calculate performance (when ground truth available)\n",
        "        if 'return_1m' in df.columns:\n",
        "            mse = np.mean((df['return_1m'] - predictions) ** 2)\n",
        "            mae = np.mean(np.abs(df['return_1m'] - predictions))\n",
        "            corr = np.corrcoef(df['return_1m'], predictions)[0, 1]\n",
        "            \n",
        "            sharpe = np.mean(predictions) / (np.std(predictions) + 1e-6) * np.sqrt(12)\n",
        "            \n",
        "            self.performance_history.append({\n",
        "                'timestamp': datetime.now(),\n",
        "                'n_samples': len(df),\n",
        "                'mse': mse,\n",
        "                'mae': mae,\n",
        "                'correlation': corr,\n",
        "                'sharpe': sharpe\n",
        "            })\n",
        "        \n",
        "        # Detect data drift (Population Stability Index)\n",
        "        psi_scores = self._calculate_psi(df[self.features])\n",
        "        \n",
        "        # Alert if drift detected\n",
        "        alerts = []\n",
        "        for feature, psi in psi_scores.items():\n",
        "            if psi > 0.25:  # Significant drift threshold\n",
        "                alerts.append(f\"⚠️  DRIFT ALERT: {feature} (PSI={psi:.3f})\")\n",
        "        \n",
        "        return predictions, psi_scores, alerts\n",
        "    \n",
        "    def _calculate_psi(self, current_data):\n",
        "        \"\"\"\n",
        "        Calculate Population Stability Index for each feature\n",
        "        \n",
        "        PSI measures how much the distribution has shifted\n",
        "        PSI < 0.1: No significant change\n",
        "        0.1 < PSI < 0.25: Moderate change\n",
        "        PSI > 0.25: Significant change\n",
        "        \"\"\"\n",
        "        psi_scores = {}\n",
        "        \n",
        "        for feature in self.features:\n",
        "            # Use baseline and current distributions\n",
        "            baseline_values = self.baseline_mean[feature]\n",
        "            current_mean = current_data[feature].mean()\n",
        "            \n",
        "            # Simplified PSI calculation\n",
        "            # In production, use proper binned distributions\n",
        "            if self.baseline_std[feature] > 0:\n",
        "                z_score = abs(current_mean - baseline_values) / self.baseline_std[feature]\n",
        "                psi = z_score / 10  # Approximate PSI\n",
        "            else:\n",
        "                psi = 0\n",
        "            \n",
        "            psi_scores[feature] = psi\n",
        "        \n",
        "        return psi_scores\n",
        "    \n",
        "    def generate_monitoring_report(self):\n",
        "        \"\"\"Generate monitoring dashboard\"\"\"\n",
        "        if not self.performance_history:\n",
        "            print(\"No performance data available yet\")\n",
        "            return\n",
        "        \n",
        "        perf_df = pd.DataFrame(self.performance_history)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"PRODUCTION MONITORING REPORT\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        print(f\"\\nPerformance Summary (last {len(perf_df)} evaluations):\")\n",
        "        print(f\"  MSE:  mean={perf_df['mse'].mean():.6f}, std={perf_df['mse'].std():.6f}\")\n",
        "        print(f\"  MAE:  mean={perf_df['mae'].mean():.6f}, std={perf_df['mae'].std():.6f}\")\n",
        "        print(f\"  Corr: mean={perf_df['correlation'].mean():.4f}, std={perf_df['correlation'].std():.4f}\")\n",
        "        print(f\"  Sharpe: mean={perf_df['sharpe'].mean():.4f}, std={perf_df['sharpe'].std():.4f}\")\n",
        "        \n",
        "        # Check for degradation\n",
        "        if len(perf_df) >= 5:\n",
        "            recent_sharpe = perf_df['sharpe'].iloc[-3:].mean()\n",
        "            historical_sharpe = perf_df['sharpe'].iloc[:-3].mean()\n",
        "            \n",
        "            if recent_sharpe < historical_sharpe - 0.5:\n",
        "                print(f\"\\n⚠️  PERFORMANCE DEGRADATION DETECTED!\")\n",
        "                print(f\"   Recent Sharpe: {recent_sharpe:.4f}\")\n",
        "                print(f\"   Historical Sharpe: {historical_sharpe:.4f}\")\n",
        "                print(f\"   Consider retraining or rolling back model\")\n",
        "        \n",
        "        # Visualize performance over time\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "        \n",
        "        # MSE over time\n",
        "        axes[0, 0].plot(range(len(perf_df)), perf_df['mse'], marker='o', linewidth=2)\n",
        "        axes[0, 0].set_xlabel('Evaluation Period')\n",
        "        axes[0, 0].set_ylabel('MSE')\n",
        "        axes[0, 0].set_title('Mean Squared Error Over Time', fontweight='bold')\n",
        "        axes[0, 0].grid(alpha=0.3)\n",
        "        \n",
        "        # Correlation over time\n",
        "        axes[0, 1].plot(range(len(perf_df)), perf_df['correlation'], marker='o', linewidth=2, color='green')\n",
        "        axes[0, 1].axhline(0, color='red', linestyle='--', alpha=0.3)\n",
        "        axes[0, 1].set_xlabel('Evaluation Period')\n",
        "        axes[0, 1].set_ylabel('Correlation')\n",
        "        axes[0, 1].set_title('Prediction-Actual Correlation Over Time', fontweight='bold')\n",
        "        axes[0, 1].grid(alpha=0.3)\n",
        "        \n",
        "        # Sharpe ratio over time\n",
        "        axes[1, 0].plot(range(len(perf_df)), perf_df['sharpe'], marker='o', linewidth=2, color='purple')\n",
        "        axes[1, 0].axhline(0, color='red', linestyle='--', alpha=0.3)\n",
        "        axes[1, 0].set_xlabel('Evaluation Period')\n",
        "        axes[1, 0].set_ylabel('Sharpe Ratio')\n",
        "        axes[1, 0].set_title('Sharpe Ratio Over Time', fontweight='bold')\n",
        "        axes[1, 0].grid(alpha=0.3)\n",
        "        \n",
        "        # Performance distribution\n",
        "        axes[1, 1].hist(perf_df['sharpe'], bins=10, edgecolor='black', alpha=0.7, color='orange')\n",
        "        axes[1, 1].axvline(perf_df['sharpe'].mean(), color='red', linestyle='--', linewidth=2,\n",
        "                          label=f'Mean = {perf_df[\"sharpe\"].mean():.3f}')\n",
        "        axes[1, 1].set_xlabel('Sharpe Ratio')\n",
        "        axes[1, 1].set_ylabel('Frequency')\n",
        "        axes[1, 1].set_title('Sharpe Ratio Distribution', fontweight='bold')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Demonstrate production monitoring\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PRODUCTION DEPLOYMENT SIMULATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use trained model from Exercise 1\n",
        "all_dates = sorted(factor_data_with_features['date'].unique())\n",
        "test_start_idx = int(len(all_dates) * 0.8)\n",
        "test_dates = all_dates[test_start_idx:]\n",
        "\n",
        "# Create baseline from training data\n",
        "train_data = factor_data_with_features[\n",
        "    factor_data_with_features['date'] <= all_dates[test_start_idx]\n",
        "]\n",
        "\n",
        "# Initialize monitor\n",
        "monitor = ProductionMonitor(model, scaler, feature_cols, train_data)\n",
        "\n",
        "# Simulate production: process data in monthly batches\n",
        "print(\"\\nSimulating production deployment with monthly monitoring:\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for i, date in enumerate(test_dates):\n",
        "    batch_data = factor_data_with_features[factor_data_with_features['date'] == date]\n",
        "    \n",
        "    predictions, psi_scores, alerts = monitor.predict_and_monitor(batch_data)\n",
        "    \n",
        "    print(f\"\\nPeriod {i+1} ({date.strftime('%Y-%m')}): {len(batch_data)} predictions\")\n",
        "    \n",
        "    # Show drift warnings\n",
        "    max_psi = max(psi_scores.values())\n",
        "    if max_psi > 0.25:\n",
        "        print(f\"  ⚠️  High drift detected (max PSI={max_psi:.3f})\")\n",
        "        for alert in alerts[:3]:  # Show top 3\n",
        "            print(f\"     {alert}\")\n",
        "    else:\n",
        "        print(f\"  ✓ No significant drift (max PSI={max_psi:.3f})\")\n",
        "\n",
        "# Generate comprehensive monitoring report\n",
        "monitor.generate_monitoring_report()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 3)\n",
        "\n",
        "Write 200-250 words addressing:\n",
        "\n",
        "1. **Monitoring Strategy**: What metrics should production ML systems monitor? Why track both model performance (MSE, correlation) and data characteristics (PSI, drift)? How quickly should monitoring systems detect issues?\n",
        "\n",
        "2. **Drift Response**: When data drift is detected, what actions should be taken? When is retraining necessary versus other interventions (feature engineering, model rollback, investigation)? What are the risks of over-reacting to drift versus under-reacting?\n",
        "\n",
        "3. **Production vs Research**: This lab demonstrated many differences between research ML and production ML. What are the three most important differences? How do these differences affect how organizations should structure their ML teams and processes?\n",
        "\n",
        "## Summary and Integration\n",
        "\n",
        "### What We've Learned\n",
        "\n",
        "Through these exercises, you've:\n",
        "\n",
        "1. **Implemented end-to-end ML pipeline** with data ingestion, feature engineering, training, and versioning\n",
        "\n",
        "2. **Ensured temporal correctness** preventing look-ahead bias that creates unrealistic performance\n",
        "\n",
        "3. **Applied multiple testing corrections** preventing false discoveries from testing many features\n",
        "\n",
        "4. **Implemented combinatorial purged CV** with proper train/test separation for time-series\n",
        "\n",
        "5. **Calculated PBO** quantifying probability that backtest performance resulted from overfitting\n",
        "\n",
        "6. **Monitored production deployment** detecting drift and performance degradation\n",
        "\n",
        "7. **Understood overfitting pervasiveness** in financial ML requiring rigorous validation\n",
        "\n",
        "### Connections to Course Themes\n",
        "\n",
        "- **Week 9 (Smart Contracts)**: Both smart contracts and ML models make consequential decisions—require rigorous testing before deployment\n",
        "\n",
        "- **Week 8 (Fraud Detection)**: Production fraud detection exemplifies real-time ML pipelines with monitoring\n",
        "\n",
        "- **Week 4 (Robo-Advisors)**: Portfolio optimization at scale requires production ML infrastructure\n",
        "\n",
        "- **Throughout course**: Evidence-based evaluation—don't trust impressive backtests without rigorous validation\n",
        "\n",
        "### Critical Evaluation Framework\n",
        "\n",
        "When evaluating ML systems or research:\n",
        "\n",
        "1. **Validation rigor**: Are multiple testing corrections applied? Is out-of-sample validation proper?\n",
        "2. **Temporal correctness**: Are features point-in-time correct? Any look-ahead bias?\n",
        "3. **Production readiness**: Is there monitoring? Drift detection? Rollback capability?\n",
        "4. **Reproducibility**: Can results be reproduced? Is data/code versioned?\n",
        "5. **Business value**: Do model improvements translate to business impact?\n",
        "\n",
        "### Assessment Preparation\n",
        "\n",
        "**FIN510 Coursework 2**: Apply rigorous backtesting to factor replication—use multiple testing corrections, implement proper temporal splits, calculate PBO, validate out-of-sample.\n",
        "\n",
        "**FIN720**: Critically evaluate ML applications in finance—assess validation rigor, production readiness, and gap between research claims and deployment reality.\n",
        "\n",
        "### Further Exploration\n",
        "\n",
        "If interested in extending your analysis:\n",
        "\n",
        "- **Advanced feature stores**: Implement Feast or Tecton for feature consistency\n",
        "- **Continuous training**: Automate retraining pipelines triggered by drift detection\n",
        "- **Fairness monitoring**: Track model performance across demographic segments\n",
        "- **Explainability**: Implement SHAP or LIME for model interpretability\n",
        "- **Regulatory compliance**: Document models following SR 11-7 framework\n",
        "\n",
        "---\n",
        "\n",
        "**Excellent work! You've built production-ready ML pipelines with rigorous validation, understanding the engineering discipline and statistical rigor required for reliable financial ML systems.**\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nlp_env",
      "language": "python",
      "display_name": "Python (nlp_env)",
      "path": "/Users/quinference/Library/Jupyter/kernels/nlp_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}