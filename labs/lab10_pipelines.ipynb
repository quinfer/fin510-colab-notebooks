{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 10: Production ML Pipelines & Rigorous Backtesting\n",
        "\n",
        "From research prototype to production-ready factor models\n",
        "\n",
        "> **Expected Time**\n",
        ">\n",
        "> -   FIN510: Exercises 1-2 ≈ 75 min\n",
        "> -   FIN720: All exercises ≈ 100 min\n",
        "> -   Directed learning extensions ≈ 60 min\n",
        ">\n",
        "> ### Prerequisites\n",
        ">\n",
        "> This lab extends concepts from the JKP factor replication lab.\n",
        "> Familiarity with factor models and portfolio construction is helpful\n",
        "> but not required.\n",
        "\n",
        "<figure>\n",
        "<a\n",
        "href=\"https://colab.research.google.com/github/quinfer/fin510-colab-notebooks/blob/main/labs/lab10_pipelines.ipynb\"><img\n",
        "src=\"https://colab.research.google.com/assets/colab-badge.svg\" /></a>\n",
        "<figcaption>Open in Colab</figcaption>\n",
        "</figure>\n",
        "\n",
        "## Before You Code: The Big Picture\n",
        "\n",
        "**Research code ≠ Production code.** Your Jupyter notebook with 85%\n",
        "backtest Sharpe ratio? It won’t survive first contact with live markets.\n",
        "Here’s how to build **production-grade** ML pipelines that actually\n",
        "work.\n",
        "\n",
        "> **The Research-to-Production Gap**\n",
        ">\n",
        "> **Why Models Fail in Production:** 1. **Look-ahead bias**: Used future\n",
        "> data to make past predictions 2. **Overfitting**: Optimized on test\n",
        "> data, no true holdout set 3. **Data drift**: Training distribution ≠\n",
        "> production distribution 4. **Multiple testing**: Tried 100 features,\n",
        "> reported the 5 that worked 5. **Leakage**: Features contain\n",
        "> information not available at prediction time\n",
        ">\n",
        "> **The Evidence:** - **70% of ML projects fail to deploy** (Gartner\n",
        "> 2021) - **90% of deployed models underperform expectations**\n",
        "> (Algorithmia 2020) - Average Sharpe ratio decline: 0.5 → 0.1 from\n",
        "> backtest to live (industry estimates)\n",
        ">\n",
        "> **What Separates Winners from Losers:** - **Rigorous validation**:\n",
        "> CPCV, PBO, embargo periods, multiple testing corrections - **Temporal\n",
        "> correctness**: Strict point-in-time data, no future information -\n",
        "> **Production monitoring**: Drift detection, model versioning,\n",
        "> automated rollback - **Documentation**: Reproducible, auditable,\n",
        "> explainable\n",
        "\n",
        "### What You’ll Build Today\n",
        "\n",
        "By the end of this lab, you will have:\n",
        "\n",
        "-   ✅ End-to-end ML pipeline (ingestion → features → training →\n",
        "    monitoring)\n",
        "-   ✅ Temporal correctness (no look-ahead bias, point-in-time features)\n",
        "-   ✅ Multiple testing corrections (Bonferroni, FDR)\n",
        "-   ✅ Combinatorial Purged Cross-Validation (gold standard for finance)\n",
        "-   ✅ Production monitoring (drift detection, performance tracking)\n",
        "\n",
        "**Time estimate:** 75 minutes (FIN510) \\| 100 minutes (FIN720 with all\n",
        "exercises)\n",
        "\n",
        "> **Why This Matters**\n",
        ">\n",
        "> **This is Coursework 2 best practices.** If you implement these\n",
        "> patterns—CPCV, PBO, embargo periods, multiple testing\n",
        "> corrections—you’ll stand out. Most students submit naive backtests.\n",
        "> You’ll submit **production-grade** work that could actually be\n",
        "> deployed.\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "By the end of this lab, you will be able to:\n",
        "\n",
        "-   Design and implement end-to-end ML pipelines for financial\n",
        "    applications\n",
        "-   Engineer features with temporal correctness preventing look-ahead\n",
        "    bias\n",
        "-   Apply multiple testing corrections (Bonferroni, FDR) to prevent\n",
        "    false discoveries\n",
        "-   Implement combinatorial purged cross-validation for robust\n",
        "    backtesting\n",
        "-   Calculate probability of backtest overfitting quantifying risk\n",
        "-   Monitor model performance and detect data drift in production\n",
        "-   Track model versions and implement rollback capabilities\n",
        "-   Evaluate production readiness using comprehensive validation\n",
        "\n",
        "## Setup and Dependencies"
      ],
      "id": "b488f690-c918-483f-aefa-85db7abf91d8"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "from scipy import stats\n",
        "from itertools import combinations\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization settings\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "# For statistical tests\n",
        "try:\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "except ImportError:\n",
        "    print(\"Installing statsmodels...\")\n",
        "    !pip install -q statsmodels\n",
        "    from statsmodels.stats.multitest import multipletests\n",
        "\n",
        "print(\"✓ Setup complete - ready for production ML pipeline development\")"
      ],
      "id": "c5cb1127"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise 1: ML Pipeline Implementation\n",
        "\n",
        "### Understanding Pipeline Architecture\n",
        "\n",
        "Production ML systems aren’t standalone scripts—they’re pipelines with\n",
        "clearly defined components, interfaces, and orchestration. We’ll\n",
        "implement a simple but realistic pipeline for factor-based investing,\n",
        "demonstrating principles applicable to any ML application.\n",
        "\n",
        "### Data Ingestion with Versioning"
      ],
      "id": "f2ef185c-232a-48eb-9996-45d706ecd8a7"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DataIngestionPipeline:\n",
        "    \"\"\"\n",
        "    Data ingestion component with versioning and validation\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, data_source=\"synthetic\"):\n",
        "        self.data_source = data_source\n",
        "        self.version = None\n",
        "        self.ingestion_timestamp = None\n",
        "        \n",
        "    def ingest_factor_data(self, n_periods=120, n_assets=50):\n",
        "        \"\"\"\n",
        "        Ingest or generate factor data with metadata\n",
        "        \n",
        "        In production, this would:\n",
        "        - Query databases or APIs\n",
        "        - Handle retries and failures\n",
        "        - Validate schemas\n",
        "        - Version the extracted data\n",
        "        \"\"\"\n",
        "        np.random.seed(42)\n",
        "        \n",
        "        # Generate synthetic factor data (simulating market data)\n",
        "        dates = pd.date_range(end=datetime.now(), periods=n_periods, freq='M')\n",
        "        \n",
        "        # Factors: Value, Momentum, Quality, Size, Low Vol\n",
        "        factor_names = ['value', 'momentum', 'quality', 'size', 'low_vol']\n",
        "        \n",
        "        data = []\n",
        "        for asset in range(n_assets):\n",
        "            asset_id = f\"asset_{asset:03d}\"\n",
        "            \n",
        "            for date in dates:\n",
        "                # Generate factor exposures with some persistence\n",
        "                row = {'date': date, 'asset_id': asset_id}\n",
        "                \n",
        "                for factor in factor_names:\n",
        "                    # Factors have autocorrelation (realistic)\n",
        "                    if len(data) > 0 and any(d['asset_id'] == asset_id for d in data):\n",
        "                        prev_vals = [d[factor] for d in data if d['asset_id'] == asset_id]\n",
        "                        prev = prev_vals[-1] if prev_vals else 0\n",
        "                        row[factor] = 0.7 * prev + 0.3 * np.random.randn()\n",
        "                    else:\n",
        "                        row[factor] = np.random.randn()\n",
        "                \n",
        "                # Generate forward returns (target variable)\n",
        "                # Returns correlated with factors (but not perfectly)\n",
        "                factor_vals = [row[f] for f in factor_names]\n",
        "                true_factor_loadings = [0.05, 0.03, 0.04, -0.02, -0.01]  # True relationships\n",
        "                \n",
        "                expected_return = sum(f * l for f, l in zip(factor_vals, true_factor_loadings))\n",
        "                row['return_1m'] = expected_return + 0.10 * np.random.randn()  # Add noise\n",
        "                \n",
        "                data.append(row)\n",
        "        \n",
        "        df = pd.DataFrame(data)\n",
        "        \n",
        "        # Add metadata\n",
        "        self.version = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.ingestion_timestamp = datetime.now()\n",
        "        \n",
        "        # Validation\n",
        "        self._validate_data(df)\n",
        "        \n",
        "        print(f\"✓ Data ingested: {len(df)} records\")\n",
        "        print(f\"  Version: {self.version}\")\n",
        "        print(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\n",
        "        print(f\"  Assets: {df['asset_id'].nunique()}\")\n",
        "        print(f\"  Factors: {', '.join(factor_names)}\")\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _validate_data(self, df):\n",
        "        \"\"\"Validate data quality\"\"\"\n",
        "        # Check for required columns\n",
        "        required_cols = ['date', 'asset_id', 'return_1m']\n",
        "        missing = [c for c in required_cols if c not in df.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing required columns: {missing}\")\n",
        "        \n",
        "        # Check for nulls\n",
        "        null_counts = df.isnull().sum()\n",
        "        if null_counts.sum() > 0:\n",
        "            print(f\"  ⚠️  Warning: Found {null_counts.sum()} null values\")\n",
        "        \n",
        "        # Check for duplicates\n",
        "        dupes = df.duplicated(['date', 'asset_id']).sum()\n",
        "        if dupes > 0:\n",
        "            raise ValueError(f\"Found {dupes} duplicate (date, asset_id) pairs\")\n",
        "        \n",
        "        print(f\"  ✓ Data validation passed\")\n",
        "\n",
        "\n",
        "# Demonstrate data ingestion\n",
        "print(\"=\"*70)\n",
        "print(\"DATA INGESTION PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "ingestion_pipeline = DataIngestionPipeline()\n",
        "factor_data = ingestion_pipeline.ingest_factor_data(n_periods=120, n_assets=50)\n",
        "\n",
        "print(\"\\nSample data:\")\n",
        "print(factor_data.head(10))\n",
        "\n",
        "print(\"\\nData statistics:\")\n",
        "print(factor_data[['value', 'momentum', 'quality', 'size', 'low_vol', 'return_1m']].describe())"
      ],
      "id": "3618c6ac"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature Engineering with Temporal Correctness"
      ],
      "id": "0d98fe9f-a4fd-44d2-b9b9-5fb8d524a0df"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FeatureEngineeringPipeline:\n",
        "    \"\"\"\n",
        "    Feature engineering ensuring temporal correctness (no look-ahead bias)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.feature_definitions = {}\n",
        "        self.version = \"1.0.0\"\n",
        "        \n",
        "    def engineer_features(self, df, lookback_periods=[3, 6, 12]):\n",
        "        \"\"\"\n",
        "        Engineer temporal features with strict point-in-time correctness\n",
        "        \n",
        "        Key principle: Only use data available at prediction time\n",
        "        \"\"\"\n",
        "        df = df.copy().sort_values(['asset_id', 'date'])\n",
        "        \n",
        "        # Original factors (already point-in-time correct)\n",
        "        features = ['value', 'momentum', 'quality', 'size', 'low_vol']\n",
        "        \n",
        "        # Engineer lagged aggregations (moving averages, volatilities)\n",
        "        for asset_id, asset_df in df.groupby('asset_id'):\n",
        "            for period in lookback_periods:\n",
        "                for factor in ['value', 'momentum', 'quality']:\n",
        "                    # Moving average (using only past data)\n",
        "                    col_name = f'{factor}_ma{period}'\n",
        "                    df.loc[df['asset_id'] == asset_id, col_name] = (\n",
        "                        asset_df[factor].rolling(window=period, min_periods=1).mean()\n",
        "                    )\n",
        "                    \n",
        "                    # Volatility (using only past data)\n",
        "                    col_name = f'{factor}_vol{period}'\n",
        "                    df.loc[df['asset_id'] == asset_id, col_name] = (\n",
        "                        asset_df[factor].rolling(window=period, min_periods=2).std()\n",
        "                    )\n",
        "        \n",
        "        # Fill NaN from rolling windows (first periods)\n",
        "        engineered_features = [c for c in df.columns if '_ma' in c or '_vol' in c]\n",
        "        df[engineered_features] = df[engineered_features].fillna(0)\n",
        "        \n",
        "        print(f\"✓ Features engineered: {len(engineered_features)} new features\")\n",
        "        print(f\"  Lookback periods: {lookback_periods}\")\n",
        "        print(f\"  Total features: {len(features) + len(engineered_features)}\")\n",
        "        \n",
        "        # Verify no look-ahead bias\n",
        "        self._verify_temporal_correctness(df)\n",
        "        \n",
        "        return df\n",
        "    \n",
        "    def _verify_temporal_correctness(self, df):\n",
        "        \"\"\"\n",
        "        Verify features don't use future information\n",
        "        \n",
        "        Check: For prediction at time t, all features use data <= t\n",
        "        \"\"\"\n",
        "        # Sample verification: check if any feature perfectly predicts returns\n",
        "        # (would indicate leakage)\n",
        "        \n",
        "        feature_cols = [c for c in df.columns if c not in ['date', 'asset_id', 'return_1m']]\n",
        "        \n",
        "        # Calculate correlation with future returns\n",
        "        max_corr = 0\n",
        "        max_corr_feature = None\n",
        "        \n",
        "        for col in feature_cols:\n",
        "            corr = abs(df[col].corr(df['return_1m']))\n",
        "            if corr > max_corr:\n",
        "                max_corr = corr\n",
        "                max_corr_feature = col\n",
        "        \n",
        "        if max_corr > 0.9:  # Suspiciously high correlation\n",
        "            print(f\"  ⚠️  Warning: Feature {max_corr_feature} has correlation {max_corr:.3f} with returns\")\n",
        "            print(f\"     This might indicate look-ahead bias!\")\n",
        "        else:\n",
        "            print(f\"  ✓ Temporal correctness verified (max correlation: {max_corr:.3f})\")\n",
        "\n",
        "\n",
        "# Demonstrate feature engineering\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FEATURE ENGINEERING PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "feature_pipeline = FeatureEngineeringPipeline()\n",
        "factor_data_with_features = feature_pipeline.engineer_features(factor_data, lookback_periods=[3, 6, 12])\n",
        "\n",
        "print(\"\\nEngineered features sample:\")\n",
        "feature_cols = [c for c in factor_data_with_features.columns if '_ma' in c or '_vol' in c]\n",
        "print(factor_data_with_features[['date', 'asset_id'] + feature_cols[:6]].head(10))"
      ],
      "id": "14319e57"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training with Versioning"
      ],
      "id": "0d8718e9-f717-43f7-9aeb-b8f2c5f5d118"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "class ModelTrainingPipeline:\n",
        "    \"\"\"\n",
        "    Model training with versioning and metadata tracking\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.models = {}\n",
        "        self.scalers = {}\n",
        "        self.metadata = {}\n",
        "        self.version_counter = 0\n",
        "        \n",
        "    def train_factor_model(self, df, train_end_date, features=None):\n",
        "        \"\"\"\n",
        "        Train factor model with proper train/test split\n",
        "        \n",
        "        Args:\n",
        "            df: Full dataset\n",
        "            train_end_date: Last date for training (anything after is test)\n",
        "            features: List of feature columns (if None, use all numeric except target)\n",
        "        \"\"\"\n",
        "        # Split data temporally (no random split - that would leak!)\n",
        "        train_df = df[df['date'] <= train_end_date].copy()\n",
        "        test_df = df[df['date'] > train_end_date].copy()\n",
        "        \n",
        "        # Select features\n",
        "        if features is None:\n",
        "            exclude_cols = ['date', 'asset_id', 'return_1m']\n",
        "            features = [c for c in df.columns if c not in exclude_cols]\n",
        "        \n",
        "        X_train = train_df[features]\n",
        "        y_train = train_df['return_1m']\n",
        "        X_test = test_df[features]\n",
        "        y_test = test_df['return_1m']\n",
        "        \n",
        "        # Scale features (fit on train only!)\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        # Train model\n",
        "        model = Ridge(alpha=1.0)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        # Evaluate\n",
        "        train_score = model.score(X_train_scaled, y_train)\n",
        "        test_score = model.score(X_test_scaled, y_test)\n",
        "        \n",
        "        train_pred = model.predict(X_train_scaled)\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        train_mse = np.mean((y_train - train_pred) ** 2)\n",
        "        test_mse = np.mean((y_test - test_pred) ** 2)\n",
        "        \n",
        "        # Version and store\n",
        "        version_id = f\"v{self.version_counter}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "        self.version_counter += 1\n",
        "        \n",
        "        self.models[version_id] = model\n",
        "        self.scalers[version_id] = scaler\n",
        "        self.metadata[version_id] = {\n",
        "            'train_end_date': train_end_date,\n",
        "            'features': features,\n",
        "            'n_train': len(train_df),\n",
        "            'n_test': len(test_df),\n",
        "            'train_r2': train_score,\n",
        "            'test_r2': test_score,\n",
        "            'train_mse': train_mse,\n",
        "            'test_mse': test_mse,\n",
        "            'model_type': 'Ridge',\n",
        "            'hyperparameters': {'alpha': 1.0}\n",
        "        }\n",
        "        \n",
        "        print(f\"\\n✓ Model trained: {version_id}\")\n",
        "        print(f\"  Training period: {train_df['date'].min()} to {train_end_date}\")\n",
        "        print(f\"  Test period: {test_df['date'].min()} to {test_df['date'].max()}\")\n",
        "        print(f\"  Training samples: {len(train_df):,}\")\n",
        "        print(f\"  Test samples: {len(test_df):,}\")\n",
        "        print(f\"  Features: {len(features)}\")\n",
        "        print(f\"\\n  Performance:\")\n",
        "        print(f\"    Train R²: {train_score:.4f}\")\n",
        "        print(f\"    Test R²:  {test_score:.4f}\")\n",
        "        print(f\"    Train MSE: {train_mse:.6f}\")\n",
        "        print(f\"    Test MSE:  {test_mse:.6f}\")\n",
        "        \n",
        "        return version_id, model, scaler\n",
        "    \n",
        "    def get_model_metadata(self, version_id):\n",
        "        \"\"\"Retrieve model metadata for versioning and auditing\"\"\"\n",
        "        return self.metadata.get(version_id, {})\n",
        "\n",
        "\n",
        "# Demonstrate model training\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"MODEL TRAINING PIPELINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "training_pipeline = ModelTrainingPipeline()\n",
        "\n",
        "# Train model with 80-20 temporal split\n",
        "all_dates = sorted(factor_data_with_features['date'].unique())\n",
        "split_idx = int(len(all_dates) * 0.8)\n",
        "train_end_date = all_dates[split_idx]\n",
        "\n",
        "version_id, model, scaler = training_pipeline.train_factor_model(\n",
        "    factor_data_with_features,\n",
        "    train_end_date\n",
        ")\n",
        "\n",
        "# Show model coefficients (feature importance)\n",
        "feature_cols = [c for c in factor_data_with_features.columns \n",
        "                if c not in ['date', 'asset_id', 'return_1m']]\n",
        "coefficients = model.coef_\n",
        "\n",
        "print(f\"\\nTop 10 features by absolute coefficient:\")\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'coefficient': coefficients\n",
        "}).sort_values('coefficient', key=abs, ascending=False)\n",
        "\n",
        "print(feature_importance.head(10).to_string(index=False))"
      ],
      "id": "227ddcdd"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 1)\n",
        "\n",
        "Write 200-250 words addressing:\n",
        "\n",
        "1.  **Pipeline Benefits**: How does organizing ML code into pipeline\n",
        "    components (ingestion, features, training) improve maintainability\n",
        "    compared to monolithic scripts? What challenges does this introduce?\n",
        "\n",
        "2.  **Temporal Correctness**: Why is temporal correctness critical for\n",
        "    financial ML? What would happen if we accidentally used future\n",
        "    information in features? How can we systematically verify temporal\n",
        "    correctness?\n",
        "\n",
        "3.  **Model Versioning**: What information should model versions track\n",
        "    for production systems? How does versioning enable debugging,\n",
        "    auditing, and rollback capabilities?\n",
        "\n",
        "## Exercise 2: Rigorous Backtesting with Multiple Testing Correction\n",
        "\n",
        "### The Multiple Testing Problem"
      ],
      "id": "d254a144-b10a-46cb-9a6e-319ef2345588"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def demonstrate_multiple_testing_problem(n_random_features=100, n_samples=1000, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Demonstrate how testing many random features produces false discoveries\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    # Generate random features (no predictive power)\n",
        "    X_random = np.random.randn(n_samples, n_random_features)\n",
        "    y = np.random.randn(n_samples)  # Random returns\n",
        "    \n",
        "    # Test each feature for significance\n",
        "    p_values = []\n",
        "    correlations = []\n",
        "    \n",
        "    for i in range(n_random_features):\n",
        "        corr = np.corrcoef(X_random[:, i], y)[0, 1]\n",
        "        # T-test for correlation significance\n",
        "        t_stat = corr * np.sqrt(n_samples - 2) / np.sqrt(1 - corr**2)\n",
        "        p_val = 2 * (1 - stats.t.cdf(abs(t_stat), n_samples - 2))\n",
        "        \n",
        "        p_values.append(p_val)\n",
        "        correlations.append(abs(corr))\n",
        "    \n",
        "    # Count \"significant\" features (without correction)\n",
        "    significant_uncorrected = sum(p < alpha for p in p_values)\n",
        "    \n",
        "    # Apply Bonferroni correction\n",
        "    alpha_bonferroni = alpha / n_random_features\n",
        "    significant_bonferroni = sum(p < alpha_bonferroni for p in p_values)\n",
        "    \n",
        "    # Apply Benjamini-Hochberg FDR correction\n",
        "    rejected, p_corrected, _, _ = multipletests(p_values, alpha=alpha, method='fdr_bh')\n",
        "    significant_fdr = sum(rejected)\n",
        "    \n",
        "    print(\"=\"*70)\n",
        "    print(\"MULTIPLE TESTING DEMONSTRATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nTesting {n_random_features} random features (no true predictive power)\")\n",
        "    print(f\"Sample size: {n_samples}\")\n",
        "    print(f\"Significance level: α = {alpha}\")\n",
        "    \n",
        "    print(f\"\\nResults WITHOUT correction:\")\n",
        "    print(f\"  'Significant' features: {significant_uncorrected}\")\n",
        "    print(f\"  Expected false positives: {n_random_features * alpha:.1f}\")\n",
        "    print(f\"  → {significant_uncorrected/n_random_features*100:.1f}% of features appear significant!\")\n",
        "    \n",
        "    print(f\"\\nResults WITH Bonferroni correction:\")\n",
        "    print(f\"  Significant features: {significant_bonferroni}\")\n",
        "    print(f\"  → Properly controls false positives\")\n",
        "    \n",
        "    print(f\"\\nResults WITH FDR correction:\")\n",
        "    print(f\"  Significant features: {significant_fdr}\")\n",
        "    print(f\"  → Balances power and error control\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # P-value distribution\n",
        "    axes[0].hist(p_values, bins=20, edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(alpha, color='red', linestyle='--', linewidth=2, label=f'α = {alpha}')\n",
        "    axes[0].axvline(alpha_bonferroni, color='green', linestyle='--', linewidth=2, \n",
        "                    label=f'Bonferroni = {alpha_bonferroni:.4f}')\n",
        "    axes[0].set_xlabel('P-value')\n",
        "    axes[0].set_ylabel('Frequency')\n",
        "    axes[0].set_title('P-value Distribution (Random Features)', fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Sorted p-values with FDR threshold\n",
        "    sorted_p = sorted(p_values)\n",
        "    ranks = np.arange(1, len(sorted_p) + 1)\n",
        "    fdr_threshold = alpha * ranks / n_random_features\n",
        "    \n",
        "    axes[1].plot(ranks, sorted_p, 'o', markersize=4, alpha=0.6, label='P-values')\n",
        "    axes[1].plot(ranks, fdr_threshold, 'r--', linewidth=2, label='FDR threshold')\n",
        "    axes[1].set_xlabel('Rank')\n",
        "    axes[1].set_ylabel('P-value')\n",
        "    axes[1].set_title('Benjamini-Hochberg FDR Procedure', fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return p_values, significant_uncorrected, significant_bonferroni, significant_fdr\n",
        "\n",
        "\n",
        "# Run demonstration\n",
        "p_vals, n_uncorr, n_bonf, n_fdr = demonstrate_multiple_testing_problem()\n",
        "\n",
        "print(\"\\n💡 Key Insight: Testing many features guarantees false discoveries\")\n",
        "print(\"   Multiple testing correction is ESSENTIAL for valid research!\")"
      ],
      "id": "b68c33b7"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing Combinatorial Purged Cross-Validation"
      ],
      "id": "076e79e6-be7b-45be-bdcd-874510ea2e4a"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combinatorial_purged_cv(df, features, n_splits=5, embargo_pct=0.05):\n",
        "    \"\"\"\n",
        "    Implement Bailey & López de Prado's Combinatorial Purged Cross-Validation\n",
        "    \n",
        "    Steps:\n",
        "    1. Create multiple train/test splits\n",
        "    2. Purge training samples near test samples (prevent leakage)\n",
        "    3. Add embargo period (account for label lag)\n",
        "    4. Train model on each split\n",
        "    5. Compute performance metrics\n",
        "    6. Calculate Probability of Backtest Overfitting (PBO)\n",
        "    \n",
        "    Args:\n",
        "        df: Dataset with features and returns\n",
        "        features: List of feature column names\n",
        "        n_splits: Number of CV splits\n",
        "        embargo_pct: Percentage of data to embargo (gap between train/test)\n",
        "    \"\"\"\n",
        "    df = df.sort_values('date').reset_index(drop=True)\n",
        "    dates = sorted(df['date'].unique())\n",
        "    n_dates = len(dates)\n",
        "    \n",
        "    # Create split indices\n",
        "    split_size = n_dates // n_splits\n",
        "    embargo_periods = int(split_size * embargo_pct)\n",
        "    \n",
        "    # Generate combinations of splits for train/test\n",
        "    # Use subset of all possible combinations (computational limit)\n",
        "    split_indices = list(range(n_splits))\n",
        "    n_combinations = min(10, 2 ** (n_splits - 1))  # Limit combinations\n",
        "    \n",
        "    results = []\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"COMBINATORIAL PURGED CROSS-VALIDATION\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"  Splits: {n_splits}\")\n",
        "    print(f\"  Embargo: {embargo_pct*100:.0f}% ({embargo_periods} periods)\")\n",
        "    print(f\"  Testing {n_combinations} train/test combinations\")\n",
        "    \n",
        "    for combo_idx in range(n_combinations):\n",
        "        # Randomly select test splits\n",
        "        np.random.seed(combo_idx)\n",
        "        test_splits = np.random.choice(split_indices, size=max(1, n_splits // 3), replace=False)\n",
        "        train_splits = [s for s in split_indices if s not in test_splits]\n",
        "        \n",
        "        # Convert splits to date ranges\n",
        "        test_dates = set()\n",
        "        for split_idx in test_splits:\n",
        "            start_idx = split_idx * split_size\n",
        "            end_idx = min((split_idx + 1) * split_size, n_dates)\n",
        "            test_dates.update(dates[start_idx:end_idx])\n",
        "        \n",
        "        train_dates = set()\n",
        "        for split_idx in train_splits:\n",
        "            start_idx = split_idx * split_size\n",
        "            end_idx = min((split_idx + 1) * split_size, n_dates)\n",
        "            \n",
        "            # Purge: remove dates close to test dates\n",
        "            split_dates = dates[start_idx:end_idx]\n",
        "            for date in split_dates:\n",
        "                # Check if date is too close to any test date\n",
        "                too_close = False\n",
        "                for test_date in test_dates:\n",
        "                    date_diff = abs((date - test_date).days)\n",
        "                    if date_diff < embargo_periods * 30:  # Assuming monthly data\n",
        "                        too_close = True\n",
        "                        break\n",
        "                \n",
        "                if not too_close:\n",
        "                    train_dates.add(date)\n",
        "        \n",
        "        # Create train/test sets\n",
        "        train_df = df[df['date'].isin(train_dates)]\n",
        "        test_df = df[df['date'].isin(test_dates)]\n",
        "        \n",
        "        if len(train_df) < 100 or len(test_df) < 50:\n",
        "            continue  # Skip if insufficient data\n",
        "        \n",
        "        # Train model\n",
        "        X_train = train_df[features]\n",
        "        y_train = train_df['return_1m']\n",
        "        X_test = test_df[features]\n",
        "        y_test = test_df['return_1m']\n",
        "        \n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        \n",
        "        model = Ridge(alpha=1.0)\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        \n",
        "        train_pred = model.predict(X_train_scaled)\n",
        "        test_pred = model.predict(X_test_scaled)\n",
        "        \n",
        "        # Calculate Sharpe ratios (common financial metric)\n",
        "        train_sharpe = np.mean(train_pred) / (np.std(train_pred) + 1e-6) * np.sqrt(12)\n",
        "        test_sharpe = np.mean(test_pred) / (np.std(test_pred) + 1e-6) * np.sqrt(12)\n",
        "        \n",
        "        results.append({\n",
        "            'combo': combo_idx,\n",
        "            'train_sharpe': train_sharpe,\n",
        "            'test_sharpe': test_sharpe,\n",
        "            'train_size': len(train_df),\n",
        "            'test_size': len(test_df)\n",
        "        })\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Calculate Probability of Backtest Overfitting (PBO)\n",
        "    median_train_sharpe = results_df['train_sharpe'].median()\n",
        "    pbo = (results_df['test_sharpe'] < median_train_sharpe).mean()\n",
        "    \n",
        "    print(f\"\\n\" + \"-\"*70)\n",
        "    print(\"RESULTS\")\n",
        "    print(\"-\"*70)\n",
        "    print(f\"\\nCombinations tested: {len(results_df)}\")\n",
        "    print(f\"Median train Sharpe: {median_train_sharpe:.4f}\")\n",
        "    print(f\"Mean test Sharpe: {results_df['test_sharpe'].mean():.4f}\")\n",
        "    print(f\"\\nProbability of Backtest Overfitting (PBO): {pbo:.4f}\")\n",
        "    \n",
        "    if pbo > 0.5:\n",
        "        print(f\"  ⚠️  HIGH OVERFITTING RISK! (PBO > 0.5)\")\n",
        "        print(f\"     Strategy likely captured noise, not signal\")\n",
        "    elif pbo > 0.3:\n",
        "        print(f\"  ⚠️  MODERATE OVERFITTING RISK (PBO > 0.3)\")\n",
        "        print(f\"     Proceed with caution, validate further\")\n",
        "    else:\n",
        "        print(f\"  ✓ LOW OVERFITTING RISK (PBO < 0.3)\")\n",
        "        print(f\"     Strategy appears robust\")\n",
        "    \n",
        "    # Visualize\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "    \n",
        "    # Train vs Test Sharpe\n",
        "    axes[0].scatter(results_df['train_sharpe'], results_df['test_sharpe'], alpha=0.6, s=50)\n",
        "    axes[0].plot([results_df['train_sharpe'].min(), results_df['train_sharpe'].max()],\n",
        "                 [results_df['train_sharpe'].min(), results_df['train_sharpe'].max()],\n",
        "                 'r--', label='45° line')\n",
        "    axes[0].axvline(median_train_sharpe, color='green', linestyle='--', label='Median train')\n",
        "    axes[0].set_xlabel('Train Sharpe Ratio')\n",
        "    axes[0].set_ylabel('Test Sharpe Ratio')\n",
        "    axes[0].set_title('Train vs Test Performance', fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "    \n",
        "    # Distribution of test Sharpe\n",
        "    axes[1].hist(results_df['test_sharpe'], bins=15, edgecolor='black', alpha=0.7)\n",
        "    axes[1].axvline(median_train_sharpe, color='green', linestyle='--', linewidth=2,\n",
        "                    label=f'Median train = {median_train_sharpe:.3f}')\n",
        "    axes[1].axvline(results_df['test_sharpe'].mean(), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Mean test = {results_df[\"test_sharpe\"].mean():.3f}')\n",
        "    axes[1].set_xlabel('Test Sharpe Ratio')\n",
        "    axes[1].set_ylabel('Frequency')\n",
        "    axes[1].set_title(f'Test Performance Distribution (PBO={pbo:.3f})', fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results_df, pbo\n",
        "\n",
        "\n",
        "# Run combinatorial purged CV\n",
        "feature_cols = [c for c in factor_data_with_features.columns \n",
        "                if c not in ['date', 'asset_id', 'return_1m']]\n",
        "\n",
        "cv_results, pbo = combinatorial_purged_cv(\n",
        "    factor_data_with_features,\n",
        "    feature_cols,\n",
        "    n_splits=5,\n",
        "    embargo_pct=0.05\n",
        ")"
      ],
      "id": "117afe4f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 2)\n",
        "\n",
        "Write 250-300 words addressing:\n",
        "\n",
        "1.  **Multiple Testing Impact**: In the demonstration, ~5 features\n",
        "    appeared significant by chance. How would this affect a researcher\n",
        "    who tests 100 features and publishes the “significant” ones? What\n",
        "    are the consequences for financial markets if overfit strategies are\n",
        "    widely adopted?\n",
        "\n",
        "2.  **PBO Interpretation**: The Probability of Backtest Overfitting\n",
        "    measures what fraction of test periods underperform the median\n",
        "    training performance. Why is this a useful metric for detecting\n",
        "    overfitting? What PBO threshold should trigger concern?\n",
        "\n",
        "3.  **Purging and Embargo**: Why do we purge training samples near test\n",
        "    periods and add embargo gaps? What would happen if we skipped these\n",
        "    steps? How do these relate to the temporal structure of financial\n",
        "    data?\n",
        "\n",
        "## Exercise 3: Production Monitoring and Drift Detection\n",
        "\n",
        "### Simulating Production Deployment"
      ],
      "id": "1bf3bbaa-517e-4251-9f21-6656c9f6e089"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ProductionMonitor:\n",
        "    \"\"\"\n",
        "    Monitor model performance and detect drift in production\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, model, scaler, features, baseline_data):\n",
        "        self.model = model\n",
        "        self.scaler = scaler\n",
        "        self.features = features\n",
        "        \n",
        "        # Calculate baseline statistics for drift detection\n",
        "        self.baseline_mean = baseline_data[features].mean()\n",
        "        self.baseline_std = baseline_data[features].std()\n",
        "        \n",
        "        # Performance tracking\n",
        "        self.performance_history = []\n",
        "        \n",
        "    def predict_and_monitor(self, df):\n",
        "        \"\"\"\n",
        "        Make predictions and monitor for drift/degradation\n",
        "        \"\"\"\n",
        "        X = df[self.features]\n",
        "        X_scaled = self.scaler.transform(X)\n",
        "        predictions = self.model.predict(X_scaled)\n",
        "        \n",
        "        # Calculate performance (when ground truth available)\n",
        "        if 'return_1m' in df.columns:\n",
        "            mse = np.mean((df['return_1m'] - predictions) ** 2)\n",
        "            mae = np.mean(np.abs(df['return_1m'] - predictions))\n",
        "            corr = np.corrcoef(df['return_1m'], predictions)[0, 1]\n",
        "            \n",
        "            sharpe = np.mean(predictions) / (np.std(predictions) + 1e-6) * np.sqrt(12)\n",
        "            \n",
        "            self.performance_history.append({\n",
        "                'timestamp': datetime.now(),\n",
        "                'n_samples': len(df),\n",
        "                'mse': mse,\n",
        "                'mae': mae,\n",
        "                'correlation': corr,\n",
        "                'sharpe': sharpe\n",
        "            })\n",
        "        \n",
        "        # Detect data drift (Population Stability Index)\n",
        "        psi_scores = self._calculate_psi(df[self.features])\n",
        "        \n",
        "        # Alert if drift detected\n",
        "        alerts = []\n",
        "        for feature, psi in psi_scores.items():\n",
        "            if psi > 0.25:  # Significant drift threshold\n",
        "                alerts.append(f\"⚠️  DRIFT ALERT: {feature} (PSI={psi:.3f})\")\n",
        "        \n",
        "        return predictions, psi_scores, alerts\n",
        "    \n",
        "    def _calculate_psi(self, current_data):\n",
        "        \"\"\"\n",
        "        Calculate Population Stability Index for each feature\n",
        "        \n",
        "        PSI measures how much the distribution has shifted\n",
        "        PSI < 0.1: No significant change\n",
        "        0.1 < PSI < 0.25: Moderate change\n",
        "        PSI > 0.25: Significant change\n",
        "        \"\"\"\n",
        "        psi_scores = {}\n",
        "        \n",
        "        for feature in self.features:\n",
        "            # Use baseline and current distributions\n",
        "            baseline_values = self.baseline_mean[feature]\n",
        "            current_mean = current_data[feature].mean()\n",
        "            \n",
        "            # Simplified PSI calculation\n",
        "            # In production, use proper binned distributions\n",
        "            if self.baseline_std[feature] > 0:\n",
        "                z_score = abs(current_mean - baseline_values) / self.baseline_std[feature]\n",
        "                psi = z_score / 10  # Approximate PSI\n",
        "            else:\n",
        "                psi = 0\n",
        "            \n",
        "            psi_scores[feature] = psi\n",
        "        \n",
        "        return psi_scores\n",
        "    \n",
        "    def generate_monitoring_report(self):\n",
        "        \"\"\"Generate monitoring dashboard\"\"\"\n",
        "        if not self.performance_history:\n",
        "            print(\"No performance data available yet\")\n",
        "            return\n",
        "        \n",
        "        perf_df = pd.DataFrame(self.performance_history)\n",
        "        \n",
        "        print(\"\\n\" + \"=\"*70)\n",
        "        print(\"PRODUCTION MONITORING REPORT\")\n",
        "        print(\"=\"*70)\n",
        "        \n",
        "        print(f\"\\nPerformance Summary (last {len(perf_df)} evaluations):\")\n",
        "        print(f\"  MSE:  mean={perf_df['mse'].mean():.6f}, std={perf_df['mse'].std():.6f}\")\n",
        "        print(f\"  MAE:  mean={perf_df['mae'].mean():.6f}, std={perf_df['mae'].std():.6f}\")\n",
        "        print(f\"  Corr: mean={perf_df['correlation'].mean():.4f}, std={perf_df['correlation'].std():.4f}\")\n",
        "        print(f\"  Sharpe: mean={perf_df['sharpe'].mean():.4f}, std={perf_df['sharpe'].std():.4f}\")\n",
        "        \n",
        "        # Check for degradation\n",
        "        if len(perf_df) >= 5:\n",
        "            recent_sharpe = perf_df['sharpe'].iloc[-3:].mean()\n",
        "            historical_sharpe = perf_df['sharpe'].iloc[:-3].mean()\n",
        "            \n",
        "            if recent_sharpe < historical_sharpe - 0.5:\n",
        "                print(f\"\\n⚠️  PERFORMANCE DEGRADATION DETECTED!\")\n",
        "                print(f\"   Recent Sharpe: {recent_sharpe:.4f}\")\n",
        "                print(f\"   Historical Sharpe: {historical_sharpe:.4f}\")\n",
        "                print(f\"   Consider retraining or rolling back model\")\n",
        "        \n",
        "        # Visualize performance over time\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "        \n",
        "        # MSE over time\n",
        "        axes[0, 0].plot(range(len(perf_df)), perf_df['mse'], marker='o', linewidth=2)\n",
        "        axes[0, 0].set_xlabel('Evaluation Period')\n",
        "        axes[0, 0].set_ylabel('MSE')\n",
        "        axes[0, 0].set_title('Mean Squared Error Over Time', fontweight='bold')\n",
        "        axes[0, 0].grid(alpha=0.3)\n",
        "        \n",
        "        # Correlation over time\n",
        "        axes[0, 1].plot(range(len(perf_df)), perf_df['correlation'], marker='o', linewidth=2, color='green')\n",
        "        axes[0, 1].axhline(0, color='red', linestyle='--', alpha=0.3)\n",
        "        axes[0, 1].set_xlabel('Evaluation Period')\n",
        "        axes[0, 1].set_ylabel('Correlation')\n",
        "        axes[0, 1].set_title('Prediction-Actual Correlation Over Time', fontweight='bold')\n",
        "        axes[0, 1].grid(alpha=0.3)\n",
        "        \n",
        "        # Sharpe ratio over time\n",
        "        axes[1, 0].plot(range(len(perf_df)), perf_df['sharpe'], marker='o', linewidth=2, color='purple')\n",
        "        axes[1, 0].axhline(0, color='red', linestyle='--', alpha=0.3)\n",
        "        axes[1, 0].set_xlabel('Evaluation Period')\n",
        "        axes[1, 0].set_ylabel('Sharpe Ratio')\n",
        "        axes[1, 0].set_title('Sharpe Ratio Over Time', fontweight='bold')\n",
        "        axes[1, 0].grid(alpha=0.3)\n",
        "        \n",
        "        # Performance distribution\n",
        "        axes[1, 1].hist(perf_df['sharpe'], bins=10, edgecolor='black', alpha=0.7, color='orange')\n",
        "        axes[1, 1].axvline(perf_df['sharpe'].mean(), color='red', linestyle='--', linewidth=2,\n",
        "                          label=f'Mean = {perf_df[\"sharpe\"].mean():.3f}')\n",
        "        axes[1, 1].set_xlabel('Sharpe Ratio')\n",
        "        axes[1, 1].set_ylabel('Frequency')\n",
        "        axes[1, 1].set_title('Sharpe Ratio Distribution', fontweight='bold')\n",
        "        axes[1, 1].legend()\n",
        "        axes[1, 1].grid(alpha=0.3)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "# Demonstrate production monitoring\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"PRODUCTION DEPLOYMENT SIMULATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Use trained model from Exercise 1\n",
        "all_dates = sorted(factor_data_with_features['date'].unique())\n",
        "test_start_idx = int(len(all_dates) * 0.8)\n",
        "test_dates = all_dates[test_start_idx:]\n",
        "\n",
        "# Create baseline from training data\n",
        "train_data = factor_data_with_features[\n",
        "    factor_data_with_features['date'] <= all_dates[test_start_idx]\n",
        "]\n",
        "\n",
        "# Initialize monitor\n",
        "monitor = ProductionMonitor(model, scaler, feature_cols, train_data)\n",
        "\n",
        "# Simulate production: process data in monthly batches\n",
        "print(\"\\nSimulating production deployment with monthly monitoring:\")\n",
        "print(\"-\"*70)\n",
        "\n",
        "for i, date in enumerate(test_dates):\n",
        "    batch_data = factor_data_with_features[factor_data_with_features['date'] == date]\n",
        "    \n",
        "    predictions, psi_scores, alerts = monitor.predict_and_monitor(batch_data)\n",
        "    \n",
        "    print(f\"\\nPeriod {i+1} ({date.strftime('%Y-%m')}): {len(batch_data)} predictions\")\n",
        "    \n",
        "    # Show drift warnings\n",
        "    max_psi = max(psi_scores.values())\n",
        "    if max_psi > 0.25:\n",
        "        print(f\"  ⚠️  High drift detected (max PSI={max_psi:.3f})\")\n",
        "        for alert in alerts[:3]:  # Show top 3\n",
        "            print(f\"     {alert}\")\n",
        "    else:\n",
        "        print(f\"  ✓ No significant drift (max PSI={max_psi:.3f})\")\n",
        "\n",
        "# Generate comprehensive monitoring report\n",
        "monitor.generate_monitoring_report()"
      ],
      "id": "f0d9465d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions (Exercise 3)\n",
        "\n",
        "Write 200-250 words addressing:\n",
        "\n",
        "1.  **Monitoring Strategy**: What metrics should production ML systems\n",
        "    monitor? Why track both model performance (MSE, correlation) and\n",
        "    data characteristics (PSI, drift)? How quickly should monitoring\n",
        "    systems detect issues?\n",
        "\n",
        "2.  **Drift Response**: When data drift is detected, what actions should\n",
        "    be taken? When is retraining necessary versus other interventions\n",
        "    (feature engineering, model rollback, investigation)? What are the\n",
        "    risks of over-reacting to drift versus under-reacting?\n",
        "\n",
        "3.  **Production vs Research**: This lab demonstrated many differences\n",
        "    between research ML and production ML. What are the three most\n",
        "    important differences? How do these differences affect how\n",
        "    organizations should structure their ML teams and processes?\n",
        "\n",
        "## Summary and Integration\n",
        "\n",
        "### What We’ve Learned\n",
        "\n",
        "Through these exercises, you’ve:\n",
        "\n",
        "1.  **Implemented end-to-end ML pipeline** with data ingestion, feature\n",
        "    engineering, training, and versioning\n",
        "\n",
        "2.  **Ensured temporal correctness** preventing look-ahead bias that\n",
        "    creates unrealistic performance\n",
        "\n",
        "3.  **Applied multiple testing corrections** preventing false\n",
        "    discoveries from testing many features\n",
        "\n",
        "4.  **Implemented combinatorial purged CV** with proper train/test\n",
        "    separation for time-series\n",
        "\n",
        "5.  **Calculated PBO** quantifying probability that backtest performance\n",
        "    resulted from overfitting\n",
        "\n",
        "6.  **Monitored production deployment** detecting drift and performance\n",
        "    degradation\n",
        "\n",
        "7.  **Understood overfitting pervasiveness** in financial ML requiring\n",
        "    rigorous validation\n",
        "\n",
        "### Connections to Course Themes\n",
        "\n",
        "-   **Week 9 (Smart Contracts)**: Both smart contracts and ML models\n",
        "    make consequential decisions—require rigorous testing before\n",
        "    deployment\n",
        "\n",
        "-   **Week 8 (Fraud Detection)**: Production fraud detection exemplifies\n",
        "    real-time ML pipelines with monitoring\n",
        "\n",
        "-   **Week 4 (Robo-Advisors)**: Portfolio optimization at scale requires\n",
        "    production ML infrastructure\n",
        "\n",
        "-   **Throughout course**: Evidence-based evaluation—don’t trust\n",
        "    impressive backtests without rigorous validation\n",
        "\n",
        "### Critical Evaluation Framework\n",
        "\n",
        "When evaluating ML systems or research:\n",
        "\n",
        "1.  **Validation rigor**: Are multiple testing corrections applied? Is\n",
        "    out-of-sample validation proper?\n",
        "2.  **Temporal correctness**: Are features point-in-time correct? Any\n",
        "    look-ahead bias?\n",
        "3.  **Production readiness**: Is there monitoring? Drift detection?\n",
        "    Rollback capability?\n",
        "4.  **Reproducibility**: Can results be reproduced? Is data/code\n",
        "    versioned?\n",
        "5.  **Business value**: Do model improvements translate to business\n",
        "    impact?\n",
        "\n",
        "### Assessment Preparation\n",
        "\n",
        "**FIN510 Coursework 2**: Apply rigorous backtesting to factor\n",
        "replication—use multiple testing corrections, implement proper temporal\n",
        "splits, calculate PBO, validate out-of-sample.\n",
        "\n",
        "**FIN720**: Critically evaluate ML applications in finance—assess\n",
        "validation rigor, production readiness, and gap between research claims\n",
        "and deployment reality.\n",
        "\n",
        "### Further Exploration\n",
        "\n",
        "If interested in extending your analysis:\n",
        "\n",
        "-   **Advanced feature stores**: Implement Feast or Tecton for feature\n",
        "    consistency\n",
        "-   **Continuous training**: Automate retraining pipelines triggered by\n",
        "    drift detection\n",
        "-   **Fairness monitoring**: Track model performance across demographic\n",
        "    segments\n",
        "-   **Explainability**: Implement SHAP or LIME for model\n",
        "    interpretability\n",
        "-   **Regulatory compliance**: Document models following SR 11-7\n",
        "    framework\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Advanced Extension: Asset Embeddings as Learned Features (FIN720)\n",
        "\n",
        "> **Optional Advanced Module**\n",
        ">\n",
        "> This extension is designed for FIN720 students seeking to demonstrate\n",
        "> technical sophistication. It connects to cutting-edge research (Gabaix\n",
        "> et al. 2025) and provides optional enhancement for Coursework 2.\n",
        ">\n",
        "> **Expected time**: 45-60 minutes\n",
        "\n",
        "### Motivation: Beyond Hand-Crafted Characteristics\n",
        "\n",
        "Traditional factor models use characteristics we believe matter—size,\n",
        "value, momentum. But what if we learned representations directly from\n",
        "portfolio holdings? This exercise demonstrates how embedding techniques\n",
        "from NLP translate to finance, bridging Week 4’s discussion of\n",
        "algorithmic investment with production ML concepts.\n",
        "\n",
        "The key insight from Gabaix et al. (2025): portfolio holdings encode\n",
        "rich information about asset relationships. Assets appearing in similar\n",
        "portfolios likely share investment characteristics—just as words\n",
        "appearing in similar contexts have related meanings. We can learn these\n",
        "relationships without hand-crafting features.\n",
        "\n",
        "### Simplified Holdings-Based Embeddings\n",
        "\n",
        "We’ll implement a lightweight version using synthetic mutual fund\n",
        "holdings:"
      ],
      "id": "5a2eadd2-b3fd-4066-8541-13cc46dcbe38"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === Asset Embeddings Implementation ===\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.spatial.distance import cosine\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ASSET EMBEDDINGS: LEARNING FROM PORTFOLIO HOLDINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate synthetic portfolio holdings\n",
        "# In practice, use 13F filings or mutual fund holdings data\n",
        "np.random.seed(42)\n",
        "\n",
        "n_funds = 100  # Number of institutional portfolios\n",
        "n_assets = 50  # Number of assets in universe\n",
        "\n",
        "print(f\"\\nGenerating synthetic holdings data:\")\n",
        "print(f\"  Portfolios: {n_funds}\")\n",
        "print(f\"  Assets: {n_assets}\")\n",
        "\n",
        "# Create holdings matrix using latent factor structure\n",
        "# This simulates reality: portfolios share common themes (value, growth, sector)\n",
        "n_latent_factors = 5\n",
        "fund_loadings = np.random.randn(n_funds, n_latent_factors)\n",
        "asset_loadings = np.random.randn(n_assets, n_latent_factors)\n",
        "\n",
        "# Holdings as exp(fund_factor • asset_factor) for log-normality\n",
        "holdings_raw = np.exp(fund_loadings @ asset_loadings.T)\n",
        "\n",
        "# Normalize to portfolio weights (each row sums to 1)\n",
        "holdings = holdings_raw / holdings_raw.sum(axis=1, keepdims=True)\n",
        "\n",
        "# Create DataFrame\n",
        "asset_names = [f\"ASSET_{i:02d}\" for i in range(n_assets)]\n",
        "fund_names = [f\"FUND_{i:03d}\" for i in range(n_funds)]\n",
        "holdings_df = pd.DataFrame(holdings, index=fund_names, columns=asset_names)\n",
        "\n",
        "print(\"\\nSample Holdings (first 5 funds × first 5 assets):\")\n",
        "print(holdings_df.iloc[:5, :5].round(4))\n",
        "print(f\"\\nPortfolio weight verification (each row should sum to ≈1.0):\")\n",
        "print(f\"  Min: {holdings_df.sum(axis=1).min():.4f}\")\n",
        "print(f\"  Max: {holdings_df.sum(axis=1).max():.4f}\")"
      ],
      "id": "b797b8a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Method 1: PCA-Based Embeddings (Recommender System Approach)"
      ],
      "id": "9000352e-c158-43d9-aaa2-2f9c9d0865a9"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PortfolioEmbeddings:\n",
        "    \"\"\"\n",
        "    Learn asset embeddings from portfolio holdings via PCA.\n",
        "    \n",
        "    This is analogous to matrix factorization in recommender systems\n",
        "    (e.g., Netflix learning movie embeddings from user ratings).\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, n_components=10):\n",
        "        self.n_components = n_components\n",
        "        self.scaler = StandardScaler()\n",
        "        self.pca = PCA(n_components=n_components)\n",
        "        self.asset_embeddings_ = None\n",
        "        self.explained_variance_ = None\n",
        "        \n",
        "    def fit(self, holdings_df):\n",
        "        \"\"\"\n",
        "        Learn embeddings from holdings matrix.\n",
        "        \n",
        "        Args:\n",
        "            holdings_df: DataFrame (funds × assets) of portfolio weights\n",
        "        \n",
        "        Returns:\n",
        "            self\n",
        "        \"\"\"\n",
        "        # Transpose: we want to represent assets in terms of fund holdings\n",
        "        # Each asset is characterized by which funds hold it\n",
        "        X = holdings_df.T  # Now assets × funds\n",
        "        \n",
        "        # Standardize (mean-center and scale)\n",
        "        X_scaled = self.scaler.fit_transform(X)\n",
        "        \n",
        "        # Fit PCA: find principal components\n",
        "        self.pca.fit(X_scaled)\n",
        "        \n",
        "        # Asset embeddings are projections onto principal components\n",
        "        self.asset_embeddings_ = pd.DataFrame(\n",
        "            self.pca.transform(X_scaled),\n",
        "            index=holdings_df.columns,  # Asset names\n",
        "            columns=[f\"embed_{i}\" for i in range(self.n_components)]\n",
        "        )\n",
        "        \n",
        "        self.explained_variance_ = self.pca.explained_variance_ratio_\n",
        "        \n",
        "        return self\n",
        "    \n",
        "    def similarity(self, asset1, asset2):\n",
        "        \"\"\"Compute cosine similarity between two assets.\"\"\"\n",
        "        emb1 = self.asset_embeddings_.loc[asset1].values\n",
        "        emb2 = self.asset_embeddings_.loc[asset2].values\n",
        "        return 1 - cosine(emb1, emb2)\n",
        "    \n",
        "    def most_similar(self, asset, top_n=5):\n",
        "        \"\"\"Find most similar assets to a given asset.\"\"\"\n",
        "        target_emb = self.asset_embeddings_.loc[asset].values\n",
        "        \n",
        "        similarities = {}\n",
        "        for other_asset in self.asset_embeddings_.index:\n",
        "            if other_asset != asset:\n",
        "                other_emb = self.asset_embeddings_.loc[other_asset].values\n",
        "                similarities[other_asset] = 1 - cosine(target_emb, other_emb)\n",
        "        \n",
        "        return pd.Series(similarities).nlargest(top_n)\n",
        "\n",
        "# Fit embeddings\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LEARNING EMBEDDINGS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "embedder = PortfolioEmbeddings(n_components=10)\n",
        "embedder.fit(holdings_df)\n",
        "\n",
        "print(f\"\\nLearned {embedder.n_components}-dimensional embeddings for {len(embedder.asset_embeddings_)} assets\")\n",
        "print(f\"\\nVariance explained by components:\")\n",
        "for i, var in enumerate(embedder.explained_variance_[:5]):\n",
        "    print(f\"  Component {i+1}: {var:.1%}\")\n",
        "print(f\"  Total (first 5): {embedder.explained_variance_[:5].sum():.1%}\")\n",
        "\n",
        "print(\"\\nExample: Most similar assets to ASSET_00:\")\n",
        "print(embedder.most_similar(\"ASSET_00\", top_n=5).round(4))"
      ],
      "id": "57b77119"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Validation: Do Embeddings Predict Co-movement?\n",
        "\n",
        "The critical test: do assets with similar embeddings have correlated\n",
        "returns? If embeddings only compress noise, similarity won’t predict\n",
        "co-movement. If they capture real structure, similar assets should move\n",
        "together."
      ],
      "id": "7b3a0eb0-d410-4038-8872-f9c872b9d1e2"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"VALIDATION: EMBEDDINGS vs RETURN COMOVEMENT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Generate synthetic returns using same factor structure\n",
        "# This creates returns with known correlation structure\n",
        "returns = asset_loadings @ np.random.randn(n_latent_factors, 120)  # 120 months\n",
        "returns_df = pd.DataFrame(returns.T, columns=asset_names)\n",
        "\n",
        "# Add idiosyncratic noise\n",
        "returns_df = returns_df + np.random.randn(*returns_df.shape) * 0.5\n",
        "\n",
        "print(f\"\\nGenerated {len(returns_df)} months of returns for {len(asset_names)} assets\")\n",
        "\n",
        "# Compute actual return correlations\n",
        "return_corr = returns_df.corr()\n",
        "\n",
        "# Compute embedding-based similarities for all pairs\n",
        "embedding_sim = pd.DataFrame(\n",
        "    index=asset_names,\n",
        "    columns=asset_names,\n",
        "    dtype=float\n",
        ")\n",
        "\n",
        "for i, asset1 in enumerate(asset_names):\n",
        "    for j, asset2 in enumerate(asset_names):\n",
        "        if i == j:\n",
        "            embedding_sim.loc[asset1, asset2] = 1.0\n",
        "        else:\n",
        "            embedding_sim.loc[asset1, asset2] = embedder.similarity(asset1, asset2)\n",
        "\n",
        "# Extract upper triangle (avoid double-counting pairs)\n",
        "upper_tri_idx = np.triu_indices_from(return_corr.values, k=1)\n",
        "actual_corrs = return_corr.values[upper_tri_idx]\n",
        "embedding_sims = embedding_sim.astype(float).values[upper_tri_idx]\n",
        "\n",
        "# Compute validation metric\n",
        "correlation = np.corrcoef(actual_corrs, embedding_sims)[0, 1]\n",
        "r_squared = correlation ** 2\n",
        "\n",
        "print(f\"\\nValidation Results:\")\n",
        "print(f\"  Correlation between embedding similarity and return correlation: {correlation:.3f}\")\n",
        "print(f\"  R² (variance explained): {r_squared:.1%}\")\n",
        "print(f\"\\n  Interpretation: Embeddings explain {r_squared:.1%} of return comovement\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Scatter plot: embeddings vs. correlations\n",
        "axes[0].scatter(actual_corrs, embedding_sims, alpha=0.3, s=20)\n",
        "axes[0].set_xlabel(\"Actual Return Correlation\", fontsize=11)\n",
        "axes[0].set_ylabel(\"Embedding Similarity\", fontsize=11)\n",
        "axes[0].set_title(\"Embeddings vs. Return Comovement\", fontsize=12, fontweight='bold')\n",
        "axes[0].plot([-1, 1], [-1, 1], 'r--', linewidth=2, label='Perfect Prediction')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "axes[0].text(0.05, 0.95, f\"Correlation: {correlation:.3f}\\nR²: {r_squared:.1%}\",\n",
        "             transform=axes[0].transAxes, verticalalignment='top', fontsize=10,\n",
        "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.7))\n",
        "\n",
        "# 2D visualization of embedding space\n",
        "embeddings_2d = TSNE(n_components=2, random_state=42, perplexity=min(30, n_assets-1)).fit_transform(\n",
        "    embedder.asset_embeddings_.values\n",
        ")\n",
        "\n",
        "axes[1].scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], alpha=0.6, s=50)\n",
        "axes[1].set_xlabel(\"Embedding Dimension 1 (t-SNE)\", fontsize=11)\n",
        "axes[1].set_ylabel(\"Embedding Dimension 2 (t-SNE)\", fontsize=11)\n",
        "axes[1].set_title(\"Asset Embedding Space\", fontsize=12, fontweight='bold')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "# Annotate a few assets\n",
        "for i in [0, 10, 20, 30, 40]:\n",
        "    if i < len(asset_names):\n",
        "        axes[1].annotate(asset_names[i], \n",
        "                         xy=(embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
        "                         xytext=(5, 5), textcoords='offset points', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Validation complete\")"
      ],
      "id": "5c62346f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Discussion Questions (Advanced Extension)\n",
        "\n",
        "Write 250-300 words addressing:\n",
        "\n",
        "1.  **Representation Learning**: How do embeddings differ from\n",
        "    traditional factor exposures (e.g., Fama-French loadings)? What\n",
        "    information might embeddings capture that hand-crafted\n",
        "    characteristics miss? What are the trade-offs?\n",
        "\n",
        "2.  **Temporal Correctness**: Holdings data (13F filings) updates\n",
        "    quarterly with 45-day lag. How does this affect using embeddings\n",
        "    for:\n",
        "\n",
        "    -   Daily portfolio rebalancing?\n",
        "    -   Monthly factor construction?\n",
        "    -   Long-term strategic allocation?\n",
        "\n",
        "    How would you construct point-in-time embeddings avoiding look-ahead\n",
        "    bias?\n",
        "\n",
        "3.  **Production Considerations**:\n",
        "\n",
        "    -   How often should embeddings be retrained as new holdings data\n",
        "        arrives?\n",
        "    -   What drift metrics would detect when embeddings become stale?\n",
        "    -   How computationally expensive is embedding training compared to\n",
        "        computing characteristics?\n",
        "\n",
        "4.  **Governance & Explainability**: Imagine presenting to a risk\n",
        "    committee:\n",
        "\n",
        "    -   “We recommend buying TSLA because it has high embedding\n",
        "        similarity to AAPL.”\n",
        "    -   How would you make this interpretable and defensible?\n",
        "    -   What documentation would satisfy regulators?\n",
        "    -   When should opaque-but-accurate embeddings be preferred over\n",
        "        transparent-but-limited characteristics?\n",
        "\n",
        "5.  **Connection to Coursework 2**: Could embeddings improve your factor\n",
        "    replication?\n",
        "\n",
        "    -   Use first few PCA components as additional factors in alpha\n",
        "        tests\n",
        "    -   Compare characteristic-based vs. embedding-based portfolio\n",
        "        construction\n",
        "    -   Evaluate out-of-sample: do embeddings generalize or overfit?\n",
        "\n",
        "### Extension for Ambitious Students\n",
        "\n",
        "If you want to push further, implement a **masked-asset prediction\n",
        "task**:\n",
        "\n",
        "1.  For each portfolio, randomly mask (hide) 10% of holdings\n",
        "2.  Train model to predict masked assets from observed holdings\n",
        "3.  Evaluate: Do top-10 predictions include actual masked assets?\n",
        "4.  Compare: PCA baseline vs. simple neural network\n",
        "\n",
        "This mirrors Gabaix et al. (2025)’ core methodology and provides\n",
        "hands-on experience with the masked prediction objective used in\n",
        "transformer models (BERT-style).\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "**Connections to Production ML:** - Embeddings are learned\n",
        "features—automatically extracted from data - Must retrain as holdings\n",
        "data arrives (temporal pipeline consideration) - Require validation (do\n",
        "they predict out-of-sample outcomes?) - Trade interpretability for\n",
        "potentially richer representations\n",
        "\n",
        "**Connections to Week 4 (Robo-Advisors):** - Embeddings enable\n",
        "recommendation systems for portfolio construction - Learning from\n",
        "professional investors’ revealed preferences - Potential to democratize\n",
        "sophisticated strategies at lower cost\n",
        "\n",
        "**Connections to Coursework:** - Optional advanced component for FIN720\n",
        "factor replication - Demonstrates technical sophistication and\n",
        "engagement with frontier research - Shows understanding of\n",
        "representation learning in financial contexts\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "**Excellent work! You’ve built production-ready ML pipelines with\n",
        "rigorous validation, understanding the engineering discipline and\n",
        "statistical rigor required for reliable financial ML systems.**\n",
        "\n",
        "Gabaix, Xavier, Ralph S. J. Koijen, Robert Richmond, and Motohiro Yogo.\n",
        "2025. “Asset Embeddings.” Working Paper. SSRN Electronic Journal.\n",
        "<https://doi.org/10.2139/ssrn.4507511>."
      ],
      "id": "6510d9fe-6fb7-427e-866d-c11a5b1a4c7c"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "fin510",
      "display_name": "FIN510 Python",
      "language": "python",
      "path": "/Users/quinference/Library/Jupyter/kernels/fin510"
    }
  }
}