{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 2: Data Acquisition with Bloomberg Terminal\n",
        "\n",
        "Professional-Grade Data Pipelines in the Financial Innovation Lab\n",
        "\n",
        "> **Bloomberg Terminal Lab**\n",
        ">\n",
        "> This lab is designed for use **in the Financial Innovation Lab** with\n",
        "> Bloomberg Terminal access. You’ll work with professional-grade data\n",
        "> and compare it to free API sources.\n",
        ">\n",
        "> **Prerequisites:**\n",
        ">\n",
        "> -   Bloomberg Terminal software running\n",
        "> -   Logged into Bloomberg\n",
        "> -   `bbg-edu` educational package (includes `blpapi` wrapper)\n",
        "> -   Anaconda/Spyder recommended\n",
        ">\n",
        "> **Installation:**\n",
        ">\n",
        "> ``` bash\n",
        "> pip install git+https://github.com/quinfer/bloomberg-edu.git\n",
        "> ```\n",
        "\n",
        "## Before You Start: The Big Picture\n",
        "\n",
        "Welcome to professional-grade financial data. The Bloomberg Terminal is\n",
        "the industry standard—used by traders, analysts, and portfolio managers\n",
        "globally. This lab lets you compare professional tools with free APIs\n",
        "and understand why financial firms pay \\$24,000/year per terminal.\n",
        "\n",
        "> **The Professional Data Advantage**\n",
        ">\n",
        "> **Why Bloomberg costs 100x more than free APIs:**\n",
        ">\n",
        "> 1.  **Data Quality** → Verified, cleaned, adjusted for corporate\n",
        ">     actions in real-time\n",
        "> 2.  **Coverage** → 350+ million securities, 4.5 million issuers,\n",
        ">     historical data back to 1970s\n",
        "> 3.  **Speed** → Real-time data, not 15-minute delayed like free\n",
        ">     sources\n",
        "> 4.  **Integration** → News, analytics, chat, execution all in one\n",
        ">     platform\n",
        "> 5.  **Support** → 24/7 Bloomberg help desk staffed by financial\n",
        ">     experts\n",
        ">\n",
        "> Free APIs work for academic projects. Bloomberg Terminal is what you\n",
        "> use when managing real money.\n",
        "\n",
        "### What You’ll Learn Today\n",
        "\n",
        "By the end of this lab, you will have:\n",
        "\n",
        "-   ✅ Used Bloomberg Desktop API (`blpapi`) to fetch professional-grade\n",
        "    data\n",
        "-   ✅ Compared Bloomberg data quality with yfinance (head-to-head)\n",
        "-   ✅ Built a unified pipeline that handles both sources (with\n",
        "    fallbacks)\n",
        "-   ✅ Understood trade-offs: quality vs. cost, speed vs. access\n",
        "-   ✅ Documented provenance (what data, from where, when, quality\n",
        "    checks)\n",
        "\n",
        "**Time estimate:** 120 minutes (in-lab only, requires Bloomberg Terminal\n",
        "access)\n",
        "\n",
        "> **Why This Matters for Your Career**\n",
        ">\n",
        "> If you want to work in trading, asset management, or investment\n",
        "> banking, you’ll use Bloomberg daily. This lab gives you hands-on\n",
        "> experience you can discuss in interviews: “I built data pipelines\n",
        "> using Bloomberg API” is a concrete skill.\n",
        "\n",
        "## Lab Objectives\n",
        "\n",
        "By the end of this lab, you will:\n",
        "\n",
        "1.  Use Bloomberg Desktop API (`blpapi`) to download professional-grade\n",
        "    data\n",
        "2.  Compare Bloomberg data quality with free APIs (yfinance)\n",
        "3.  Build a pipeline that handles both data sources\n",
        "4.  Understand trade-offs between professional and free data\n",
        "5.  Document data provenance and quality differences\n",
        "\n",
        "## Setup\n",
        "\n",
        "### 1. Verify Bloomberg Terminal Access\n",
        "\n",
        "Open Bloomberg Terminal and confirm you’re logged in. Test basic\n",
        "functions: - Type `AAPL US Equity <GO>` - Type `GP <GO>` to see graphs -\n",
        "Type `FLDS <GO>` to search for field mnemonics\n",
        "\n",
        "### 2. Python Environment Check"
      ],
      "id": "85e40bbf-d785-4b95-9e92-37e68f9e6e5b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import platform\n",
        "\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Platform: {platform.platform()}\")\n",
        "\n",
        "# Check for Bloomberg Educational Package\n",
        "try:\n",
        "    from bbg_edu import BloombergClient\n",
        "    print(\"✓ bbg-edu package installed\")\n",
        "    \n",
        "    # Check if Bloomberg Terminal is available\n",
        "    client = BloombergClient()\n",
        "    if client.is_available():\n",
        "        print(\"✓ Bloomberg Terminal connected\")\n",
        "    else:\n",
        "        print(\"⚠  Bloomberg Terminal not available\")\n",
        "        print(\"   Make sure Terminal is running and logged in\")\n",
        "except ImportError:\n",
        "    print(\"✗ bbg-edu not installed\")\n",
        "    print(\"  Install with: pip install git+https://github.com/quinfer/bloomberg-edu.git\")\n",
        "\n",
        "# Check for other required packages\n",
        "required = ['pandas', 'numpy', 'yfinance', 'matplotlib']\n",
        "for pkg in required:\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "        print(f\"✓ {pkg}\")\n",
        "    except ImportError:\n",
        "        print(f\"✗ {pkg} - install with: pip install {pkg}\")"
      ],
      "id": "f317d4ec"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: Bloomberg Data Pull\n",
        "\n",
        "### Task 1.1: Initialize Bloomberg Client\n",
        "\n",
        "We’ll use the `bbg-edu` educational package, which provides a clean\n",
        "interface to Bloomberg’s Desktop API:"
      ],
      "id": "4a415771-fe63-4f05-a2da-756efbdd6bb9"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bbg_edu import BloombergClient\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Initialize Bloomberg client\n",
        "client = BloombergClient(\n",
        "    validate_data=True,        # Automatic quality checks\n",
        "    rate_limit_sec=2.0         # Conservative rate limiting\n",
        ")\n",
        "\n",
        "# Verify connection\n",
        "if not client.is_available():\n",
        "    raise RuntimeError(\"Bloomberg Terminal not available. Check it's running and logged in.\")\n",
        "\n",
        "print(\"✓ Bloomberg client initialized\")\n",
        "print(\"✓ Ready to fetch data\")"
      ],
      "id": "69a55350"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Pull Historical Data for Core Equities"
      ],
      "id": "6dfb157b-5e52-492f-8b0a-094d196ba8d1"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define our universe (start small for lab)\n",
        "symbols_bbg = [\n",
        "    \"AAPL US Equity\",\n",
        "    \"MSFT US Equity\", \n",
        "    \"GOOGL US Equity\",\n",
        "    \"SPY US Equity\"\n",
        "]\n",
        "\n",
        "fields_bbg = [\n",
        "    \"PX_OPEN\",\n",
        "    \"PX_HIGH\",\n",
        "    \"PX_LOW\",\n",
        "    \"PX_LAST\",\n",
        "    \"PX_VOLUME\"\n",
        "]\n",
        "\n",
        "# Date range (last 2 years)\n",
        "end_date = datetime.today()\n",
        "start_date = end_date - timedelta(days=365*2)\n",
        "\n",
        "print(f\"Downloading {len(symbols_bbg)} symbols from Bloomberg\")\n",
        "print(f\"Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")\n",
        "print(f\"Fields: {', '.join(fields_bbg)}\")\n",
        "\n",
        "# Fetch historical data\n",
        "bbg_data = client.historical(\n",
        "    securities=symbols_bbg,\n",
        "    fields=fields_bbg,\n",
        "    start_date=start_date.strftime('%Y-%m-%d'),\n",
        "    end_date=end_date.strftime('%Y-%m-%d'),\n",
        "    periodicity='DAILY'\n",
        ")\n",
        "\n",
        "print(f\"\\n✓ Downloaded {len(bbg_data)} rows\")\n",
        "print(f\"  Date range: {bbg_data['date'].min()} to {bbg_data['date'].max()}\")\n",
        "print(f\"  Tickers: {bbg_data['security'].nunique()}\")\n",
        "\n",
        "# View fetch metadata (provenance tracking)\n",
        "print(\"\\nFetch Metadata:\")\n",
        "for key, value in client.last_fetch_metadata.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "# Save to CSV for later comparison\n",
        "bbg_data.to_csv('bloomberg_data.csv', index=False)\n",
        "print(\"\\n✓ Saved to bloomberg_data.csv\")"
      ],
      "id": "b194c900"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.3: Explore Bloomberg Data Quality"
      ],
      "id": "f6c4318b-1fac-48fe-8995-45842e9c5e7d"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load Bloomberg data\n",
        "bbg = pd.read_csv('bloomberg_data.csv', parse_dates=['date'])\n",
        "\n",
        "# Check data quality\n",
        "print(\"Bloomberg Data Quality Report:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for ticker in bbg['security'].unique():\n",
        "    ticker_data = bbg[bbg['security'] == ticker]\n",
        "    \n",
        "    # Count missing values\n",
        "    missing = ticker_data[fields_bbg].isna().sum()\n",
        "    \n",
        "    # Check for data anomalies\n",
        "    prices = ticker_data['PX_LAST']\n",
        "    returns = prices.pct_change().dropna()\n",
        "    extreme_returns = (returns.abs() > 0.2).sum()\n",
        "    \n",
        "    # Check High >= Low constraint\n",
        "    if 'PX_HIGH' in ticker_data.columns and 'PX_LOW' in ticker_data.columns:\n",
        "        high_low_violations = (ticker_data['PX_HIGH'] < ticker_data['PX_LOW']).sum()\n",
        "    else:\n",
        "        high_low_violations = 0\n",
        "    \n",
        "    print(f\"\\n{ticker}:\")\n",
        "    print(f\"  Rows: {len(ticker_data)}\")\n",
        "    print(f\"  Missing values: {missing.sum()} ({missing.sum()/len(ticker_data)*100:.1f}%)\")\n",
        "    print(f\"  Extreme returns (>20%): {extreme_returns}\")\n",
        "    print(f\"  High<Low violations: {high_low_violations}\")"
      ],
      "id": "b0d921b3"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Free API Data Pull (Comparison)\n",
        "\n",
        "### Task 2.1: Download Same Data from yfinance"
      ],
      "id": "4bfedfcd-287d-4fb4-8ae8-0a9d8ef9f9bf"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import yfinance as yf\n",
        "import time\n",
        "import random\n",
        "\n",
        "# Convert Bloomberg tickers to Yahoo format\n",
        "symbols_yf = ['AAPL', 'MSFT', 'GOOGL', 'SPY']\n",
        "\n",
        "def fetch_yfinance_with_retry(symbols, period='2y', max_tries=3):\n",
        "    \"\"\"\n",
        "    Fetch historical data from Yahoo Finance with exponential backoff retry logic.\n",
        "    \n",
        "    Implements resilient API calls with automatic retries when rate limits or\n",
        "    temporary failures occur. Uses exponential backoff to avoid overwhelming\n",
        "    the API during high-traffic periods.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    symbols : list of str\n",
        "        Stock tickers in Yahoo Finance format (e.g., ['AAPL', 'MSFT'])\n",
        "    period : str, default='2y'\n",
        "        Historical period to fetch: '1d', '5d', '1mo', '3mo', '6mo', \n",
        "        '1y', '2y', '5y', '10y', 'ytd', 'max'\n",
        "    max_tries : int, default=3\n",
        "        Maximum number of retry attempts before raising error\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Multi-level DataFrame with OHLCV data (Open, High, Low, Close, Volume)\n",
        "        grouped by ticker. Index is datetime, columns are MultiIndex.\n",
        "        \n",
        "    Raises\n",
        "    ------\n",
        "    RuntimeError\n",
        "        If all retry attempts fail\n",
        "        \n",
        "    Notes\n",
        "    -----\n",
        "    - Uses `auto_adjust=True` for split/dividend-adjusted prices\n",
        "    - Implements exponential backoff: waits 1s, 2s, 4s between retries\n",
        "    - Adds random jitter to avoid thundering herd\n",
        "    - Returns only if non-empty DataFrame received\n",
        "    \n",
        "    Examples\n",
        "    --------\n",
        "    >>> data = fetch_yfinance_with_retry(['AAPL', 'MSFT'], period='1y')\n",
        "    >>> data['AAPL']['Close'].iloc[-1]  # Latest AAPL closing price\n",
        "    182.50\n",
        "    >>> data.shape\n",
        "    (252, 10)  # 252 days × (5 fields × 2 symbols)\n",
        "    \n",
        "    See Also\n",
        "    --------\n",
        "    yf.download : Underlying Yahoo Finance API\n",
        "    get_close_from_yf : Alternative implementation from Lab 02 APIs\n",
        "    \"\"\"\n",
        "    for attempt in range(max_tries):\n",
        "        try:\n",
        "            print(f\"Attempt {attempt + 1}/{max_tries}...\")\n",
        "            data = yf.download(\n",
        "                symbols, \n",
        "                period=period,\n",
        "                auto_adjust=True,  # Adjust for splits/dividends\n",
        "                progress=False,    # Suppress progress bar\n",
        "                group_by='ticker'  # Group columns by ticker\n",
        "            )\n",
        "            \n",
        "            if not data.empty:\n",
        "                return data\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"  Error: {e}\")\n",
        "            if attempt < max_tries - 1:\n",
        "                # Exponential backoff with random jitter\n",
        "                sleep_time = 2 ** attempt + random.uniform(0, 1)\n",
        "                print(f\"  Retrying in {sleep_time:.1f}s...\")\n",
        "                time.sleep(sleep_time)\n",
        "    \n",
        "    raise RuntimeError(f\"Failed after {max_tries} attempts\")\n",
        "\n",
        "# Download from yfinance\n",
        "print(\"Downloading from Yahoo Finance...\")\n",
        "yf_data = fetch_yfinance_with_retry(symbols_yf)\n",
        "\n",
        "# Restructure to match Bloomberg format\n",
        "yf_clean = []\n",
        "for ticker in symbols_yf:\n",
        "    ticker_df = yf_data[ticker].copy()\n",
        "    ticker_df['security'] = ticker\n",
        "    ticker_df['date'] = ticker_df.index\n",
        "    ticker_df = ticker_df.rename(columns={\n",
        "        'Open': 'PX_OPEN',\n",
        "        'High': 'PX_HIGH',\n",
        "        'Low': 'PX_LOW',\n",
        "        'Close': 'PX_LAST',\n",
        "        'Volume': 'PX_VOLUME'\n",
        "    })\n",
        "    yf_clean.append(ticker_df[['date', 'security', 'PX_OPEN', 'PX_HIGH', 'PX_LOW', 'PX_LAST', 'PX_VOLUME']])\n",
        "\n",
        "yf_combined = pd.concat(yf_clean, ignore_index=True)\n",
        "yf_combined.to_csv('yfinance_data.csv', index=False)\n",
        "\n",
        "print(f\"\\n✓ Downloaded {len(yf_combined)} rows from Yahoo Finance\")\n",
        "print(f\"✓ Saved to yfinance_data.csv\")"
      ],
      "id": "432b7c37"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Yahoo Finance Data Quality"
      ],
      "id": "82dafa9c-668c-4b22-8357-12af55f968b5"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Yahoo Finance data\n",
        "yf_df = pd.read_csv('yfinance_data.csv', parse_dates=['date'])\n",
        "\n",
        "print(\"Yahoo Finance Data Quality Report:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for ticker in yf_df['security'].unique():\n",
        "    ticker_data = yf_df[yf_df['security'] == ticker]\n",
        "    \n",
        "    # Count missing values\n",
        "    value_cols = ['PX_OPEN', 'PX_HIGH', 'PX_LOW', 'PX_LAST', 'PX_VOLUME']\n",
        "    missing = ticker_data[value_cols].isna().sum()\n",
        "    \n",
        "    # Check for anomalies\n",
        "    prices = ticker_data['PX_LAST']\n",
        "    returns = prices.pct_change().dropna()\n",
        "    extreme_returns = (returns.abs() > 0.2).sum()\n",
        "    \n",
        "    # High >= Low check\n",
        "    high_low_violations = (ticker_data['PX_HIGH'] < ticker_data['PX_LOW']).sum()\n",
        "    \n",
        "    print(f\"\\n{ticker}:\")\n",
        "    print(f\"  Rows: {len(ticker_data)}\")\n",
        "    print(f\"  Missing values: {missing.sum()} ({missing.sum()/len(ticker_data)*100:.1f}%)\")\n",
        "    print(f\"  Extreme returns (>20%): {extreme_returns}\")\n",
        "    print(f\"  High<Low violations: {high_low_violations}\")"
      ],
      "id": "656f406c"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: Comparative Analysis\n",
        "\n",
        "### Task 3.1: Direct Price Comparison"
      ],
      "id": "f7f7458a-1c00-403e-95ba-ab81ec341c2c"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load both datasets\n",
        "bbg = pd.read_csv('bloomberg_data.csv', parse_dates=['date'])\n",
        "yf_df = pd.read_csv('yfinance_data.csv', parse_dates=['date'])\n",
        "\n",
        "# Compare AAPL prices\n",
        "ticker_to_compare = 'AAPL'\n",
        "bbg_ticker = 'AAPL US Equity'\n",
        "\n",
        "bbg_aapl = bbg[bbg['security'] == bbg_ticker].set_index('date')['PX_LAST']\n",
        "yf_aapl = yf_df[yf_df['security'] == ticker_to_compare].set_index('date')['PX_LAST']\n",
        "\n",
        "# Merge on date\n",
        "comparison = pd.DataFrame({\n",
        "    'Bloomberg': bbg_aapl,\n",
        "    'Yahoo_Finance': yf_aapl\n",
        "}).dropna()\n",
        "\n",
        "# Calculate differences\n",
        "comparison['Difference'] = comparison['Bloomberg'] - comparison['Yahoo_Finance']\n",
        "comparison['Pct_Difference'] = (comparison['Difference'] / comparison['Bloomberg']) * 100\n",
        "\n",
        "print(f\"Price Comparison for {ticker_to_compare}:\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Overlapping dates: {len(comparison)}\")\n",
        "print(f\"\\nPrice Differences:\")\n",
        "print(f\"  Mean difference: ${comparison['Difference'].mean():.4f}\")\n",
        "print(f\"  Std dev: ${comparison['Difference'].std():.4f}\")\n",
        "print(f\"  Max difference: ${comparison['Difference'].abs().max():.4f}\")\n",
        "print(f\"  Mean % difference: {comparison['Pct_Difference'].mean():.6f}%\")\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "# Plot prices\n",
        "axes[0].plot(comparison.index, comparison['Bloomberg'], label='Bloomberg', alpha=0.7)\n",
        "axes[0].plot(comparison.index, comparison['Yahoo_Finance'], label='Yahoo Finance', alpha=0.7)\n",
        "axes[0].set_title(f'{ticker_to_compare} Price Comparison: Bloomberg vs. Yahoo Finance')\n",
        "axes[0].set_ylabel('Price ($)')\n",
        "axes[0].legend()\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Plot difference\n",
        "axes[1].plot(comparison.index, comparison['Difference'], color='red', alpha=0.7)\n",
        "axes[1].axhline(0, color='black', linestyle='--', alpha=0.3)\n",
        "axes[1].set_title('Price Difference (Bloomberg - Yahoo Finance)')\n",
        "axes[1].set_ylabel('Difference ($)')\n",
        "axes[1].set_xlabel('Date')\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('bloomberg_vs_yfinance_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n✓ Saved comparison chart to bloomberg_vs_yfinance_comparison.png\")"
      ],
      "id": "230c0fbe"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.2: Coverage and Completeness Analysis"
      ],
      "id": "a5039115-d58f-4d59-a096-748abc74e3e7"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze data coverage differences\n",
        "print(\"Coverage Analysis:\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "for ticker_yf in symbols_yf:\n",
        "    ticker_bbg = f\"{ticker_yf} US Equity\"\n",
        "    \n",
        "    # Bloomberg data\n",
        "    bbg_ticker = bbg[bbg['security'] == ticker_bbg]\n",
        "    bbg_dates = set(bbg_ticker['date'].dt.date)\n",
        "    \n",
        "    # Yahoo Finance data\n",
        "    yf_ticker = yf_df[yf_df['security'] == ticker_yf]\n",
        "    yf_dates = set(yf_ticker['date'].dt.date)\n",
        "    \n",
        "    # Find differences\n",
        "    only_bbg = bbg_dates - yf_dates\n",
        "    only_yf = yf_dates - bbg_dates\n",
        "    overlap = bbg_dates & yf_dates\n",
        "    \n",
        "    print(f\"\\n{ticker_yf}:\")\n",
        "    print(f\"  Bloomberg trading days: {len(bbg_dates)}\")\n",
        "    print(f\"  Yahoo Finance trading days: {len(yf_dates)}\")\n",
        "    print(f\"  Overlapping dates: {len(overlap)}\")\n",
        "    print(f\"  Only in Bloomberg: {len(only_bbg)}\")\n",
        "    print(f\"  Only in Yahoo Finance: {len(only_yf)}\")\n",
        "    \n",
        "    if only_bbg:\n",
        "        print(f\"  Example Bloomberg-only dates: {sorted(list(only_bbg))[:3]}\")\n",
        "    if only_yf:\n",
        "        print(f\"  Example Yahoo-only dates: {sorted(list(only_yf))[:3]}\")"
      ],
      "id": "8dad413f"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Professional Pipeline with Both Sources\n",
        "\n",
        "### Task 4.1: Create Unified Data Pipeline"
      ],
      "id": "dc04919c-6f90-48d2-94a3-055e6e902bed"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import hashlib\n",
        "from datetime import datetime\n",
        "\n",
        "class DataPipeline:\n",
        "    \"\"\"\n",
        "    Professional-grade data pipeline supporting Bloomberg Terminal and free APIs.\n",
        "    \n",
        "    This class demonstrates production patterns: multi-source fetching, data\n",
        "    validation, provenance logging, and integrity checking. Use this pattern\n",
        "    for real trading/research systems where data quality is critical.\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    output_dir : str\n",
        "        Directory for output files (data CSVs, metadata JSON)\n",
        "    metadata : dict\n",
        "        Comprehensive provenance log tracking all pipeline operations:\n",
        "        - pipeline_version : version identifier\n",
        "        - created : ISO timestamp of pipeline creation\n",
        "        - python_version : Python version used\n",
        "        - sources : list of fetch operations with details\n",
        "        - validation results : quality checks per source\n",
        "        - data_hash : SHA-256 hash for data integrity\n",
        "        \n",
        "    Examples\n",
        "    --------\n",
        "    >>> pipeline = DataPipeline(output_dir='my_data')\n",
        "    >>> # Try Bloomberg first\n",
        "    >>> data, source = pipeline.fetch_bloomberg(\n",
        "    ...     symbols=[\"AAPL US Equity\"],\n",
        "    ...     fields=[\"PX_LAST\"],\n",
        "    ...     start_date='20230101',\n",
        "    ...     end_date='20231231'\n",
        "    ... )\n",
        "    >>> if data is not None:\n",
        "    ...     pipeline.validate(data, source)\n",
        "    ...     pipeline.save(data, 'bloomberg_data.csv')\n",
        "    >>> else:\n",
        "    ...     # Fallback to free API\n",
        "    ...     data, source = pipeline.fetch_yfinance(['AAPL'])\n",
        "    ...     pipeline.save(data, 'fallback_data.csv')\n",
        "    \n",
        "    Notes\n",
        "    -----\n",
        "    The metadata JSON file provides complete reproducibility:\n",
        "    - What data was fetched (symbols, fields, dates)\n",
        "    - From where (Bloomberg vs yfinance)\n",
        "    - When (timestamps for each operation)\n",
        "    - Quality (validation results, issues found)\n",
        "    - Integrity (data hash for change detection)\n",
        "    \n",
        "    This level of logging is required for:\n",
        "    - Regulatory compliance (MiFID II, SEC Rule 15c3-5)\n",
        "    - Research reproducibility (academic standards)\n",
        "    - Production debugging (when models fail, audit the data)\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, output_dir='pipeline_output'):\n",
        "        \"\"\"\n",
        "        Initialize data pipeline with output directory and metadata tracking.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        output_dir : str, default='pipeline_output'\n",
        "            Directory path for all pipeline outputs (data CSVs, metadata JSON).\n",
        "            Created automatically if it doesn't exist.\n",
        "        \"\"\"\n",
        "        self.output_dir = output_dir\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        self.metadata = {\n",
        "            'pipeline_version': '1.0',\n",
        "            'created': datetime.now().isoformat(),\n",
        "            'python_version': sys.version.split()[0],\n",
        "            'sources': []\n",
        "        }\n",
        "    \n",
        "    def fetch_bloomberg(self, symbols, fields, start_date, end_date):\n",
        "        \"\"\"\n",
        "        Fetch historical data from Bloomberg Terminal via Desktop API.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        symbols : list of str\n",
        "            Bloomberg security identifiers (e.g., [\"AAPL US Equity\", \"MSFT US Equity\"])\n",
        "        fields : list of str\n",
        "            Bloomberg field mnemonics (e.g., [\"PX_LAST\", \"PX_VOLUME\"])\n",
        "            Use FLDS <GO> on Terminal to search available fields\n",
        "        start_date : str\n",
        "            Start date in YYYYMMDD format (e.g., '20230101')\n",
        "        end_date : str\n",
        "            End date in YYYYMMDD format (e.g., '20231231')\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        tuple of (pd.DataFrame, str) or (None, None)\n",
        "            If successful: (data DataFrame, 'bloomberg')\n",
        "            If failed: (None, None)\n",
        "            \n",
        "        Notes\n",
        "        -----\n",
        "        - Requires Bloomberg Terminal running and logged in\n",
        "        - Requires `blpapi` Python package installed\n",
        "        - Logs fetch metadata for provenance tracking\n",
        "        - Fetches daily periodicity (can be modified for intraday)\n",
        "        \"\"\"\n",
        "        print(\"Fetching from Bloomberg...\")\n",
        "        \n",
        "        try:\n",
        "            with BbgSession() as session:\n",
        "                data = session.historical(\n",
        "                    securities=symbols,\n",
        "                    fields=fields,\n",
        "                    start_date=start_date,\n",
        "                    end_date=end_date,\n",
        "                    periodicity='DAILY'\n",
        "                )\n",
        "            \n",
        "            self.metadata['sources'].append({\n",
        "                'provider': 'Bloomberg Desktop API',\n",
        "                'symbols': symbols,\n",
        "                'fields': fields,\n",
        "                'date_range': [start_date, end_date],\n",
        "                'rows': len(data),\n",
        "                'fetch_time': datetime.now().isoformat()\n",
        "            })\n",
        "            \n",
        "            return data, 'bloomberg'\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Bloomberg fetch failed: {e}\")\n",
        "            return None, None\n",
        "    \n",
        "    def fetch_yfinance(self, symbols, period='2y'):\n",
        "        \"\"\"\n",
        "        Fetch historical data from Yahoo Finance (fallback when Bloomberg unavailable).\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        symbols : list of str\n",
        "            Yahoo Finance tickers (e.g., ['AAPL', 'MSFT'])\n",
        "        period : str, default='2y'\n",
        "            Historical period: '1d', '5d', '1mo', '3mo', '6mo', '1y', '2y', '5y', '10y'\n",
        "            \n",
        "        Returns\n",
        "        -------\n",
        "        tuple of (pd.DataFrame, str) or (None, None)\n",
        "            If successful: (data DataFrame with standardized columns, 'yfinance')\n",
        "            If failed: (None, None)\n",
        "            \n",
        "        Notes\n",
        "        -----\n",
        "        - Converts yfinance OHLCV format to match Bloomberg structure\n",
        "        - Logs fetch metadata for provenance tracking\n",
        "        - Uses retry logic from fetch_yfinance_with_retry()\n",
        "        \"\"\"\n",
        "        print(\"Fetching from Yahoo Finance...\")\n",
        "        \n",
        "        try:\n",
        "            data = fetch_yfinance_with_retry(symbols, period)\n",
        "            \n",
        "            # Convert to standard format\n",
        "            clean_data = []\n",
        "            for sym in symbols:\n",
        "                ticker_df = data[sym].copy()\n",
        "                ticker_df['security'] = sym\n",
        "                ticker_df['date'] = ticker_df.index\n",
        "                clean_data.append(ticker_df)\n",
        "            \n",
        "            combined = pd.concat(clean_data, ignore_index=True)\n",
        "            \n",
        "            self.metadata['sources'].append({\n",
        "                'provider': 'Yahoo Finance',\n",
        "                'symbols': symbols,\n",
        "                'period': period,\n",
        "                'rows': len(combined),\n",
        "                'fetch_time': datetime.now().isoformat()\n",
        "            })\n",
        "            \n",
        "            return combined, 'yfinance'\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Yahoo Finance fetch failed: {e}\")\n",
        "            return None, None\n",
        "    \n",
        "    def validate(self, data, source):\n",
        "        \"\"\"Validate data quality\"\"\"\n",
        "        print(f\"Validating {source} data...\")\n",
        "        \n",
        "        issues = {\n",
        "            'missing_values': int(data.isna().sum().sum()),\n",
        "            'negative_prices': 0,\n",
        "            'high_low_violations': 0,\n",
        "            'extreme_returns': 0\n",
        "        }\n",
        "        \n",
        "        # Check for negative prices\n",
        "        price_cols = [c for c in data.columns if 'PX_' in c or 'Close' in c]\n",
        "        for col in price_cols:\n",
        "            if col in data.columns:\n",
        "                issues['negative_prices'] += int((data[col] <= 0).sum())\n",
        "        \n",
        "        # Check High >= Low\n",
        "        if 'PX_HIGH' in data.columns and 'PX_LOW' in data.columns:\n",
        "            issues['high_low_violations'] = int((data['PX_HIGH'] < data['PX_LOW']).sum())\n",
        "        \n",
        "        self.metadata[f'{source}_validation'] = issues\n",
        "        \n",
        "        print(f\"  Missing values: {issues['missing_values']}\")\n",
        "        print(f\"  Negative prices: {issues['negative_prices']}\")\n",
        "        print(f\"  High<Low violations: {issues['high_low_violations']}\")\n",
        "        \n",
        "        return issues\n",
        "    \n",
        "    def save(self, data, filename):\n",
        "        \"\"\"Save data with metadata\"\"\"\n",
        "        filepath = os.path.join(self.output_dir, filename)\n",
        "        data.to_csv(filepath, index=False)\n",
        "        \n",
        "        # Generate data hash for integrity\n",
        "        data_hash = hashlib.sha256(data.to_csv(index=False).encode()).hexdigest()[:12]\n",
        "        self.metadata['data_hash'] = data_hash\n",
        "        \n",
        "        # Save metadata\n",
        "        meta_path = os.path.join(self.output_dir, 'metadata.json')\n",
        "        with open(meta_path, 'w') as f:\n",
        "            json.dump(self.metadata, f, indent=2)\n",
        "        \n",
        "        print(f\"✓ Saved data to {filepath}\")\n",
        "        print(f\"✓ Saved metadata to {meta_path}\")\n",
        "        print(f\"  Data hash: {data_hash}\")\n",
        "\n",
        "# Use the pipeline\n",
        "pipeline = DataPipeline()\n",
        "\n",
        "# Try Bloomberg first\n",
        "bbg_symbols = [\"AAPL US Equity\", \"MSFT US Equity\"]\n",
        "bbg_data, source = pipeline.fetch_bloomberg(\n",
        "    symbols=bbg_symbols,\n",
        "    fields=[\"PX_LAST\", \"PX_VOLUME\"],\n",
        "    start_date=(datetime.now() - timedelta(days=365)).strftime('%Y%m%d'),\n",
        "    end_date=datetime.now().strftime('%Y%m%d')\n",
        ")\n",
        "\n",
        "if bbg_data is not None:\n",
        "    issues = pipeline.validate(bbg_data, source)\n",
        "    pipeline.save(bbg_data, 'professional_data.csv')\n",
        "else:\n",
        "    print(\"Bloomberg failed; would use yfinance fallback in production\")"
      ],
      "id": "0b0fe8ea"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 5: Lab Report\n",
        "\n",
        "### Deliverables\n",
        "\n",
        "Create a short report (1-2 pages) covering:\n",
        "\n",
        "**1. Data Quality Comparison (30 points)** - Compare missing data rates\n",
        "between Bloomberg and Yahoo Finance - Identify any price discrepancies\n",
        "and explain why they occur - Assess data coverage differences\n",
        "\n",
        "**2. Professional vs. Free API Trade-offs (25 points)** - What are the\n",
        "advantages of Bloomberg data? - When would free APIs be sufficient? -\n",
        "How do costs (time, money) factor into the decision?\n",
        "\n",
        "**3. Pipeline Design (25 points)** - Explain your data validation\n",
        "strategy - How did you handle errors and retries? - What metadata did\n",
        "you log and why?\n",
        "\n",
        "**4. Bloomberg Terminal Experience (20 points)** - What Bloomberg\n",
        "functions did you explore? - How would you integrate Bloomberg into a\n",
        "professional workflow? - What surprised you about Bloomberg vs. free\n",
        "APIs?\n",
        "\n",
        "### Report Template\n",
        "\n",
        "``` markdown\n",
        "# Lab 2 Report: Bloomberg Terminal Data Pipeline\n",
        "\n",
        "**Name:** [Your Name]\n",
        "**Date:** [Date]\n",
        "**Lab Session:** [Session Time]\n",
        "\n",
        "## 1. Data Quality Comparison\n",
        "\n",
        "### Bloomberg Data\n",
        "- Symbols analyzed: [list]\n",
        "- Date range: [range]\n",
        "- Missing data rate: [X%]\n",
        "- Issues found: [describe]\n",
        "\n",
        "### Yahoo Finance Data  \n",
        "- Symbols analyzed: [list]\n",
        "- Date range: [range]\n",
        "- Missing data rate: [X%]\n",
        "- Issues found: [describe]\n",
        "\n",
        "### Price Discrepancies\n",
        "[Describe any differences you found and explain why they occur]\n",
        "\n",
        "## 2. Professional vs. Free API Trade-offs\n",
        "\n",
        "### Bloomberg Advantages\n",
        "- [List 3-5 advantages you observed]\n",
        "\n",
        "### When Free APIs Are Sufficient\n",
        "- [Discuss appropriate use cases]\n",
        "\n",
        "### Cost-Benefit Analysis\n",
        "- [Discuss time, money, and quality trade-offs]\n",
        "\n",
        "## 3. Pipeline Design\n",
        "\n",
        "### Validation Strategy\n",
        "- [Explain your validation checks]\n",
        "\n",
        "### Error Handling\n",
        "- [Describe your retry logic and fallback strategy]\n",
        "\n",
        "### Metadata Logging\n",
        "- [List what you logged and why it matters]\n",
        "\n",
        "## 4. Bloomberg Terminal Experience\n",
        "\n",
        "### Functions Explored\n",
        "- [List Bloomberg functions you tried: GP, FLDS, etc.]\n",
        "\n",
        "### Professional Workflow Integration\n",
        "- [How would you use Bloomberg in a real trading/research role?]\n",
        "\n",
        "### Insights and Surprises\n",
        "- [What did you learn? What surprised you?]\n",
        "\n",
        "## Appendix: Code and Output\n",
        "[Attach key code snippets and output screenshots]\n",
        "```\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "### Bloomberg Terminal Functions to Explore\n",
        "\n",
        "-   **FLDS <GO>**: Search for field mnemonics\n",
        "-   **GP <GO>**: Graph prices with technical indicators\n",
        "-   **FA <GO>**: Financial analysis (ratios, comps)\n",
        "-   **ANR <GO>**: Analyst recommendations\n",
        "-   **N <GO>**: News (real-time, searchable)\n",
        "-   **CACS <GO>**: Corporate actions (splits, dividends)\n",
        "-   **DES <GO>**: Security description\n",
        "-   **HDS <GO>**: Historical data (alternative to API)\n",
        "\n",
        "### Further Reading\n",
        "\n",
        "-   Bloomberg API Documentation: Available on Bloomberg Terminal (DAPI\n",
        "    <GO>)\n",
        "-   Hilpisch (2019), Chapter 3: Financial Data (Bloomberg integration)\n",
        "-   Ulster Bloomberg Training: Contact Financial Innovation Lab staff\n",
        "\n",
        "### Troubleshooting\n",
        "\n",
        "**Bloomberg API errors:** - Ensure Terminal is running and you’re logged\n",
        "in - Check `blpapi` is installed: `pip list | grep blpapi` - Verify\n",
        "security names: Use `FLDS <GO>` to check exact mnemonics\n",
        "\n",
        "**Data discrepancies:** - Check adjustment settings (splits/dividends) -\n",
        "Verify date ranges are comparable - Consider different market\n",
        "hours/timezones\n",
        "\n",
        "**Performance issues:** - Reduce batch sizes (max 6 tickers per request\n",
        "recommended) - Increase sleep times between requests - Use time-limited\n",
        "queries (max 2 years for lab)\n",
        "\n",
        "> **Lab Hours and Support**\n",
        ">\n",
        "> -   Financial Innovation Lab hours: \\[TBD\\]\n",
        "> -   Bloomberg Terminal bookings: Contact lab staff\n",
        "> -   Technical support: Office hours or lab assistants"
      ],
      "id": "eb388f08-07c3-4389-922b-dd8fb8e6fd8d"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "fin510",
      "display_name": "FIN510 Python",
      "language": "python",
      "path": "/Users/quinference/Library/Jupyter/kernels/fin510"
    }
  }
}