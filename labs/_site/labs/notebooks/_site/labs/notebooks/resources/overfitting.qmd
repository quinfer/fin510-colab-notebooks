---
title: "Backtest Overfitting: CSCV, PBO, PSR/DSR"
bibliography: reading.bib
---

## Overview

- Problem: With many candidate strategies, the best in‑sample often overstates true performance (selection bias, data snooping).
- Guardrails:
  - Time‑aware model selection (CSCV) → quantify Probability of Backtest Overfitting (PBO) [@bailey2015pbo]
  - Performance significance with non‑normality: PSR; selection‑bias deflation: DSR [@lopezdeprado2014dsr]

## Key Definitions

- CSCV: Combinatorially Symmetric Cross‑Validation (contiguous folds; swap in/out halves; select in‑sample champion; score out‑of‑sample).
- PBO: Fraction of CSCV splits where the in‑sample champion ranks poorly OOS (logit rank < 0).
- PSR: Probabilistic Sharpe Ratio — probability that true SR exceeds a benchmark under finite samples, skew, kurtosis.
- DSR: Deflated Sharpe Ratio — raises the benchmark SR to account for multiple, correlated trials.

## Minimal Equations

Let \(\hat S\) be observed Sharpe, \(S_\star\) the benchmark Sharpe. PSR (Lopez de Prado/Bailey):

\[
\mathrm{PSR} = \Phi\!\left(\frac{(\hat S - S_\star)\,\sqrt{n-1}}{\sqrt{1 - \gamma_3\hat S + ((\gamma_4-1)/4)\,\hat S^2}}\right)\,,
\]

where \(\gamma_3\) is skewness and \(\gamma_4\) kurtosis (normal=3), \(n\) sample size.

DSR increases \(S_\star\) to reflect the number of trials and their dependence (see @lopezdeprado2014dsr).

## Reporting Template (copy/paste)

- Data: sample period, asset universe, costs/slippage assumptions, data vintage/release timing.
- Trials: number of candidate strategies or hyper‑parameter settings explored; correlation comment (e.g., similar families).
- Selection: selection rule (e.g., in‑sample Sharpe), time‑aware protocol (CSCV/walk‑forward).
- Robustness: CSCV PBO = X.XX (N splits); logit rank distribution shown.
- Significance: PSR = X.XX vs benchmark SR*; DSR = X.XX (assumptions noted).
- Decision: promote/park; plan for live validation.

## Lightweight Code (repo utilities)

- `scripts/overfit_metrics.py`:
  - `cscv_pbo(returns, n_folds=10)` → PBO and diagnostics
  - `probabilistic_sharpe_ratio(sr_hat, sr_star, n_obs, skew, kurtosis)`
  - `generate_noise_strategies(T, N, rho)` for demos

See Lab 6B for full examples and figures. If `mlfinlab` is available, compare DSR implementations.

## References

- @bailey2015pbo — Probability of Backtest Overfitting (PBO) and CSCV  
- @lopezdeprado2014dsr — Deflated Sharpe Ratio (DSR)  
- @white2000realitycheck — Reality Check  
- @hansen2005spa — SPA test  

## Visual Workflows

![](../images/overfitting/dsr_workflow.png){width="900"}

![](../images/overfitting/cpcv.png){width="900"}

![](../images/overfitting/walkforward_vs_cpcv.png){width="900"}

![](../images/overfitting/robust_workflow.png){width="900"}
