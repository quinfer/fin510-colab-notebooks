{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab 11: Market Surveillance & Detection Algorithms\n",
        "\n",
        "## Before You Code: The Big Picture\n",
        "\n",
        "**Every trade you make is watched.** Exchanges run real-time\n",
        "surveillance systems scanning millions of orders per day for\n",
        "manipulation. Can you build algorithms that catch bad actors without\n",
        "drowning investigators in false positives?\n",
        "\n",
        "> **The Market Surveillance Challenge**\n",
        ">\n",
        "> **Why Surveillance Exists:** - **Spoofing**: Place large orders to\n",
        "> move prices, cancel before execution - **Wash trading**: Trade with\n",
        "> yourself to inflate volume, manipulate prices - **Layering**: Stack\n",
        "> multiple fake orders to create false supply/demand -\n",
        "> **Front-running**: Use non-public information to trade ahead of\n",
        "> clients\n",
        ">\n",
        "> **The Regulatory Mandate:** - **MAR (Europe)**: Market Abuse\n",
        "> Regulation requires surveillance of all trading venues - **SEC/FINRA\n",
        "> (US)**: Exchanges must detect and report suspicious activity - **MiFID\n",
        "> II**: Transaction reporting within 24 hours, audit trails for 5 years\n",
        ">\n",
        "> **The Problem:** - **Scale**: NYSE processes ~10B quotes per day,\n",
        "> NASDAQ ~20B - **False positives**: 95-99% of alerts are legitimate\n",
        "> activity (Nasdaq 2019) - **Cost**: \\$10-30K to investigate each alert\n",
        "> (compliance team time) - **Adversarial**: Manipulators adapt to\n",
        "> detection systems\n",
        ">\n",
        "> **The Evidence:** - \\$3.4B in fines for market manipulation\n",
        "> (2020-2022, FCA/SEC/CFTC) - JP Morgan: \\$920M fine for spoofing\n",
        "> precious metals (2020) - Navinder Sarao: \\$41M penalty for Flash Crash\n",
        "> spoofing (2010) - Most violations go undetected (estimates: \\<10%\n",
        "> detection rate)\n",
        "\n",
        "### What You’ll Build Today\n",
        "\n",
        "By the end of this lab, you will have:\n",
        "\n",
        "-   ✅ Spoofing detector analyzing order book patterns\n",
        "-   ✅ Wash trading detector using network analysis\n",
        "-   ✅ ML alert triage system prioritizing investigations\n",
        "-   ✅ Understanding of precision-recall tradeoffs in surveillance\n",
        "\n",
        "**Time estimate:** FIN510: 75 min (Ex 1-2) \\| FIN720: 100 min (all\n",
        "exercises)\n",
        "\n",
        "> **Why This Matters**\n",
        ">\n",
        "> Financial surveillance is a \\$50B+ industry (compliance, RegTech,\n",
        "> monitoring systems). Every bank, exchange, and broker needs\n",
        "> surveillance analysts and systems engineers. If you can build\n",
        "> detection algorithms and manage false positive rates, you’re\n",
        "> employable at FINRA, exchanges (NYSE, NASDAQ), or compliance teams at\n",
        "> any bank.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "This lab applies market surveillance concepts to realistic scenarios\n",
        "detecting manipulation patterns. You’ll implement detection algorithms\n",
        "used in production surveillance systems, experience false positive\n",
        "challenges, and evaluate trade-offs between sensitivity and specificity.\n",
        "The exercises connect technical detection to regulatory requirements and\n",
        "operational constraints shaping real-world surveillance deployment.\n",
        "\n",
        "Market surveillance protects market integrity by detecting and deterring\n",
        "manipulation—spoofing, layering, wash trading, front-running, and other\n",
        "prohibited behaviors. Regulatory frameworks (MAR in Europe, SEC/FINRA in\n",
        "US) mandate surveillance systems generating alerts for suspicious\n",
        "activity. However, perfect detection is impossible—surveillance must\n",
        "balance catching violations against managing false positives that\n",
        "overwhelm investigators. This lab makes these trade-offs concrete\n",
        "through hands-on implementation.\n",
        "\n",
        "We’ll work with three scenarios increasingly sophisticated. First,\n",
        "spoofing detection using order book data—implementing rules-based\n",
        "detector and calibrating thresholds balancing false positives against\n",
        "false negatives. Second, wash trading detection using transaction data\n",
        "and graph analytics—identifying circular trading networks and\n",
        "quantifying volume inflation. Third, alert triage using machine\n",
        "learning—prioritizing investigator attention by predicting which alerts\n",
        "represent genuine violations versus false positives.\n",
        "\n",
        "**Prerequisites**: Understanding of Week 11 material (surveillance\n",
        "frameworks, manipulation patterns, detection approaches), Python\n",
        "programming, pandas data manipulation, and basic machine learning\n",
        "concepts from previous labs.\n",
        "\n",
        "**Learning Objectives**: By completing this lab, you will be able to\n",
        "implement pattern detection algorithms for market manipulation, evaluate\n",
        "detection performance using appropriate metrics, apply graph analytics\n",
        "to transaction surveillance, use machine learning for alert\n",
        "prioritization, and assess surveillance system effectiveness considering\n",
        "operational constraints.\n",
        "\n",
        "> **FIN510 vs FIN720**\n",
        ">\n",
        "> -   **FIN510 students**: Complete Exercises 1-2 (pattern detection)\n",
        "> -   **FIN720 students**: Complete all three exercises including ML\n",
        ">     triage\n",
        "\n",
        "## Exercise 1: Detecting Spoofing Patterns\n",
        "\n",
        "### Context\n",
        "\n",
        "Spoofing manipulates order books by placing orders without execution\n",
        "intent—creating false supply/demand signals inducing price movements\n",
        "that benefit the spoofer. Regulators prohibit spoofing under MAR\n",
        "(Europe) and Dodd-Frank (US); violations carry severe penalties\n",
        "including imprisonment. Surveillance systems must detect spoofing\n",
        "patterns whilst avoiding false positives from legitimate market making\n",
        "strategies that also involve high cancellation rates.\n",
        "\n",
        "This exercise provides simulated order book data from a single security\n",
        "trading session. The data contains both legitimate market making\n",
        "(providing liquidity, canceling based on market conditions) and injected\n",
        "spoofing (large orders placed opposite desired direction, cancelled\n",
        "quickly after triggering desired price movement). Your task is\n",
        "implementing a spoofing detector and calibrating thresholds.\n",
        "\n",
        "### Generating Simulated Order Data"
      ],
      "id": "0b1541b4-a215-48bb-bfaa-2a3c85dd291b"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Simulation parameters\n",
        "n_participants = 10\n",
        "n_orders = 5000\n",
        "session_start = datetime(2024, 10, 1, 9, 30)\n",
        "\n",
        "# Generate orders\n",
        "orders = []\n",
        "for i in range(n_orders):\n",
        "    # Participant (two are spoofers)\n",
        "    participant_id = np.random.choice(range(n_participants), \n",
        "                                     p=[0.15, 0.15, 0.10, 0.10, 0.08, \n",
        "                                        0.08, 0.08, 0.08, 0.09, 0.09])\n",
        "    \n",
        "    # Spoofers are participants 0 and 1\n",
        "    is_spoofer = participant_id in [0, 1]\n",
        "    \n",
        "    # Time\n",
        "    timestamp = session_start + timedelta(\n",
        "        microseconds=np.random.uniform(0, 6.5*3600*1e6)\n",
        "    )\n",
        "    \n",
        "    # Side\n",
        "    if is_spoofer:\n",
        "        # Spoofers alternate: place spoof on one side, trade on other\n",
        "        side = 'buy' if np.random.random() < 0.3 else 'sell'\n",
        "        if side == 'sell':  # Spoof sell orders (want to buy)\n",
        "            is_spoof = np.random.random() < 0.8\n",
        "        else:  # Real buy orders\n",
        "            is_spoof = False\n",
        "    else:\n",
        "        side = 'buy' if np.random.random() < 0.5 else 'sell'\n",
        "        is_spoof = False\n",
        "    \n",
        "    # Price (around 100 with noise)\n",
        "    mid_price = 100 + np.sin(i/200) * 0.5 + np.random.normal(0, 0.1)\n",
        "    \n",
        "    if side == 'buy':\n",
        "        price = mid_price - np.random.uniform(0.01, 0.10)\n",
        "    else:\n",
        "        price = mid_price + np.random.uniform(0.01, 0.10)\n",
        "    \n",
        "    # Quantity\n",
        "    if is_spoof:\n",
        "        quantity = np.random.randint(5000, 20000)  # Large spoof orders\n",
        "    else:\n",
        "        quantity = np.random.randint(100, 2000)\n",
        "    \n",
        "    # Outcome: spoof orders cancelled quickly, others fill or cancel based on market\n",
        "    if is_spoof:\n",
        "        time_to_cancel = np.random.uniform(0.1, 2.0)  # 100ms - 2s\n",
        "        outcome = 'cancelled'\n",
        "        fill_price = None\n",
        "    elif is_spoofer and not is_spoof:\n",
        "        # Spoofer's real orders fill (benefiting from manipulation)\n",
        "        time_to_cancel = None\n",
        "        outcome = 'filled'\n",
        "        fill_price = price\n",
        "    else:\n",
        "        # Legitimate orders: 40% fill, 60% cancel after longer time\n",
        "        if np.random.random() < 0.4:\n",
        "            outcome = 'filled'\n",
        "            fill_price = price\n",
        "            time_to_cancel = None\n",
        "        else:\n",
        "            outcome = 'cancelled'\n",
        "            fill_price = None\n",
        "            time_to_cancel = np.random.uniform(10, 300)  # 10s - 5min\n",
        "    \n",
        "    orders.append({\n",
        "        'order_id': f'ORD_{i:06d}',\n",
        "        'participant_id': f'P{participant_id:02d}',\n",
        "        'timestamp': timestamp,\n",
        "        'side': side,\n",
        "        'price': round(price, 2),\n",
        "        'quantity': quantity,\n",
        "        'outcome': outcome,\n",
        "        'fill_price': fill_price,\n",
        "        'time_to_cancel': time_to_cancel,\n",
        "        'is_spoof_actual': is_spoof  # Ground truth (not observable in practice)\n",
        "    })\n",
        "\n",
        "orders_df = pd.DataFrame(orders)\n",
        "orders_df = orders_df.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "print(f\"Generated {len(orders_df)} orders from {n_participants} participants\")\n",
        "print(f\"\\nSpoof orders (ground truth): {orders_df['is_spoof_actual'].sum()}\")\n",
        "print(f\"\\nFirst few orders:\")\n",
        "print(orders_df[['order_id', 'participant_id', 'timestamp', \n",
        "                 'side', 'price', 'quantity', 'outcome']].head(10))"
      ],
      "id": "22fb33d9"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.1: Participant-Level Statistics\n",
        "\n",
        "Calculate participant-level statistics indicative of spoofing behavior:\n",
        "\n",
        "-   **Cancel rate**: Fraction of orders cancelled vs filled\n",
        "-   **Average time to cancel**: How quickly cancelled orders are\n",
        "    cancelled\n",
        "-   **Opposite-side fills**: Whether participant has fills opposite\n",
        "    their cancelled orders\n",
        "-   **Order size distribution**: Cancelled orders vs filled orders"
      ],
      "id": "5707de26-93e7-43b7-b135-f344efa4e690"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate participant statistics\n",
        "def calculate_participant_stats(orders_df):\n",
        "    \"\"\"\n",
        "    Calculate participant-level statistics for spoofing detection.\n",
        "    \"\"\"\n",
        "    stats_list = []\n",
        "    \n",
        "    for participant in orders_df['participant_id'].unique():\n",
        "        p_orders = orders_df[orders_df['participant_id'] == participant]\n",
        "        \n",
        "        # Total orders\n",
        "        n_orders = len(p_orders)\n",
        "        \n",
        "        # Cancel rate\n",
        "        n_cancelled = (p_orders['outcome'] == 'cancelled').sum()\n",
        "        n_filled = (p_orders['outcome'] == 'filled').sum()\n",
        "        cancel_rate = n_cancelled / n_orders if n_orders > 0 else 0\n",
        "        \n",
        "        # Average time to cancel (for cancelled orders)\n",
        "        cancelled_orders = p_orders[p_orders['outcome'] == 'cancelled']\n",
        "        avg_time_to_cancel = cancelled_orders['time_to_cancel'].mean() if len(cancelled_orders) > 0 else np.nan\n",
        "        \n",
        "        # Median time to cancel (more robust to outliers)\n",
        "        median_time_to_cancel = cancelled_orders['time_to_cancel'].median() if len(cancelled_orders) > 0 else np.nan\n",
        "        \n",
        "        # Average cancelled order size vs filled order size\n",
        "        cancelled_size = cancelled_orders['quantity'].mean() if len(cancelled_orders) > 0 else 0\n",
        "        filled_orders = p_orders[p_orders['outcome'] == 'filled']\n",
        "        filled_size = filled_orders['quantity'].mean() if len(filled_orders) > 0 else 0\n",
        "        size_ratio = cancelled_size / filled_size if filled_size > 0 else np.inf\n",
        "        \n",
        "        # Check for opposite-side activity\n",
        "        has_buy_fills = ((p_orders['side'] == 'buy') & (p_orders['outcome'] == 'filled')).any()\n",
        "        has_sell_fills = ((p_orders['side'] == 'sell') & (p_orders['outcome'] == 'filled')).any()\n",
        "        has_buy_cancels = ((p_orders['side'] == 'buy') & (p_orders['outcome'] == 'cancelled')).any()\n",
        "        has_sell_cancels = ((p_orders['side'] == 'sell') & (p_orders['outcome'] == 'cancelled')).any()\n",
        "        \n",
        "        # Spoof pattern: cancelled on one side, filled on other\n",
        "        potential_spoof_pattern = (has_buy_fills and has_sell_cancels) or (has_sell_fills and has_buy_cancels)\n",
        "        \n",
        "        # Profitability proxy: did fills benefit from cancelled orders?\n",
        "        # Simplified: look at timing - fills after cancels on opposite side\n",
        "        profitability_score = 0\n",
        "        if potential_spoof_pattern:\n",
        "            for _, fill_order in filled_orders.iterrows():\n",
        "                # Count recent cancels on opposite side\n",
        "                opposite_side = 'sell' if fill_order['side'] == 'buy' else 'buy'\n",
        "                recent_cancels = cancelled_orders[\n",
        "                    (cancelled_orders['side'] == opposite_side) &\n",
        "                    (cancelled_orders['timestamp'] < fill_order['timestamp']) &\n",
        "                    (cancelled_orders['timestamp'] > fill_order['timestamp'] - timedelta(seconds=10))\n",
        "                ]\n",
        "                profitability_score += len(recent_cancels)\n",
        "        \n",
        "        stats_list.append({\n",
        "            'participant_id': participant,\n",
        "            'n_orders': n_orders,\n",
        "            'cancel_rate': cancel_rate,\n",
        "            'avg_time_to_cancel_sec': avg_time_to_cancel,\n",
        "            'median_time_to_cancel_sec': median_time_to_cancel,\n",
        "            'cancelled_size': cancelled_size,\n",
        "            'filled_size': filled_size,\n",
        "            'size_ratio': size_ratio,\n",
        "            'potential_spoof_pattern': potential_spoof_pattern,\n",
        "            'profitability_score': profitability_score\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(stats_list)\n",
        "\n",
        "# Calculate statistics\n",
        "participant_stats = calculate_participant_stats(orders_df)\n",
        "\n",
        "# Sort by cancel rate (high to low)\n",
        "participant_stats = participant_stats.sort_values('cancel_rate', ascending=False)\n",
        "\n",
        "print(\"Participant Statistics:\")\n",
        "print(participant_stats.to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Cancel rate\n",
        "axes[0, 0].barh(participant_stats['participant_id'], \n",
        "                participant_stats['cancel_rate'])\n",
        "axes[0, 0].set_xlabel('Cancel Rate')\n",
        "axes[0, 0].set_title('Cancel Rate by Participant')\n",
        "axes[0, 0].axvline(0.95, color='red', linestyle='--', label='High threshold (0.95)')\n",
        "axes[0, 0].legend()\n",
        "\n",
        "# Time to cancel\n",
        "axes[0, 1].barh(participant_stats['participant_id'], \n",
        "                participant_stats['median_time_to_cancel_sec'])\n",
        "axes[0, 1].set_xlabel('Median Time to Cancel (seconds)')\n",
        "axes[0, 1].set_title('Cancellation Speed by Participant')\n",
        "axes[0, 1].axvline(5, color='red', linestyle='--', label='Fast threshold (5s)')\n",
        "axes[0, 1].legend()\n",
        "\n",
        "# Size ratio\n",
        "axes[1, 0].barh(participant_stats['participant_id'], \n",
        "                participant_stats['size_ratio'])\n",
        "axes[1, 0].set_xlabel('Cancelled Size / Filled Size')\n",
        "axes[1, 0].set_title('Order Size Ratio by Participant')\n",
        "axes[1, 0].axvline(2, color='red', linestyle='--', label='Suspicious threshold (2x)')\n",
        "axes[1, 0].set_xlim(0, 10)\n",
        "axes[1, 0].legend()\n",
        "\n",
        "# Profitability score\n",
        "axes[1, 1].barh(participant_stats['participant_id'], \n",
        "                participant_stats['profitability_score'])\n",
        "axes[1, 1].set_xlabel('Profitability Score')\n",
        "axes[1, 1].set_title('Opposite-Side Profitability by Participant')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('spoofing_participant_stats.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Visualization saved as 'spoofing_participant_stats.png'\")"
      ],
      "id": "3d25b826"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.2: Implement Spoofing Detection Rule\n",
        "\n",
        "Implement a rule-based spoofing detector using thresholds on calculated\n",
        "statistics:"
      ],
      "id": "48857f46-bac5-4aa3-b7d5-f32cba1af2da"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_spoofing(participant_stats, \n",
        "                   cancel_rate_threshold=0.90,\n",
        "                   time_to_cancel_threshold=5.0,\n",
        "                   size_ratio_threshold=2.0,\n",
        "                   min_profitability_score=1):\n",
        "    \"\"\"\n",
        "    Detect potential spoofing based on participant statistics.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    participant_stats : DataFrame\n",
        "        Participant-level statistics\n",
        "    cancel_rate_threshold : float\n",
        "        Minimum cancel rate to trigger alert\n",
        "    time_to_cancel_threshold : float\n",
        "        Maximum median time to cancel (seconds) to trigger alert\n",
        "    size_ratio_threshold : float\n",
        "        Minimum cancelled/filled size ratio to trigger alert\n",
        "    min_profitability_score : int\n",
        "        Minimum profitability score to trigger alert\n",
        "        \n",
        "    Returns:\n",
        "    --------\n",
        "    DataFrame with alerts and scores\n",
        "    \"\"\"\n",
        "    alerts = []\n",
        "    \n",
        "    for _, row in participant_stats.iterrows():\n",
        "        # Rule checks\n",
        "        high_cancel_rate = row['cancel_rate'] >= cancel_rate_threshold\n",
        "        fast_cancellation = row['median_time_to_cancel_sec'] <= time_to_cancel_threshold\n",
        "        large_cancelled_orders = row['size_ratio'] >= size_ratio_threshold\n",
        "        profitable_pattern = row['profitability_score'] >= min_profitability_score\n",
        "        has_spoof_pattern = row['potential_spoof_pattern']\n",
        "        \n",
        "        # Scoring: sum of conditions met\n",
        "        conditions_met = sum([high_cancel_rate, fast_cancellation, \n",
        "                             large_cancelled_orders, profitable_pattern, \n",
        "                             has_spoof_pattern])\n",
        "        \n",
        "        # Generate alert if multiple conditions met\n",
        "        if conditions_met >= 3:\n",
        "            alert_priority = 'HIGH' if conditions_met >= 4 else 'MEDIUM'\n",
        "            \n",
        "            alerts.append({\n",
        "                'participant_id': row['participant_id'],\n",
        "                'alert_priority': alert_priority,\n",
        "                'conditions_met': conditions_met,\n",
        "                'high_cancel_rate': high_cancel_rate,\n",
        "                'fast_cancellation': fast_cancellation,\n",
        "                'large_cancelled_orders': large_cancelled_orders,\n",
        "                'profitable_pattern': profitable_pattern,\n",
        "                'has_spoof_pattern': has_spoof_pattern,\n",
        "                'cancel_rate': row['cancel_rate'],\n",
        "                'median_time_to_cancel': row['median_time_to_cancel_sec'],\n",
        "                'size_ratio': row['size_ratio'],\n",
        "                'profitability_score': row['profitability_score']\n",
        "            })\n",
        "    \n",
        "    return pd.DataFrame(alerts)\n",
        "\n",
        "# Detect spoofing with default thresholds\n",
        "alerts = detect_spoofing(participant_stats)\n",
        "\n",
        "print(f\"\\n🚨 Generated {len(alerts)} alerts\")\n",
        "if len(alerts) > 0:\n",
        "    print(\"\\nAlerts:\")\n",
        "    print(alerts[['participant_id', 'alert_priority', 'conditions_met', \n",
        "                  'cancel_rate', 'median_time_to_cancel', 'size_ratio']].to_string(index=False))\n",
        "\n",
        "# Evaluate against ground truth\n",
        "# Count actual spoofers (P00 and P01)\n",
        "actual_spoofers = ['P00', 'P01']\n",
        "detected_participants = alerts['participant_id'].tolist() if len(alerts) > 0 else []\n",
        "\n",
        "true_positives = sum(1 for p in detected_participants if p in actual_spoofers)\n",
        "false_positives = sum(1 for p in detected_participants if p not in actual_spoofers)\n",
        "false_negatives = sum(1 for p in actual_spoofers if p not in detected_participants)\n",
        "true_negatives = sum(1 for p in participant_stats['participant_id'] \n",
        "                    if p not in detected_participants and p not in actual_spoofers)\n",
        "\n",
        "precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "print(f\"\\n📊 Detection Performance:\")\n",
        "print(f\"True Positives:  {true_positives}\")\n",
        "print(f\"False Positives: {false_positives}\")\n",
        "print(f\"False Negatives: {false_negatives}\")\n",
        "print(f\"True Negatives:  {true_negatives}\")\n",
        "print(f\"\\nPrecision: {precision:.2%}\")\n",
        "print(f\"Recall:    {recall:.2%}\")\n",
        "print(f\"F1 Score:  {f1:.2%}\")"
      ],
      "id": "e0c79976"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 1.3: Threshold Calibration\n",
        "\n",
        "Experiment with different threshold combinations and observe the\n",
        "precision/recall trade-off:"
      ],
      "id": "ecbea669-4cb8-4638-9ef5-ffa33b65c7bd"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test multiple threshold combinations\n",
        "threshold_experiments = [\n",
        "    {'cancel_rate': 0.85, 'time': 10.0, 'size_ratio': 1.5, 'prof': 1, 'name': 'Sensitive'},\n",
        "    {'cancel_rate': 0.90, 'time': 5.0, 'size_ratio': 2.0, 'prof': 1, 'name': 'Moderate'},\n",
        "    {'cancel_rate': 0.95, 'time': 2.0, 'size_ratio': 3.0, 'prof': 2, 'name': 'Conservative'},\n",
        "    {'cancel_rate': 0.98, 'time': 1.0, 'size_ratio': 5.0, 'prof': 3, 'name': 'Very Conservative'}\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "for exp in threshold_experiments:\n",
        "    alerts = detect_spoofing(participant_stats,\n",
        "                           cancel_rate_threshold=exp['cancel_rate'],\n",
        "                           time_to_cancel_threshold=exp['time'],\n",
        "                           size_ratio_threshold=exp['size_ratio'],\n",
        "                           min_profitability_score=exp['prof'])\n",
        "    \n",
        "    detected = alerts['participant_id'].tolist() if len(alerts) > 0 else []\n",
        "    tp = sum(1 for p in detected if p in actual_spoofers)\n",
        "    fp = sum(1 for p in detected if p not in actual_spoofers)\n",
        "    fn = sum(1 for p in actual_spoofers if p not in detected)\n",
        "    \n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "    \n",
        "    results.append({\n",
        "        'configuration': exp['name'],\n",
        "        'n_alerts': len(alerts),\n",
        "        'true_positives': tp,\n",
        "        'false_positives': fp,\n",
        "        'false_negatives': fn,\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1_score': f1\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "print(\"\\n📊 Threshold Calibration Results:\")\n",
        "print(results_df.to_string(index=False))\n",
        "\n",
        "# Visualize precision-recall trade-off\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.plot(results_df['recall'], results_df['precision'], 'o-', linewidth=2, markersize=10)\n",
        "\n",
        "for _, row in results_df.iterrows():\n",
        "    ax.annotate(row['configuration'], \n",
        "               (row['recall'], row['precision']),\n",
        "               textcoords=\"offset points\", \n",
        "               xytext=(0,10), \n",
        "               ha='center',\n",
        "               fontsize=9)\n",
        "\n",
        "ax.set_xlabel('Recall (True Positive Rate)')\n",
        "ax.set_ylabel('Precision')\n",
        "ax.set_title('Precision-Recall Trade-off: Spoofing Detection Calibration')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(-0.05, 1.05)\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('spoofing_precision_recall.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Visualization saved as 'spoofing_precision_recall.png'\")"
      ],
      "id": "4ea6d95d"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions\n",
        "\n",
        "1.  **Threshold Selection**: Which threshold configuration would you\n",
        "    deploy in production? Consider both detection effectiveness (F1\n",
        "    score) and operational constraints (investigation capacity). How\n",
        "    would you justify your choice to regulators?\n",
        "\n",
        "2.  **False Positives**: Examine false positive alerts. What legitimate\n",
        "    trading behaviors trigger alerts? How could detection be refined to\n",
        "    reduce false positives without sacrificing recall?\n",
        "\n",
        "3.  **Evidence Requirements**: Suppose an alert escalates to regulatory\n",
        "    investigation. What additional evidence beyond statistical patterns\n",
        "    would strengthen the case? (Consider: trader communications,\n",
        "    algorithm code, market impact analysis, pattern repetition)\n",
        "\n",
        "4.  **Adversarial Adaptation**: If spoofers know detection thresholds,\n",
        "    how might they adapt tactics to evade detection whilst still\n",
        "    benefiting from manipulation? What surveillance enhancements would\n",
        "    counter such adaptation?\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Exercise 2: Wash Trading Detection\n",
        "\n",
        "### Context\n",
        "\n",
        "Wash trading creates false volume impressions through self-dealing or\n",
        "coordinated trading between related parties—violating market\n",
        "manipulation rules worldwide. Detection requires identifying\n",
        "transactions where beneficial ownership is identical or related parties\n",
        "trade in prearranged manner. Graph analytics reveal circular trading\n",
        "networks and relationships difficult to detect from individual\n",
        "transactions.\n",
        "\n",
        "This exercise provides simulated transaction data including legitimate\n",
        "trading and injected wash trading. You’ll construct transaction\n",
        "networks, calculate network metrics identifying suspicious subgraphs,\n",
        "and quantify volume inflation from wash trades.\n",
        "\n",
        "### Generating Transaction Network Data"
      ],
      "id": "0ce2186d-116b-4190-b13d-071dc8b6b027"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "from itertools import combinations\n",
        "\n",
        "# Generate transaction data with wash trading\n",
        "np.random.seed(42)\n",
        "\n",
        "n_traders = 20\n",
        "n_transactions = 1000\n",
        "\n",
        "# Define wash trading groups (coordinated traders)\n",
        "wash_groups = [\n",
        "    ['T00', 'T01', 'T02'],  # Group 1: circular trading\n",
        "    ['T15', 'T16']  # Group 2: simple wash trading pair\n",
        "]\n",
        "\n",
        "transactions = []\n",
        "\n",
        "for i in range(n_transactions):\n",
        "    # Determine if this is wash trade\n",
        "    is_wash = np.random.random() < 0.15\n",
        "    \n",
        "    if is_wash:\n",
        "        # Pick wash group\n",
        "        group = wash_groups[np.random.choice(len(wash_groups))]\n",
        "        buyer = np.random.choice(group)\n",
        "        # Seller is different member of same group\n",
        "        seller = np.random.choice([t for t in group if t != buyer])\n",
        "        price = 100 + np.random.normal(0, 0.5)  # Tight price clustering\n",
        "    else:\n",
        "        # Legitimate trade\n",
        "        buyer = f'T{np.random.randint(0, n_traders):02d}'\n",
        "        seller = f'T{np.random.randint(0, n_traders):02d}'\n",
        "        while seller == buyer:  # Avoid self-trading in legitimate trades\n",
        "            seller = f'T{np.random.randint(0, n_traders):02d}'\n",
        "        price = 100 + np.random.normal(0, 2.0)  # More price dispersion\n",
        "    \n",
        "    quantity = np.random.randint(100, 1000) if not is_wash else np.random.randint(500, 2000)\n",
        "    \n",
        "    transactions.append({\n",
        "        'transaction_id': f'TX{i:04d}',\n",
        "        'buyer': buyer,\n",
        "        'seller': seller,\n",
        "        'price': round(price, 2),\n",
        "        'quantity': quantity,\n",
        "        'value': round(price * quantity, 2),\n",
        "        'timestamp': session_start + timedelta(seconds=np.random.uniform(0, 6.5*3600)),\n",
        "        'is_wash_actual': is_wash  # Ground truth\n",
        "    })\n",
        "\n",
        "transactions_df = pd.DataFrame(transactions)\n",
        "transactions_df = transactions_df.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "print(f\"Generated {len(transactions_df)} transactions\")\n",
        "print(f\"Wash trades (ground truth): {transactions_df['is_wash_actual'].sum()}\")\n",
        "print(f\"\\nFirst few transactions:\")\n",
        "print(transactions_df[['transaction_id', 'buyer', 'seller', \n",
        "                       'price', 'quantity', 'value']].head(10))"
      ],
      "id": "64872451"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.1: Construct and Visualize Transaction Network\n",
        "\n",
        "Build a directed graph where nodes are traders and edges are\n",
        "transactions:"
      ],
      "id": "c82d3785-7955-4ea4-a125-181f5396cef6"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Construct transaction network\n",
        "def build_transaction_network(transactions_df):\n",
        "    \"\"\"\n",
        "    Build directed graph from transaction data.\n",
        "    \"\"\"\n",
        "    G = nx.DiGraph()\n",
        "    \n",
        "    # Add edges with attributes\n",
        "    for _, txn in transactions_df.iterrows():\n",
        "        if G.has_edge(txn['buyer'], txn['seller']):\n",
        "            # Update existing edge\n",
        "            G[txn['buyer']][txn['seller']]['weight'] += 1\n",
        "            G[txn['buyer']][txn['seller']]['total_value'] += txn['value']\n",
        "        else:\n",
        "            # New edge\n",
        "            G.add_edge(txn['buyer'], txn['seller'], \n",
        "                      weight=1, \n",
        "                      total_value=txn['value'])\n",
        "    \n",
        "    return G\n",
        "\n",
        "G = build_transaction_network(transactions_df)\n",
        "\n",
        "print(f\"\\nNetwork Statistics:\")\n",
        "print(f\"Nodes (traders): {G.number_of_nodes()}\")\n",
        "print(f\"Edges (trader pairs): {G.number_of_edges()}\")\n",
        "print(f\"Network density: {nx.density(G):.3f}\")\n",
        "\n",
        "# Calculate node metrics\n",
        "in_degree = dict(G.in_degree())\n",
        "out_degree = dict(G.out_degree())\n",
        "reciprocity_scores = {}\n",
        "\n",
        "for node in G.nodes():\n",
        "    # Reciprocity: how many connections go both ways?\n",
        "    neighbors_out = set(G.successors(node))\n",
        "    neighbors_in = set(G.predecessors(node))\n",
        "    reciprocal = neighbors_out.intersection(neighbors_in)\n",
        "    \n",
        "    total_connections = len(neighbors_out.union(neighbors_in))\n",
        "    reciprocity = len(reciprocal) / total_connections if total_connections > 0 else 0\n",
        "    reciprocity_scores[node] = reciprocity\n",
        "\n",
        "# Visualize network\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Use spring layout\n",
        "pos = nx.spring_layout(G, k=0.5, iterations=50, seed=42)\n",
        "\n",
        "# Node sizes based on degree\n",
        "node_sizes = [300 * (in_degree[node] + out_degree[node]) for node in G.nodes()]\n",
        "\n",
        "# Node colors based on reciprocity (red = high reciprocity = suspicious)\n",
        "node_colors = [reciprocity_scores[node] for node in G.nodes()]\n",
        "\n",
        "# Draw network\n",
        "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, \n",
        "                       node_color=node_colors, cmap='YlOrRd',\n",
        "                       alpha=0.7, vmin=0, vmax=1)\n",
        "nx.draw_networkx_labels(G, pos, font_size=8)\n",
        "nx.draw_networkx_edges(G, pos, alpha=0.3, edge_color='gray',\n",
        "                       arrows=True, arrowsize=10, arrowstyle='->')\n",
        "\n",
        "plt.title('Transaction Network (Node Color = Reciprocity Score)', fontsize=14)\n",
        "plt.colorbar(plt.cm.ScalarMappable(cmap='YlOrRd', \n",
        "                                   norm=plt.Normalize(vmin=0, vmax=1)),\n",
        "            label='Reciprocity Score', shrink=0.8)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "plt.savefig('wash_trading_network.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Visualization saved as 'wash_trading_network.png'\")"
      ],
      "id": "8fd451f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2: Detect Wash Trading Using Network Metrics\n",
        "\n",
        "Calculate network metrics identifying suspicious trading patterns:"
      ],
      "id": "c3229db4-cc99-481e-9436-052407285db3"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def detect_wash_trading_network(G, transactions_df, \n",
        "                               reciprocity_threshold=0.5,\n",
        "                               min_transactions=3):\n",
        "    \"\"\"\n",
        "    Detect potential wash trading using network analysis.\n",
        "    \"\"\"\n",
        "    alerts = []\n",
        "    \n",
        "    # Find strongly connected components (circular trading)\n",
        "    sccs = list(nx.strongly_connected_components(G))\n",
        "    \n",
        "    for scc in sccs:\n",
        "        if len(scc) >= 2:  # Ignore single nodes\n",
        "            subgraph = G.subgraph(scc)\n",
        "            \n",
        "            # Calculate metrics for this component\n",
        "            scc_nodes = list(scc)\n",
        "            \n",
        "            # Count transactions within component\n",
        "            internal_txns = transactions_df[\n",
        "                (transactions_df['buyer'].isin(scc_nodes)) &\n",
        "                (transactions_df['seller'].isin(scc_nodes))\n",
        "            ]\n",
        "            n_internal = len(internal_txns)\n",
        "            \n",
        "            # Calculate total volume\n",
        "            internal_volume = internal_txns['value'].sum()\n",
        "            \n",
        "            # External transactions\n",
        "            external_txns = transactions_df[\n",
        "                (transactions_df['buyer'].isin(scc_nodes)) |\n",
        "                (transactions_df['seller'].isin(scc_nodes))\n",
        "            ]\n",
        "            external_txns = external_txns[\n",
        "                ~((external_txns['buyer'].isin(scc_nodes)) &\n",
        "                  (external_txns['seller'].isin(scc_nodes)))\n",
        "            ]\n",
        "            n_external = len(external_txns)\n",
        "            \n",
        "            # Calculate insularity: internal vs external trading ratio\n",
        "            insularity = n_internal / (n_internal + n_external) if (n_internal + n_external) > 0 else 0\n",
        "            \n",
        "            # Average reciprocity in component\n",
        "            avg_reciprocity = np.mean([reciprocity_scores[node] for node in scc_nodes])\n",
        "            \n",
        "            # Detect wash trading if:\n",
        "            # 1. High reciprocity\n",
        "            # 2. Significant internal trading\n",
        "            # 3. High insularity (trades mostly within group)\n",
        "            if (avg_reciprocity >= reciprocity_threshold and \n",
        "                n_internal >= min_transactions and\n",
        "                insularity > 0.3):\n",
        "                \n",
        "                alert_priority = 'HIGH' if insularity > 0.6 else 'MEDIUM'\n",
        "                \n",
        "                alerts.append({\n",
        "                    'traders': ', '.join(sorted(scc_nodes)),\n",
        "                    'n_traders': len(scc_nodes),\n",
        "                    'alert_priority': alert_priority,\n",
        "                    'internal_transactions': n_internal,\n",
        "                    'external_transactions': n_external,\n",
        "                    'insularity': insularity,\n",
        "                    'avg_reciprocity': avg_reciprocity,\n",
        "                    'internal_volume': internal_volume\n",
        "                })\n",
        "    \n",
        "    return pd.DataFrame(alerts)\n",
        "\n",
        "# Detect wash trading\n",
        "wash_alerts = detect_wash_trading_network(G, transactions_df)\n",
        "\n",
        "print(f\"\\n🚨 Generated {len(wash_alerts)} wash trading alerts\")\n",
        "if len(wash_alerts) > 0:\n",
        "    print(\"\\nAlerts:\")\n",
        "    print(wash_alerts.to_string(index=False))\n",
        "    \n",
        "    # Calculate volume inflation\n",
        "    total_volume = transactions_df['value'].sum()\n",
        "    wash_volume = wash_alerts['internal_volume'].sum()\n",
        "    inflation_pct = (wash_volume / total_volume) * 100\n",
        "    \n",
        "    print(f\"\\n📊 Volume Inflation Analysis:\")\n",
        "    print(f\"Total volume: ${total_volume:,.0f}\")\n",
        "    print(f\"Suspected wash volume: ${wash_volume:,.0f}\")\n",
        "    print(f\"Inflation: {inflation_pct:.1f}% of reported volume\")\n",
        "    \n",
        "    # Ground truth comparison\n",
        "    actual_wash_volume = transactions_df[transactions_df['is_wash_actual']]['value'].sum()\n",
        "    print(f\"\\nActual wash volume (ground truth): ${actual_wash_volume:,.0f}\")\n",
        "    print(f\"Detection accuracy: {(wash_volume / actual_wash_volume * 100):.1f}% of true wash volume detected\")"
      ],
      "id": "2d7d3541"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.3: Visualize Suspicious Subnetworks\n",
        "\n",
        "Highlight detected wash trading networks:"
      ],
      "id": "da2062ff-b63f-4037-b360-7a05535a25dc"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "if len(wash_alerts) > 0:\n",
        "    fig, axes = plt.subplots(1, len(wash_alerts), figsize=(7*len(wash_alerts), 6))\n",
        "    \n",
        "    if len(wash_alerts) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for idx, (_, alert) in enumerate(wash_alerts.iterrows()):\n",
        "        traders = alert['traders'].split(', ')\n",
        "        subG = G.subgraph(traders)\n",
        "        \n",
        "        pos = nx.circular_layout(subG)\n",
        "        \n",
        "        # Get edge weights for this subgraph\n",
        "        edge_weights = [subG[u][v]['weight'] for u, v in subG.edges()]\n",
        "        \n",
        "        nx.draw_networkx_nodes(subG, pos, node_size=800, \n",
        "                             node_color='red', alpha=0.7, ax=axes[idx])\n",
        "        nx.draw_networkx_labels(subG, pos, font_size=10, ax=axes[idx])\n",
        "        nx.draw_networkx_edges(subG, pos, width=2, alpha=0.6, \n",
        "                             edge_color='darkred',\n",
        "                             arrows=True, arrowsize=20, \n",
        "                             arrowstyle='->', ax=axes[idx])\n",
        "        \n",
        "        # Add edge labels (transaction counts)\n",
        "        edge_labels = {(u, v): f\"{subG[u][v]['weight']}\" for u, v in subG.edges()}\n",
        "        nx.draw_networkx_edge_labels(subG, pos, edge_labels, ax=axes[idx])\n",
        "        \n",
        "        axes[idx].set_title(f\"Alert {idx+1}: {alert['n_traders']} traders\\n\"\n",
        "                          f\"Insularity: {alert['insularity']:.0%}, \"\n",
        "                          f\"{alert['internal_transactions']} transactions\",\n",
        "                          fontsize=11)\n",
        "        axes[idx].axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.savefig('wash_trading_subnetworks.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n📊 Visualization saved as 'wash_trading_subnetworks.png'\")"
      ],
      "id": "93d2d06a"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions\n",
        "\n",
        "1.  **Network Metrics**: Which network metrics were most effective at\n",
        "    identifying wash trading? Why do reciprocity and insularity indicate\n",
        "    coordinated trading?\n",
        "\n",
        "2.  **False Positives**: Could legitimate trading patterns resemble wash\n",
        "    trading networks? (Consider: market makers trading with each other,\n",
        "    broker internalization, algorithmic trading across related\n",
        "    accounts). How would you distinguish legitimate from manipulative\n",
        "    patterns?\n",
        "\n",
        "3.  **Volume Inflation Impact**: Quantify the market harm from detected\n",
        "    wash trading. How does false volume affect other participants’\n",
        "    decisions? Should penalties relate to volume inflation magnitude?\n",
        "\n",
        "4.  **Beneficial Ownership**: Real-world detection requires knowing\n",
        "    beneficial ownership—multiple accounts controlled by single entity.\n",
        "    How would incomplete beneficial ownership data affect detection?\n",
        "    What additional data sources would improve accuracy?\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Exercise 3: Alert Triage and ML Scoring (FIN720)\n",
        "\n",
        "### Context\n",
        "\n",
        "Surveillance systems generate thousands of alerts daily; investigators\n",
        "can review hundreds. Effective triage prioritizes alerts by likelihood\n",
        "of being genuine violations, maximizing detection given limited\n",
        "investigation capacity. Machine learning can predict investigation\n",
        "outcomes from alert characteristics, automating prioritization and\n",
        "improving surveillance efficiency.\n",
        "\n",
        "This exercise provides simulated alerts from multiple detection systems\n",
        "(spoofing, wash trading, layering, marking the close). Each alert has\n",
        "features describing participant, pattern characteristics, and market\n",
        "context. You’ll train ML classifiers predicting investigation outcomes\n",
        "and compare ML triage to baseline prioritization approaches.\n",
        "\n",
        "### Generating Alert Data with Investigation Outcomes"
      ],
      "id": "00cba1e5-a2d7-482a-9dea-d4d1c5df7806"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Generate alert data\n",
        "np.random.seed(42)\n",
        "\n",
        "n_alerts = 500\n",
        "violation_rate = 0.12  # 12% of alerts are actual violations\n",
        "\n",
        "alerts_data = []\n",
        "\n",
        "for i in range(n_alerts):\n",
        "    # Violation determination (ground truth)\n",
        "    is_violation = np.random.random() < violation_rate\n",
        "    \n",
        "    # Alert type\n",
        "    alert_type = np.random.choice(['spoofing', 'wash_trading', 'layering', 'marking_close'],\n",
        "                                  p=[0.35, 0.25, 0.25, 0.15])\n",
        "    \n",
        "    # Participant characteristics (violations more likely from certain profiles)\n",
        "    if is_violation:\n",
        "        participant_size = np.random.choice(['large', 'medium', 'small'], p=[0.2, 0.3, 0.5])\n",
        "        prior_violations = np.random.choice([0, 1, 2, 3], p=[0.3, 0.4, 0.2, 0.1])\n",
        "        trader_experience = np.random.choice(['junior', 'mid', 'senior'], p=[0.5, 0.3, 0.2])\n",
        "    else:\n",
        "        participant_size = np.random.choice(['large', 'medium', 'small'], p=[0.4, 0.4, 0.2])\n",
        "        prior_violations = np.random.choice([0, 1, 2, 3], p=[0.7, 0.25, 0.04, 0.01])\n",
        "        trader_experience = np.random.choice(['junior', 'mid', 'senior'], p=[0.3, 0.4, 0.3])\n",
        "    \n",
        "    # Pattern characteristics\n",
        "    if is_violation:\n",
        "        pattern_clarity = np.random.uniform(0.6, 0.95)\n",
        "        n_occurrences = np.random.randint(3, 20)\n",
        "        profitability = np.random.uniform(5000, 50000)\n",
        "    else:\n",
        "        pattern_clarity = np.random.uniform(0.3, 0.7)\n",
        "        n_occurrences = np.random.randint(1, 5)\n",
        "        profitability = np.random.uniform(-5000, 10000)\n",
        "    \n",
        "    # Market context\n",
        "    volatility = np.random.uniform(0.01, 0.05)\n",
        "    liquidity_score = np.random.uniform(0.2, 1.0)\n",
        "    \n",
        "    # Rule confidence (violations match rules more closely)\n",
        "    if is_violation:\n",
        "        rule_confidence = np.random.uniform(0.7, 0.98)\n",
        "        n_rules_triggered = np.random.randint(2, 6)\n",
        "    else:\n",
        "        rule_confidence = np.random.uniform(0.4, 0.75)\n",
        "        n_rules_triggered = np.random.randint(1, 3)\n",
        "    \n",
        "    alerts_data.append({\n",
        "        'alert_id': f'A{i:04d}',\n",
        "        'alert_type': alert_type,\n",
        "        'participant_size': participant_size,\n",
        "        'prior_violations': prior_violations,\n",
        "        'trader_experience': trader_experience,\n",
        "        'pattern_clarity': pattern_clarity,\n",
        "        'n_occurrences': n_occurrences,\n",
        "        'profitability': profitability,\n",
        "        'volatility': volatility,\n",
        "        'liquidity_score': liquidity_score,\n",
        "        'rule_confidence': rule_confidence,\n",
        "        'n_rules_triggered': n_rules_triggered,\n",
        "        'is_violation': is_violation  # Ground truth\n",
        "    })\n",
        "\n",
        "alerts_df = pd.DataFrame(alerts_data)\n",
        "\n",
        "print(f\"Generated {len(alerts_df)} alerts\")\n",
        "print(f\"True violations: {alerts_df['is_violation'].sum()} ({alerts_df['is_violation'].mean():.1%})\")\n",
        "print(f\"\\nFirst few alerts:\")\n",
        "print(alerts_df[['alert_id', 'alert_type', 'participant_size', \n",
        "                 'prior_violations', 'pattern_clarity', 'is_violation']].head(10))"
      ],
      "id": "931dcddb"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.1: Prepare Features and Train ML Models\n",
        "\n",
        "Encode categorical features and train multiple classifiers:"
      ],
      "id": "afdd927a-2648-4ab3-a2c0-9a40287f7cc9"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare features\n",
        "def prepare_features(df):\n",
        "    \"\"\"\n",
        "    Encode categorical variables and prepare features.\n",
        "    \"\"\"\n",
        "    df_encoded = df.copy()\n",
        "    \n",
        "    # One-hot encode categorical variables\n",
        "    df_encoded = pd.get_dummies(df_encoded, \n",
        "                               columns=['alert_type', 'participant_size', 'trader_experience'],\n",
        "                               drop_first=False)\n",
        "    \n",
        "    # Feature columns\n",
        "    feature_cols = [col for col in df_encoded.columns \n",
        "                   if col not in ['alert_id', 'is_violation']]\n",
        "    \n",
        "    X = df_encoded[feature_cols]\n",
        "    y = df_encoded['is_violation'].astype(int)\n",
        "    \n",
        "    return X, y, feature_cols\n",
        "\n",
        "X, y, feature_cols = prepare_features(alerts_df)\n",
        "\n",
        "print(f\"\\nFeatures: {len(feature_cols)}\")\n",
        "print(f\"Feature names: {feature_cols}\")\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"\\nTraining set: {len(X_train)} alerts ({y_train.sum()} violations)\")\n",
        "print(f\"Test set: {len(X_test)} alerts ({y_test.sum()} violations)\")\n",
        "\n",
        "# Train multiple models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10),\n",
        "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
        "}\n",
        "\n",
        "model_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    \n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred, target_names=['Not Violation', 'Violation']))\n",
        "    \n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    print(cm)\n",
        "    print(f\"(TN={cm[0,0]}, FP={cm[0,1]}, FN={cm[1,0]}, TP={cm[1,1]})\")\n",
        "    \n",
        "    auc = roc_auc_score(y_test, y_pred_proba)\n",
        "    print(f\"\\nROC AUC: {auc:.3f}\")\n",
        "    \n",
        "    model_results[name] = {\n",
        "        'model': model,\n",
        "        'y_pred': y_pred,\n",
        "        'y_pred_proba': y_pred_proba,\n",
        "        'auc': auc\n",
        "    }\n",
        "\n",
        "# Feature importance (Random Forest)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Feature Importance (Random Forest):\")\n",
        "importances = models['Random Forest'].feature_importances_\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'feature': feature_cols,\n",
        "    'importance': importances\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "print(feature_importance_df.head(10).to_string(index=False))"
      ],
      "id": "a45f3d10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3.2: Compare ML Triage to Baselines\n",
        "\n",
        "Evaluate ML prioritization against baseline approaches:"
      ],
      "id": "7b94e9b1-3115-4a79-b99f-9258c6db41cb"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline 1: Random ordering\n",
        "np.random.seed(42)\n",
        "baseline_random = np.random.permutation(len(X_test))\n",
        "\n",
        "# Baseline 2: Rule confidence ordering\n",
        "alerts_test = alerts_df.iloc[X_test.index].copy()\n",
        "baseline_rule = alerts_test.sort_values('rule_confidence', ascending=False).index\n",
        "\n",
        "# ML ordering: Gradient Boosting predicted probability\n",
        "ml_proba = model_results['Gradient Boosting']['y_pred_proba']\n",
        "alerts_test_ml = alerts_test.copy()\n",
        "alerts_test_ml['ml_score'] = ml_proba\n",
        "ml_ordering = alerts_test_ml.sort_values('ml_score', ascending=False).index\n",
        "\n",
        "# Evaluate: Cumulative violations detected in top-N alerts\n",
        "def cumulative_detection(y_true, ordering, top_n_list):\n",
        "    \"\"\"\n",
        "    Calculate cumulative violations detected for different top-N thresholds.\n",
        "    \"\"\"\n",
        "    y_ordered = y_true.iloc[ordering].values\n",
        "    cumulative = np.cumsum(y_ordered)\n",
        "    \n",
        "    results = []\n",
        "    for top_n in top_n_list:\n",
        "        if top_n <= len(y_ordered):\n",
        "            detected = cumulative[top_n - 1]\n",
        "            detection_rate = detected / y_true.sum()\n",
        "            results.append(detected)\n",
        "        else:\n",
        "            results.append(cumulative[-1])\n",
        "    \n",
        "    return results\n",
        "\n",
        "top_n_list = [10, 25, 50, 100, len(X_test)]\n",
        "\n",
        "random_detection = cumulative_detection(y_test, baseline_random, top_n_list)\n",
        "rule_detection = cumulative_detection(y_test, baseline_rule, top_n_list)\n",
        "ml_detection = cumulative_detection(y_test, ml_ordering, top_n_list)\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'top_n_alerts': top_n_list,\n",
        "    'random_triage': random_detection,\n",
        "    'rule_confidence_triage': rule_detection,\n",
        "    'ml_triage': ml_detection,\n",
        "    'total_violations': [y_test.sum()] * len(top_n_list)\n",
        "})\n",
        "\n",
        "comparison_df['random_rate'] = comparison_df['random_triage'] / comparison_df['total_violations']\n",
        "comparison_df['rule_rate'] = comparison_df['rule_confidence_triage'] / comparison_df['total_violations']\n",
        "comparison_df['ml_rate'] = comparison_df['ml_triage'] / comparison_df['total_violations']\n",
        "\n",
        "print(f\"\\n📊 Triage Comparison: Violations Detected in Top-N Alerts\")\n",
        "print(comparison_df[['top_n_alerts', 'random_triage', 'rule_confidence_triage', \n",
        "                    'ml_triage', 'total_violations']].to_string(index=False))\n",
        "\n",
        "print(f\"\\n📊 Detection Rates:\")\n",
        "print(comparison_df[['top_n_alerts', 'random_rate', 'rule_rate', 'ml_rate']].to_string(index=False))\n",
        "\n",
        "# Visualize\n",
        "fig, ax = plt.subplots(figsize=(12, 7))\n",
        "\n",
        "ax.plot(comparison_df['top_n_alerts'], comparison_df['random_rate'], \n",
        "       'o-', label='Random Triage', linewidth=2)\n",
        "ax.plot(comparison_df['top_n_alerts'], comparison_df['rule_rate'], \n",
        "       's-', label='Rule Confidence Triage', linewidth=2)\n",
        "ax.plot(comparison_df['top_n_alerts'], comparison_df['ml_rate'], \n",
        "       '^-', label='ML Triage (Gradient Boosting)', linewidth=2)\n",
        "\n",
        "# Perfect triage (all violations first)\n",
        "perfect_detection = [min(n, y_test.sum()) / y_test.sum() for n in top_n_list]\n",
        "ax.plot(comparison_df['top_n_alerts'], perfect_detection, \n",
        "       '--', color='green', label='Perfect Triage', linewidth=2, alpha=0.5)\n",
        "\n",
        "ax.set_xlabel('Number of Alerts Investigated (Top-N)', fontsize=12)\n",
        "ax.set_ylabel('Fraction of Violations Detected', fontsize=12)\n",
        "ax.set_title('Triage Effectiveness: Violations Detected vs Investigation Effort', fontsize=14)\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim(0, max(top_n_list))\n",
        "ax.set_ylim(0, 1.05)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('triage_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n📊 Visualization saved as 'triage_comparison.png'\")\n",
        "\n",
        "# Calculate improvement\n",
        "print(f\"\\n💡 ML Triage Improvement:\")\n",
        "for idx, top_n in enumerate([10, 25, 50]):\n",
        "    random_rate = comparison_df.iloc[idx]['random_rate']\n",
        "    rule_rate = comparison_df.iloc[idx]['rule_rate']\n",
        "    ml_rate = comparison_df.iloc[idx]['ml_rate']\n",
        "    \n",
        "    improvement_vs_random = ((ml_rate - random_rate) / random_rate * 100) if random_rate > 0 else 0\n",
        "    improvement_vs_rule = ((ml_rate - rule_rate) / rule_rate * 100) if rule_rate > 0 else 0\n",
        "    \n",
        "    print(f\"\\nTop-{top_n} alerts investigated:\")\n",
        "    print(f\"  ML detection rate: {ml_rate:.1%}\")\n",
        "    print(f\"  Improvement vs random: +{improvement_vs_random:.0f}%\")\n",
        "    print(f\"  Improvement vs rule: +{improvement_vs_rule:.0f}%\")"
      ],
      "id": "152b7525"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Reflection Questions\n",
        "\n",
        "1.  **ML Value Proposition**: Quantify the operational benefit of ML\n",
        "    triage. If investigators can review 50 alerts per day, how many more\n",
        "    violations does ML detect compared to baselines? How would you\n",
        "    present this ROI to management justifying ML investment?\n",
        "\n",
        "2.  **Explainability Requirements**: Regulators may question ML\n",
        "    triage—why was specific alert deprioritized potentially missing\n",
        "    violation? Using feature importance and SHAP values, how would you\n",
        "    explain individual triage decisions? What documentation would\n",
        "    support regulatory audit?\n",
        "\n",
        "3.  **Adversarial Adaptation**: If manipulators learn that certain\n",
        "    characteristics (participant size, prior violations, pattern\n",
        "    clarity) increase triage priority, how might they adapt tactics to\n",
        "    score lower? What surveillance enhancements would detect such\n",
        "    adaptation?\n",
        "\n",
        "4.  **Cost Asymmetry**: False negatives (missing violations) and false\n",
        "    positives (wasting investigator time) have different costs. How\n",
        "    would you adjust ML decision thresholds accounting for asymmetric\n",
        "    costs? Should thresholds vary by violation type (insider trading vs\n",
        "    spoofing)?\n",
        "\n",
        "------------------------------------------------------------------------\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "This lab applied market surveillance concepts to realistic detection\n",
        "scenarios. You implemented spoofing detection rules, calibrated\n",
        "thresholds managing false positive trade-offs, used graph analytics\n",
        "detecting wash trading networks, and trained ML models improving alert\n",
        "triage efficiency. The exercises demonstrated that effective\n",
        "surveillance requires not just technical detection algorithms but also\n",
        "operational judgment balancing detection sensitivity against\n",
        "investigation capacity and regulatory requirements.\n",
        "\n",
        "Key insights: (1) Rules-based detection provides explainability\n",
        "essential for regulatory acceptance but requires continuous maintenance\n",
        "and misses novel tactics. (2) Network analytics reveal collusion\n",
        "patterns invisible from individual transactions, though false positives\n",
        "arise from legitimate coordinated trading. (3) Machine learning\n",
        "substantially improves triage efficiency—detecting more violations given\n",
        "limited investigation resources—but regulatory explainability\n",
        "requirements constrain model complexity. (4) Surveillance effectiveness\n",
        "depends on operational context—detection algorithms must be calibrated\n",
        "considering investigator capacity, violation costs, and regulatory\n",
        "expectations, not just technical metrics.\n",
        "\n",
        "Real-world surveillance systems operate at institutional scale (billions\n",
        "of events daily), integrate data from multiple sources (order books,\n",
        "trades, communications, external intelligence), and support complex\n",
        "investigation workflows (case management, regulatory reporting, evidence\n",
        "documentation). This lab provided simplified scenarios illustrating core\n",
        "concepts; production deployment requires additional engineering\n",
        "(scalability, reliability, security), domain expertise (market\n",
        "microstructure, regulatory requirements), and operational processes\n",
        "(governance, tuning, training).\n",
        "\n",
        "As Week 11 concludes, reflect on surveillance’s role in market\n",
        "integrity—protecting participants from manipulation whilst imposing\n",
        "compliance costs. The challenge is achieving effective deterrence\n",
        "without excessive burden, particularly for smaller firms lacking large\n",
        "compliance teams. RegTech promises automation reducing costs whilst\n",
        "improving effectiveness, but adoption faces barriers from data\n",
        "sensitivity, regulatory conservatism, and integration complexity. The\n",
        "future surveillance landscape will increasingly incorporate AI,\n",
        "blockchain analytics, and cross-border coordination addressing\n",
        "manipulation in fragmented global markets.\n",
        "\n",
        "**Next steps**: Week 12 synthesizes course material, discusses ethical\n",
        "implications of FinTech innovations, and prepares for assessments.\n",
        "Consider how surveillance concepts apply to assessment work—trading\n",
        "strategies should avoid patterns appearing manipulative; FinTech\n",
        "evaluations should assess regulatory compliance and identify gaps\n",
        "between technological capabilities and legal obligations.\n",
        "\n",
        "## Additional Resources\n",
        "\n",
        "**Regulatory guidance:**\n",
        "\n",
        "-   FCA Market Watch newsletters: Current enforcement actions and\n",
        "    emerging risks\n",
        "-   ESMA MAR Q&A: Common questions on Market Abuse Regulation\n",
        "-   FINRA Surveillance Reports: US broker-dealer surveillance best\n",
        "    practices\n",
        "-   SEC Division of Enforcement: Case studies of prosecuted manipulation\n",
        "\n",
        "**Technical resources:**\n",
        "\n",
        "-   NetworkX documentation: Graph analytics in Python\n",
        "-   Scikit-learn: Machine learning model documentation\n",
        "-   SHAP: Explainable AI for model interpretability\n",
        "-   Production surveillance vendors: FICO, NICE Actimize, SAS (white\n",
        "    papers and case studies)\n",
        "\n",
        "**Academic papers:**\n",
        "\n",
        "-   Scopino (2015): “The (Questionable) Legality of High-Speed\n",
        "    Pinging” - HFT manipulation debate\n",
        "-   Comerton-Forde & Putniņš (2014): “Stock Price Manipulation” - Survey\n",
        "    of manipulation research\n",
        "-   Jiang et al. (2018): “Machine Learning in Market Manipulation\n",
        "    Detection” - ML applications\n",
        "\n",
        "**Professional development:**\n",
        "\n",
        "-   CAMS certification: Certified Anti-Money Laundering Specialist\n",
        "-   SCR exam: Securities Industry Essentials and Series 7 (US market\n",
        "    knowledge)\n",
        "-   AML/Surveillance conferences: ACAMS, FINRA Annual Conference"
      ],
      "id": "c9650ca9-2df3-4060-9625-72048552ff19"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python",
      "path": "/Users/quinference/Library/Jupyter/kernels/python3"
    }
  }
}