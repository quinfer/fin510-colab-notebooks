{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Lab 6B: Backtest Overfitting (CSCV, PBO, PSR/DSR)\"\n",
        "format:\n",
        "  html:\n",
        "    toc: true\n",
        "bibliography: ../resources/reading.bib\n",
        "execute:\n",
        "  echo: false\n",
        "  warning: false\n",
        "  message: false\n",
        "---\n",
        "\n",
        "## Before You Code: The Big Picture\n",
        "\n",
        "The #1 problem in quantitative finance: **your backtest looks great, but it fails in live trading**. Why? **Overfitting**—you optimized parameters on the same data you tested on. Your \"alpha\" is actually selection bias.\n",
        "\n",
        "::: {.callout-note}\n",
        "## The Backtest Overfitting Problem\n",
        "\n",
        "**The Scenario:**\n",
        "You test 200 trading strategies on 20 years of data. One strategy has a Sharpe ratio of 2.5—amazing! You deploy it with real money. It loses money immediately. What happened?\n",
        "\n",
        "**The Problem:**\n",
        "- With 200 tries, **one will look good by pure luck** (multiple testing)\n",
        "- In-sample optimization + in-sample testing = guaranteed overfitting\n",
        "- Traditional cross-validation doesn't detect this (data leakage across folds)\n",
        "\n",
        "**The Solution (Bailey & López de Prado):**\n",
        "1. **CSCV (Combinatorially Symmetric Cross-Validation)**: Proper walk-forward splits\n",
        "2. **PBO (Probability of Backtest Overfitting)**: Quantifies selection bias\n",
        "3. **PSR (Probabilistic Sharpe Ratio)**: Tests if Sharpe > 0 with statistical significance\n",
        "4. **DSR (Deflated Sharpe Ratio)**: Adjusts for multiple testing\n",
        "\n",
        "**The Evidence:**\n",
        "Harvey, Liu & Zhu (2016, RFS): Most published factor strategies fail out-of-sample due to p-hacking and multiple testing. PBO/PSR help detect this **before** losing real money.\n",
        ":::\n",
        "\n",
        "### What You'll Build Today\n",
        "\n",
        "By the end of this lab, you will have:\n",
        "\n",
        "- ✅ Understanding of why standard backtesting fails\n",
        "- ✅ CSCV implementation for honest validation\n",
        "- ✅ PBO calculation showing selection bias\n",
        "- ✅ PSR/DSR metrics for performance significance\n",
        "- ✅ Critical perspective on published trading strategies\n",
        "\n",
        "**Time estimate:** 90-120 minutes (this is advanced material—take your time)\n",
        "\n",
        "::: {.callout-important}\n",
        "## Why This Matters for Coursework 2\n",
        "Your factor replication **must** use walk-forward validation and report PBO/PSR. Otherwise, your Sharpe ratio is meaningless—it's just in-sample optimization parading as out-of-sample performance. This lab shows you how to do it right.\n",
        ":::\n",
        "\n",
        "# Objectives\n",
        "\n",
        "- Diagnose backtest overfitting with combinatorially symmetric cross‑validation (CSCV)  \n",
        "- Estimate Probability of Backtest Overfitting (PBO)  \n",
        "- Quantify performance significance via Probabilistic Sharpe Ratio (PSR); discuss Deflated Sharpe Ratio (DSR)\n",
        "\n",
        "::: callout-note\n",
        "This lab follows Bailey & López de Prado's approach to selection bias: CSCV → PBO and PSR/DSR. We implement lightweight utilities and show how to compare against `mlfinlab` if available.\n",
        ":::\n",
        "\n",
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import sys, pathlib\n",
        "# Ensure project root (parent of labs/) is on the Python path for `scripts/`\n",
        "sys.path.append(str(pathlib.Path().resolve().parent))\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scripts.overfit_metrics import (\n",
        "    generate_noise_strategies,\n",
        "    sharpe_ratio,\n",
        "    probabilistic_sharpe_ratio,\n",
        "    cscv_pbo,\n",
        ")\n",
        "\n",
        "# Optional: compare with mlfinlab if installed\n",
        "try:\n",
        "    from mlfinlab.backtest_statistics import deflated_sharpe_ratio as dsr_mlfinlab\n",
        "except Exception:\n",
        "    dsr_mlfinlab = None\n",
        "\n",
        "np.random.seed(123)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part A — A garden of strategies on pure noise\n",
        "\n",
        "We simulate `N=200` strategies with no true edge. In a finite sample, one will “win” in‑sample by chance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "T, N = 240, 200  # 20 years of monthly returns (approx)\n",
        "X = generate_noise_strategies(T=T, N=N, rho=0.2, seed=123)\n",
        "\n",
        "# In‑sample Sharpe ratios across strategies\n",
        "sr_all = np.array([sharpe_ratio(X[:, j]) for j in range(N)])\n",
        "j_star = int(np.argmax(sr_all))\n",
        "sr_star = sr_all[j_star]\n",
        "\n",
        "fig, ax = plt.subplots(1,1, figsize=(7,4))\n",
        "ax.hist(sr_all, bins=30, color='tab:gray', alpha=0.8)\n",
        "ax.axvline(sr_star, color='r', linestyle='--', label=f'Winner SR≈{sr_star:.2f}')\n",
        "ax.set_title('In‑sample Sharpe across noise strategies')\n",
        "ax.legend(); plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observation: Even with zero true edge, the best in‑sample Sharpe can look compelling.\n",
        "\n",
        "# Part B — CSCV and Probability of Backtest Overfitting (PBO)\n",
        "\n",
        "We split the time axis into contiguous folds and repeatedly pick the in‑sample “champion”, then measure its out‑of‑sample rank. PBO is the fraction of splits where the champion underperforms out‑of‑sample (negative logit rank)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "res = cscv_pbo(X, n_folds=10, max_splits=150)  # subsample CSCV splits for speed; increase if time allows\n",
        "res.pbo, res.splits_used"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
        "ax[0].hist(res.taus, bins=30, color='tab:blue', alpha=0.8)\n",
        "ax[0].axvline(0, color='r', linestyle='--', label='tau=0'); ax[0].legend()\n",
        "ax[0].set_title('Logit ranks of in‑sample champion (CSCV)')\n",
        "\n",
        "ax[1].hist(res.oos_ranks, bins=np.arange(1, X.shape[1]+2)-0.5, color='tab:orange', alpha=0.8)\n",
        "ax[1].set_title('OOS ranks of in‑sample champion')\n",
        "ax[1].set_xlim(0.5, min(40.5, X.shape[1]+0.5))\n",
        "plt.tight_layout(); plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Interpretation: A high PBO indicates that selecting the in‑sample “winner” is likely to disappoint out‑of‑sample.\n",
        "\n",
        "# Part C — PSR and discussion of DSR\n",
        "\n",
        "We compute the Probabilistic Sharpe Ratio (PSR) of the champion against a 0 benchmark. DSR additionally deflates for selection bias by using a higher benchmark Sharpe (selection threshold). If `mlfinlab` is installed, we compare against its DSR."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Champion’s in‑sample series and summary stats\n",
        "x_star = X[:, j_star]\n",
        "sr_hat = sharpe_ratio(x_star)\n",
        "n_obs = len(x_star)\n",
        "\n",
        "# Use normal‑like defaults for skew/kurtosis when unknown\n",
        "skew = pd.Series(x_star).skew()\n",
        "kurt = pd.Series(x_star).kurtosis() + 3  # pandas returns excess kurtosis\n",
        "\n",
        "psr_0 = probabilistic_sharpe_ratio(sr_hat, 0.0, n_obs, skew=skew, kurtosis=kurt)\n",
        "psr_0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optional: compare with `mlfinlab`’s DSR (if available). Note DSR uses an elevated benchmark Sharpe that accounts for the number of trials and their correlation (see paper for details)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "if dsr_mlfinlab is not None:\n",
        "    # Example parameters — you should set n_trials and correlation based on your research context\n",
        "    n_trials = N\n",
        "    corr = 0.2\n",
        "    dsr_val = dsr_mlfinlab(observed_sr=sr_hat, number_of_trials=n_trials, skew=skew, kurtosis=kurt, rho=corr, length=n_obs)\n",
        "    dsr_val\n",
        "else:\n",
        "    'mlfinlab not available in this environment'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional — Empirical Selection Benchmark (SR*)\n",
        "\n",
        "An intuitive (but approximate) benchmark SR* is the selection threshold you would have used to promote a strategy, e.g., the 95th percentile of candidate SRs or the top‑k cutoff used in model selection. This inflates the benchmark to reflect the search."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Naive empirical SR* from the in-sample garden (use with caution)\n",
        "sr_garden = sr_all  # in-sample Sharpe across candidates\n",
        "sr_star_empirical = np.quantile(sr_garden, 0.95)\n",
        "psr_emp = probabilistic_sharpe_ratio(sr_hat, sr_star_empirical, n_obs, skew=skew, kurtosis=kurt)\n",
        "sr_star_empirical, psr_emp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: callout-tip\n",
        "Guidance: PSR answers “what is the probability that the true SR > benchmark SR*?”. DSR raises SR* to deflate for selection bias (many trials and correlation among them). When reporting results, disclose the number of trials and use CSCV/PBO to evidence robustness.\n",
        ":::\n",
        "\n",
        "# Extension — Replace noise with weak‑edge signals\n",
        "\n",
        "Modify the simulation so a small subset of strategies has a slight positive mean. Re‑run CSCV/PBO and PSR to see whether evidence accumulates honestly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example: 10 strategies have a small true edge\n",
        "T, N = 240, 200\n",
        "X = generate_noise_strategies(T=T, N=N, rho=0.2, seed=777)\n",
        "edge_idx = np.arange(10)\n",
        "X[:, edge_idx] += 0.05 / np.sqrt(12)  # ~5% annual edge distributed monthly\n",
        "\n",
        "res2 = cscv_pbo(X, n_folds=10)\n",
        "res2.pbo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Deliverables\n",
        "\n",
        "- Report the observed PBO and interpret its meaning\n",
        "- Report PSR for the selected strategy; if available, compare with DSR\n",
        "- Describe how your result changes when a few strategies have a genuine (small) edge\n",
        "\n",
        "## How to Report (Template)\n",
        "\n",
        "- Trials: We evaluated N strategies/hyper‑parameters (comment on similarity/correlation if relevant).  \n",
        "- Selection: In‑sample selection metric = [Sharpe/alpha/etc.] with CSCV splits (k=10).  \n",
        "- Robustness: PBO = X.XX across S splits (show logit rank histogram).  \n",
        "- Significance: PSR = X.XX vs SR*=0 (skew=..., kurt=..., n=...)  \n",
        "  - Optional: DSR = X.XX (assumptions: trials=N, rho=..., length=n).  \n",
        "- Data: period, universe, costs/slippage, vintages/release timing.  \n",
        "- Decision: [Promote/Park], rationale and next steps (e.g., live paper trading).\n",
        "\n",
        "# References\n",
        "\n",
        "- @bailey2015pbo — Probability of Backtest Overfitting (PBO) and CSCV  \n",
        "- @lopezdeprado2014dsr — Deflated Sharpe Ratio (DSR)  \n",
        "- López de Prado, M. — Deflated Sharpe Ratio (DSR), SSRN  \n",
        "- White (2000) — Reality Check for data snooping  \n",
        "- Hansen (2005) — Superior Predictive Ability (SPA) test  \n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "nlp_env",
      "language": "python",
      "display_name": "Python (nlp_env)",
      "path": "/Users/quinference/Library/Jupyter/kernels/nlp_env"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}